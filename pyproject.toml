[build-system]
requires = ["setuptools>=61.0", "wheel"]
build-backend = "setuptools.build_meta"

[project]
name = "data_ev"
version = "1.0.0"
description = "Electric Vehicle Data Processing Pipeline"
readme = "readme.md"
requires-python = ">=3.11,<3.14"
dependencies = [
    # Core dependencies used across all modules
    "python-dotenv>=1.0.0",
    "pandas>=2.2.2", # Adds 150MB to images, might be worth to install only in needed modules
    "msgspec>=0.19.0",
    "schedule>=1.2.2",
    "rich>=13.5.2",
    "requests>=2.30.0",
    "redis>=6.0.0",
    "aiohttp>=3.11.11",
    "aioboto3>=15.5.0",
    "types-aioboto3[s3]>=15.5.0",
    "boto3>=1.36",
    "joblib>=1.5.2",
    "sqlalchemy>=2.0.0",
    "pydantic>=2.11",
    "pydantic-settings>=2.11",
    "click>=8.1.7",
]

[project.optional-dependencies]
# Activation module dependencies - handles vehicle API communication
activation = [
    "gspread",
    "oauth2client",
    "psycopg2-binary>=2.9.9",
    "asyncpg>=0.30",
    "uuid6>=2022.10.25",
    "sqlalchemy>=2.0.0",
    "rapidfuzz>=3.13.0",
]

# Transform module dependencies - handles ETL pipeline with Spark
transform = [
    "pyspark-client>=4.0.1",
    "pyarrow>=19.0.0",
    "apscheduler>=3.10.4",
    "psycopg2-binary>=2.9.9",
    "asyncpg>=0.30",
    "minio>=7.1.13",
    "pyyaml>=6.0",
    "dagster-pipes>=1.11",
]

# EDA module dependencies - for exploratory data analysis notebooks
eda = [
    "plotly>=6.4.0",
    "matplotlib>=3.10",
    "scipy>=1.16",
    "scikit-learn>=1.7",
    "statsmodels>=0.14",
    "pyarrow>=22.0.0",
    "nbformat>=5.10",
    "nbstripout>=0.8.1",
    "tqdm>=4.67",
    "xgboost>=2.0.0",
]

# Load module dependencies - for loading and processing trendlines
load = [
    "rapidfuzz>=3.13.0",
    "gspread",
    "oauth2client",
    "psycopg2-binary>=2.9.9",
    "asyncpg>=0.30",
    "sqlmodel>=0.0.24",
]

# Results/visualization module dependencies
results = [
    "plotly>=6.4.0",
    "psycopg2-binary>=2.9.9",
    "gspread",
    "oauth2client",
    "scikit-learn>=1.7",
    "pyarrow>=22.0.0",
    "pyspark-client>=4.0.1",
    "pyyaml>=6.0",
    "rapidfuzz>=3.13.0",
]

# Ingestion module dependencies - for data ingestion from various sources
ingestion = [
    "rapidfuzz>=3.13.0",
    "aiokafka>=0.10.0",
    "fastapi>=0.120",
    "prometheus-fastapi-instrumentator>=7.0",
    "uvicorn",
    "python-multipart>=0.0.18",
    "orjson>=3.11",
    "pyjwt",
    "pyyaml>=6.0",
    "psycopg2-binary>=2.9.9"
]

# Scraping module dependencies
scraping = [
    "google-auth>=2.24.0",
    "rapidfuzz>=3.13.0",
    "tqdm>=4.67",
    "gspread",
    "oauth2client",
    "beautifulsoup4>=4.13.4",
    "selenium>=4.33.0",
    "PyPDF2>=3.0.1",
    "lxml>=6.0.1",
]

# Scraping module dependencies
scraping_playwright = [
    "google-auth>=2.24.0",
    "gspread",
    "oauth2client",
    "PyPDF2>=3.0.1",
    "lxml>=6.0.1",
    "playwright>=1.49.0",
    "google-api-python-client>=2.157.0",
    "pyyaml>=6.0",
    "psycopg2-binary>=2.9.9",
    "celery>=5.4.0",
]

external_api = [
    "bcrypt>=3.2.2",
    "fastapi>=0.120",
    "fastapi-pagination>=0.14",
    "prometheus-fastapi-instrumentator>=7.0",
    "uvicorn[standard]>=0.23.2",
    "python-jose[cryptography]>=3.3.0",
    "python-multipart>=0.0.6",
    "passlib[bcrypt]>=1.7.4",
    "pydantic[email]>=2.12",
    "asyncpg>=0.30",
    "psycopg2-binary>=2.9.9",
    "redis>=6.0.0",
    "python-dotenv>=1.0.0",
    "structlog>=23.1.0", # Logging structurÃ©"
    "oauth2client>=4.1.3", # Pour l'authentification avec Google API "
    "boto3>=1.36", # S3 integration",
    "pandas>=2.2", # Data analysis",
    "joblib>=1.3.2", # Joblib for machine learning"
    "asteval>=1.0.6",
    "gspread>=6.1.4",
    "streamlit>=1.35.0",
    "sqlmodel>=0.0.24",
    "rapidfuzz>=3.13.0",
    "pyspark>=3.5.5",
    "scikit-learn>=1.5.1",
    "greenlet>=2.0.1",
    "aiohttp>=3.13",
    "celery>=5.4.0",
]

migrations = [
    "alembic>=1.17",
    "streamlit>=1.35.0",
    "aiosmtplib>=1.2.0",
    "sqlmodel>=0.0.24"
]

visualization = [
    "streamlit>=1.28.0",
    "pyarrow>=12.0.0",
    "pyspark>=3.4.0",
    "pyyaml>=6.0",
    "fastapi>=0.100.0",
    "prometheus-fastapi-instrumentator>=7.0",
    "plotly>=5.19.0",
    "scipy>=1.10.1",
    "gspread",
    "psycopg2-binary>=2.9.9",
    "asyncpg>=0.30",
    "sqlalchemy>=2.0.0",
    "pydantic>=2.12",
    "pydantic-settings>=2.11",
]


orchestration = [
    "dagster>=1.11.13",
    "dagster-aws>=0.27.13",
    "dagster-duckdb>=0.27.13",
    "dagster-k8s>=0.27.13",
    "dagster-postgres>=0.27.13",
    "dagster-slack>=0.27.13",
    "dagster-webserver",
    "dagster-dg-cli",
]

[dependency-groups]
# Full development environment with all dependencies
dev = [
    "data_ev[activation,transform,eda,load,results,ingestion,external_api,migrations,orchestration]",
    "pytest",
    "pytest-asyncio>=1.2.0",
    "pytest-cov>=6.0.0",
    "httpx>=0.25.0",
    "ruff",
    "nbstripout",
    "fakeredis>=2.31.3",
    "py-spy>=0.4.1",
    "ipykernel",
]

local = ["pyspark>=4.0.1", "jupyterlab"]

[tool.setuptools.packages.find]
where = ["src"]

[tool.dg]
directory_type = "project"

[tool.dg.project]
root_module = "bib_dagster"
registry_modules = [
    "bib_dagster.components.*",
]


[tool.ruff]
target-version = "py311"
line-length = 88
extend-exclude = ["*.ipynb", "*.csv"]

[tool.ruff.lint]
select = [
    "E", # pycodestyle (error)
    "F", # pyflakes
    "B", # bugbear
    "B9",
    "C4", # flake8-comprehensions
    "SIM", # flake8-simplify
    "I", # isort
    "UP", # pyupgrade
    "PIE", # flake8-pie
    "PGH", # pygrep-hooks
    "PYI", # flake8-pyi
    "RUF",
]

ignore = [
    # only relevant if you run a script with `python -0`,
    # which seems unlikely for any of the scripts in this repo
    "B011",
    # B008: Do not perform function call in argument defaults
    # Disabled for FastAPI Depends() which is designed to work this way
    "B008",
    # Leave it to the formatter to split long lines and
    # the judgement of all of us.
    "E501"
]

[tool.ruff.lint.per-file-ignores]
"__init__.py" = ["F401"]

[tool.ruff.lint.isort]
known-first-party = ["core", "activation", "transform", "eda", "load", "web", "results", "external_api", "visualization"]

[tool.pytest.ini_options]
testpaths = ["tests"]
#pythonpath = ["src"]
python_files = ["test_*.py"]
python_classes = ["Test*"]
python_functions = ["test_*"]
asyncio_mode = "auto"
asyncio_default_fixture_loop_scope = "function"
addopts = [
    "-v",
    "--tb=short",
    "--strict-markers",
    "--import-mode=importlib",
    "-m", "not integration",  # Skip integration tests by default
]
markers = [
    "asyncio: mark test as async",
    "integration: mark test as integration test (requires external APIs, database, S3, etc.)",
    "unit: mark test as unit test (self-sufficient, no external dependencies)",
]
filterwarnings = [
    "ignore::DeprecationWarning",
]

[tool.coverage.run]
relative_files = true
