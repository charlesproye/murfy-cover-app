{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# number of unique vins through out the transform pipeline\n",
    "The goal of this notebook is to check where vins are \"lost\" in the pipeline.  \n",
    "To make our estimations as acurrate as possible we have to prune out some parts of the data.  \n",
    "Sometimes this removes data of the entirety of the life of a vehicle.  \n",
    "This in turn, causes the monitor website to have no results for those vehicles. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from core.sql_utils import * \n",
    "from core.pandas_utils import * \n",
    "from transform.fleet_info.main import fleet_info\n",
    "from pyspark.sql import functions as F\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from core.s3.s3_utils import S3Service, S3Settings\n",
    "from core.spark_utils import create_spark_session\n",
    "settings = S3Settings()\n",
    "\n",
    "spark = create_spark_session(\n",
    "    settings.S3_KEY,\n",
    "    settings.S3_SECRET\n",
    ")\n",
    "\n",
    "s3 = S3Service()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pipeline analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAKES = [\"bmw\", ]#\"ford\", \"kia\", \"mercedes_benz\", \"renault\", \"tesla_fleet_telemetry\", \"volvo_cars\", \"volkswagen\"] # \"tesla\", \"opel\" , \"ds\", "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(fleet_info[\"vin\"].nunique())\n",
    "fleet_info_spark = spark.createDataFrame(fleet_info.rename(columns={\"vin\": \"VIN\"}))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nunique_vins_in_raw_tss = Series({make: s3.read_parquet_df_spark(spark, f'raw_ts/{make}/time_series/raw_ts_spark.parquet').join(fleet_info_spark, on=\"VIN\", how=\"inner\").select(F.countDistinct(\"VIN\").alias(\"unique_vin_count\")).collect()[0][0] for make in MAKES}    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nunique_vins_in_result_phases = Series({make: s3.read_parquet_df_spark(spark, f'result_phases/result_phases_{make}.parquet').join(fleet_info_spark, on=\"VIN\", how=\"inner\").select(F.countDistinct(\"VIN\").alias(\"unique_vin_count\")).collect()[0][0] for make in MAKES}  )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nunique_vins_in_processed_phases = Series({make: s3.read_parquet_df_spark(spark, f\"processed_phases/processed_phases_{make}.parquet\").join(fleet_info_spark, on=\"VIN\", how=\"inner\").select(F.countDistinct(\"VIN\").alias(\"unique_vin_count\")).collect()[0][0] for make in MAKES})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from core.sql_utils import *\n",
    "engine = get_sqlalchemy_engine()\n",
    "with engine.connect() as con:\n",
    "    dbeaver_df = pd.read_sql(text(\"\"\"SELECT * from vehicle_data\n",
    "        LEFT join vehicle on vehicle.id = vehicle_data.vehicle_id\n",
    "        LEFT join vehicle_model on vehicle.vehicle_model_id = vehicle_model.id\n",
    "        LEFT join oem on vehicle_model.oem_id = oem.id\n",
    "        LEFT join fleet on vehicle.fleet_id = fleet.id;\"\"\"), con)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nunique_vins_in_vehicle_data = dbeaver_df.groupby(\"oem_name\")[\"vin\"].nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_vin_counts = pd.concat(\n",
    "    {\n",
    "        \"vehicle\": fleet_info.groupby(\"make\")[\"vin\"].nunique(),\n",
    "        \"raw_tss\": nunique_vins_in_raw_tss,\n",
    "        \"processed_phases\": nunique_vins_in_processed_phases,\n",
    "        \"results_phases\": nunique_vins_in_result_phases,\n",
    "        \"vehicle_data\": nunique_vins_in_vehicle_data,\n",
    "    },\n",
    "    axis=\"columns\",\n",
    ")\n",
    "unique_vin_counts.loc[\"total\"] = unique_vin_counts.sum().astype(\"int\")\n",
    "unique_vin_counts"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "data_ev",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

