{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DATAEV-291 new tss processing method comparaison with previous one\n",
    "After identifying some issues in the Tesla results, we realized that some of them came from the tss processing step.  \n",
    "This notebook aims at evaluating the impact of this new tss processing method on the final soh estimation. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "\n",
    "import plotly.express as px\n",
    "\n",
    "from core.pandas_utils import *\n",
    "from core.stats_utils import *\n",
    "from core.plt_utils import scatter_and_arrow_fig\n",
    "from transform.fleet_info.main import fleet_info\n",
    "from transform.raw_results.tesla_results import get_results\n",
    "from transform.processed_tss.ProcessedTimeSeries import TeslaProcessedTimeSeries\n",
    "from transform.raw_results.config import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "legacy_results = pd.read_parquet(\"./data_cache/tesla_legacy_results.parquet\")\n",
    "# As if writing this notebook, the vehilce table in the DB is faulty so we relly on a tesla results backup to  \n",
    "fake_fleet_info = legacy_results.groupby(\"vin\", observed=True, as_index=False)[[\"capacity\", \"tesla_code\"]].first()\n",
    "display(sanity_check(fake_fleet_info))\n",
    "\n",
    "def get_results() -> DF:\n",
    "    logger.info(\"Processing raw tesla results.\")\n",
    "    return (\n",
    "        TeslaProcessedTimeSeries(\"tesla\", columns=TESLA_USE_COLS, filters=[(\"trimmed_in_charge\", \"==\", True)])\n",
    "        .groupby([\"vin\", \"trimmed_in_charge_idx\"], observed=True)\n",
    "        .agg(\n",
    "            energy_added_min=pd.NamedAgg(\"charge_energy_added\", \"min\"),\n",
    "            energy_added_end=pd.NamedAgg(\"charge_energy_added\", \"last\"),\n",
    "            soc_diff=pd.NamedAgg(\"soc\", series_start_end_diff),\n",
    "            inside_temp=pd.NamedAgg(\"inside_temp\", \"mean\"),\n",
    "            # capacity=pd.NamedAgg(\"capacity\", \"first\"),\n",
    "            odometer=pd.NamedAgg(\"odometer\", \"first\"),\n",
    "            # version=pd.NamedAgg(\"version\", \"first\"),\n",
    "            size=pd.NamedAgg(\"soc\", \"size\"),\n",
    "            # model=pd.NamedAgg(\"model\", \"first\"),\n",
    "            date=pd.NamedAgg(\"date\", \"first\"),\n",
    "            charging_power=pd.NamedAgg(\"charging_power\", \"median\"),\n",
    "            # tesla_code=pd.NamedAgg(\"tesla_code\", \"first\"),\n",
    "        )\n",
    "        .merge(fake_fleet_info, \"left\", \"vin\")\n",
    "        .eval(\"energy_added = energy_added_end - energy_added_min\")\n",
    "        .eval(\"soh = energy_added / (soc_diff / 100.0 * capacity)\")\n",
    "        # .query(\"soc_diff > 40 & soh.between(0.75, 1.05)\")\n",
    "        .eval(\"level_1 = soc_diff * (charging_power < @LEVEL_1_MAX_POWER) / 100\")\n",
    "        .eval(\"level_2 = soc_diff * (charging_power.between(@LEVEL_1_MAX_POWER, @LEVEL_2_MAX_POWER)) / 100\")\n",
    "        .eval(\"level_3 = soc_diff * (charging_power > @LEVEL_2_MAX_POWER) / 100\")\n",
    "\t    .eval(\"bottom_soh = soh.between(0.75, 0.9)\")\n",
    "        .eval(\"fixed_soh_min_end = soh.mask(tesla_code == 'MTY13', soh / 0.96)\")\n",
    "        .eval(\"fixed_soh_min_end = fixed_soh_min_end.mask(bottom_soh & tesla_code == 'MTY13', fixed_soh_min_end + 0.08)\")\n",
    "        .eval(\"soh = fixed_soh_min_end\")\n",
    "        .sort_values([\"tesla_code\", \"vin\", \"date\"])\n",
    "    )\n",
    "\n",
    "new_raw_results = get_results()\n",
    "new_results = new_raw_results.query(\"soc_diff > 40 & soh.between(0.75, 1.05)\")\n",
    "sanity_check(new_raw_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_results = (\n",
    "    pd.concat({\"new_results\":new_results, \"legacy_results\":legacy_results}, names=[\"res_type\", \"range_index\"])\n",
    "    .reset_index(level=0)\n",
    "    .reset_index(drop=True)\n",
    "    .eval(\"bottom_MT336 = tesla_code == 'MT336' & soh < 0.87\")\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Comparaison"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visaulize comparaison\n",
    "While it will be hard to see minor differences, let's make sure there are no wild differences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "px.scatter(\n",
    "    all_results,\n",
    "    x=\"odometer\",\n",
    "    y=\"soh\",\n",
    "    color=\"tesla_code\",\n",
    "    # symbol=\"res_type\",\n",
    "    facet_row=\"res_type\",\n",
    "    # symbol_map={\"legacy_results\": \"cross\", \"new_results\": \"square\"},\n",
    "    opacity=0.3,\n",
    "    height=700,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Quantitative comparaison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.concat({\n",
    "    \"new_results\": evaluate_soh_estimations(new_results, [\"soh\"]),\n",
    "    \"legacy_results\": evaluate_soh_estimations(legacy_results, [\"soh\"]),\n",
    "})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Coverage comparaison\n",
    "One of the issues that the previous tss processing method had was that some charges were split up with different `trimmed_in_charge_idx`.  \n",
    "This in turn would filter out charges that were marked with a lower soc_diff than the real soc_diff, because we have a `soc_diff > 40` query line.  \n",
    "Let's see if the final coverage is better.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To know what is the coverage difference with the previous method we will get all the unique vins in the7 time series as the vehicle RDB table is currently faulty.\n",
    "unique_vins = TeslaProcessedTimeSeries(columns=[\"vin\"])[\"vin\"].pipe(uniques_as_series)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(unique_vins.isin(new_results[\"vin\"].pipe(uniques_as_series)).value_counts(dropna=False, normalize=True))\n",
    "display(unique_vins.isin(legacy_results[\"vin\"].pipe(uniques_as_series)).value_counts(dropna=False, normalize=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The final cache is actually worse..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "It seems like the new tss processing is equal if not worse than the legacy one that's okay it's not like I spent a ~month on it..."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "data_ev",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

