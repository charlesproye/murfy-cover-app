{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Improve time series processing\n",
    "Most of the current code for processing time series is legacy code from watea POC.  \n",
    "It works but is not optimized and is not scalable.  \n",
    "This is because it was concieved to be used on a single time series at a time.  \n",
    "We need to change this to be able to process multiple time series in a single DataFrame.  \n",
    "This will hopefully improve the performance and scalability of the code.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "from pandas import Timedelta as TD\n",
    "from rich.progress import Progress\n",
    "import plotly.express as px\n",
    "\n",
    "from core.logging_utils import set_level_of_loggers_with_prefix\n",
    "from core.pandas_utils import *\n",
    "from core.time_series_processing import *\n",
    "from transform.raw_tss.main import get_raw_tss\n",
    "from transform.processed_tss.config import *\n",
    "from transform.fleet_info.main import fleet_info\n",
    "\n",
    "logger = getLogger(\"transform.processed_tss.tesla\")\n",
    "set_level_of_loggers_with_prefix(\"DEBUG\", \"transform.processed_tss.tesla\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TEST_BRAND = \"renault\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_tss = get_raw_tss(TEST_BRAND)\n",
    "\n",
    "if TEST_BRAND == \"tesla\":\n",
    "    VINS = [\n",
    "        \"LRW3E7EK5PC797921\",\n",
    "        \"5YJ3E7EA9LF751886\",\n",
    "        \"XP7YGCEL2RB413022\",\n",
    "        \"LRW3E7FR7NC480876\",\n",
    "        \"XP7YGCES9RB442881\",\n",
    "    ]\n",
    "else:\n",
    "    all_vins = raw_tss[\"vin\"].unique()\n",
    "    VINS = random.sample(list(all_vins), 5)\n",
    "raw_tss_test = raw_tss.query(\"vin in @VINS\")\n",
    "display(raw_tss_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Legacy code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I copied pasted the legacy code here for future references once the new implementation will have replaced it.  \n",
    "I also added comments to the code to explain what it does(wrong).  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def legacy_tesla_processed_tss(raw_tss:DF) -> DF:\n",
    "    with Progress(transient=True) as progress:\n",
    "        task = progress.add_task(\"Processing VINs...\", visible=False, total=raw_tss[\"vin\"].nunique())\n",
    "        return (\n",
    "            raw_tss\n",
    "            .rename(columns=RENAME_COLS_DICT, errors=\"ignore\")\n",
    "            .pipe(safe_locate, col_loc=list(COL_DTYPES.keys()), logger=logger)\n",
    "            .pipe(safe_astype, COL_DTYPES, logger=logger)\n",
    "            # We should probably not drop duplicates as there might be multiple measurements for the same date\n",
    "            # We should probobably check the responses parsing and switch from concatenating to joining/merging on date instead.\n",
    "            .drop_duplicates(subset=[\"vin\", \"date\"]) \n",
    "            .sort_values(by=[\"vin\", \"date\"])\n",
    "            .pipe(legacy_charge_n_discharging, \"vin\", CHARGING_STATUS_VAL_TO_MASK, logger) \n",
    "            .groupby(\"vin\")\n",
    "            .apply(legacy_tesla_process_ts, progress, task, include_groups=False)\n",
    "            .reset_index(drop=False)\n",
    "            .pipe(set_all_str_cols_to_lower, but=[\"vin\"])\n",
    "            .pipe(left_merge, fleet_info.dropna(subset=[\"vin\"]), \"vin\", \"vin\", COLS_TO_CPY_FROM_FLEET_INFO, logger)\n",
    "            .pipe(compute_discharge_diffs, DISCHARGE_VARS_TO_MEASURE, logger)\n",
    "        )\n",
    "\n",
    "def legacy_tesla_process_ts(raw_ts: DF, progress: Progress, task) -> DF:\n",
    "    vin = raw_ts.name\n",
    "    progress.update(task, visible=True, advance=1, description=f\"Processing vin {vin}...\")\n",
    "    if progress.finished:\n",
    "        progress.update(task, visible=False)\n",
    "    return (\n",
    "        raw_ts\n",
    "        .assign(\n",
    "            # We don't use any of these variables later in the pipeline so we can drop them\n",
    "            ffiled_outside_temp=raw_ts[\"outside_temp\"].ffill(),\n",
    "            ffiled_inside_temp=raw_ts[\"inside_temp\"].ffill(),\n",
    "            floored_soc=floor_to(raw_ts[\"soc\"].ffill(), 1),\n",
    "            date_diff=raw_ts[\"date\"].diff(),\n",
    "            soc_diff=raw_ts[\"soc\"].diff(),\n",
    "        )\n",
    "        .pipe(compute_cum_var, power_col=\"power\", cum_energy_col=\"cum_energy\")\n",
    "        # The only column we actually use from this function is cum_charge_energy_added from charger_power\n",
    "        # Instead of doing a groupby/apply we can perform a single compute_cum_energy call\n",
    "        # And then compute some sort of energy_added_offset that resets the results to zero at the start of each vin time series.\n",
    "        .pipe(compute_cum_var, power_col=\"charger_power\", cum_energy_col=\"cum_charge_energy_added\")\n",
    "        .assign(energy_added=lambda tss: tss[\"cum_charge_energy_added\"].diff())\n",
    "        .assign(energy_diff=lambda df: df[\"cum_energy\"].diff())\n",
    "        .pipe(fillna_vars, COLS_TO_FILL, MAX_TIME_DIFF_TO_FILL)\n",
    "    )\n",
    "\n",
    "def legacy_charge_n_discharging(tss:DF, id_col:str=None, charging_status_val_to_mask:dict=None, logger:Logger=logger) -> DF:\n",
    "    \"\"\"\n",
    "    ### Description:\n",
    "    Computes the charging and discharging masks for a time series.\n",
    "    Uses the string charging_status column if it exists, otherwise uses the soc difference.\n",
    "    ### Parameters:\n",
    "    id_col: optional parameter to provide if the dataframe represents multiple time series.\n",
    "    charging_status_val_to_mask: dict mapping charging status values to boolean values to create masks.\n",
    "    \"\"\"\n",
    "    logger.info(f\"compute_charging_n_discharging_masks called.\")\n",
    "    if \"charging_status\" in tss.columns and charging_status_val_to_mask is not None:\n",
    "        logger.debug(f\"Computing charging and discharging masks using charging status dictionary.\")\n",
    "        charge_mask = tss[\"charging_status\"].map(charging_status_val_to_mask)\n",
    "        tss[\"in_charge\"] = charge_mask\n",
    "        tss[\"in_discharge\"] = charge_mask == False\n",
    "        if id_col is not None and id_col in tss.columns:\n",
    "            tss = (\n",
    "                tss\n",
    "                .groupby(id_col)\n",
    "                .apply(compute_charge_n_discharge_perf_mask_and_idx_from_masks)\n",
    "                .reset_index(drop=True)\n",
    "            )\n",
    "        else:\n",
    "            tss = compute_charge_n_discharge_perf_mask_and_idx_from_masks(tss)\n",
    "        return tss\n",
    "    elif \"soc\" in tss.columns:\n",
    "        logger.debug(f\"Computing charging and discharging masks using soc difference.\")\n",
    "        if id_col in tss.columns:\n",
    "            return (\n",
    "                tss\n",
    "                .groupby(id_col)\n",
    "                .apply(low_freq_compute_charge_n_discharge_vars)\n",
    "                .reset_index(drop=True)\n",
    "            )\n",
    "        else:\n",
    "            return low_freq_compute_charge_n_discharge_vars(tss)\n",
    "    else:\n",
    "        logger.warning(\"No charging status or soc column found to compute charging and discharging masks, returning original tss.\")\n",
    "        return tss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## New implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_TD = TD(hours=1, minutes=30)\n",
    "\n",
    "class NewTimeSeriesProcessing:\n",
    "    def __init__(self, name:str, id_col:str=\"vin\"):\n",
    "        self.name = name\n",
    "        self.logger = getLogger(f\"transform.processed_tss.{name}\")\n",
    "        self.id_col = id_col\n",
    "\n",
    "    def new_process_raw_tss(self, raw_tss:DF) -> DF:\n",
    "        return (\n",
    "            raw_tss\n",
    "            .rename(columns=RENAME_COLS_DICT, errors=\"ignore\")\n",
    "            .pipe(safe_locate, col_loc=list(COL_DTYPES.keys()), logger=logger)\n",
    "            .pipe(safe_astype, COL_DTYPES, logger=logger)\n",
    "            .sort_values(by=[\"vin\", \"date\"])\n",
    "            .pipe(set_all_str_cols_to_lower, but=[\"vin\"])\n",
    "            .pipe(self.compute_date_vars)\n",
    "            .pipe(self.compute_charge_n_discharge_masks, IN_CHARGE_CHARGING_STATUS_VALS, IN_DISCHARGE_CHARGING_STATUS_VALS)\n",
    "            .pipe(self.perf_masks_and_idx_from_condition_mask, [\"in_charge\", \"in_discharge\"], MAX_TD)\n",
    "            .pipe(self.compute_cum_var, \"power\", \"cum_energy\")\n",
    "            .pipe(self.compute_cum_var, \"charger_power\", \"cum_charge_energy_added\")\n",
    "            .merge(fleet_info, on=\"vin\", how=\"left\")\n",
    "        )\n",
    "\n",
    "    def compute_cum_var(self, tss: DF, var_col:str, cum_var_col:str) -> DF:\n",
    "        if var_col in tss.columns:\n",
    "            self.logger.debug(f\"Computing {cum_var_col} from {var_col}.\")\n",
    "            tss[cum_var_col] = (\n",
    "                cumulative_trapezoid(\n",
    "                    # Leave the keywords as default order is y x not x y (-_-)\n",
    "                    # Make sure that date time units are in seconds before converting to int\n",
    "                    x=tss[\"date\"].dt.as_unit(\"s\").astype(int),\n",
    "                    y=tss[var_col].fillna(0).values,\n",
    "                    initial=0,\n",
    "                )            \n",
    "                .astype(\"float32\")\n",
    "            )\n",
    "            tss[cum_var_col] *= KJ_TO_KWH # Convert from kj to kwh\n",
    "            # Reset value to zero at the start of each vin time series\n",
    "            tss[cum_var_col] -= tss.groupby(self.id_col)[cum_var_col].transform(\"first\")\n",
    "        else:\n",
    "            self.logger.debug(f\"{var_col} not found, not computing {cum_var_col}.\")\n",
    "        return tss\n",
    "\n",
    "    def compute_date_vars(self, tss:DF) -> DF:\n",
    "        self.logger.debug(f\"Computing sec_date and sec_date_diff.\")\n",
    "        tss[\"time_diff\"] = tss.groupby(self.id_col)[\"date\"].diff()\n",
    "        tss[\"sec_time_diff\"] = tss[\"time_diff\"].dt.total_seconds()\n",
    "        return tss\n",
    "\n",
    "    def compute_charge_n_discharge_masks(self, tss:DF, in_charge_vals:list, in_discharge_vals:list) -> DF:\n",
    "        self.logger.debug(f\"Computing charging and discharging masks.\")\n",
    "        #if \"charging_status\" in tss.columns:\n",
    "        #    return new_charge_n_discharging_from_charging_status(tss, in_charge_vals, in_discharge_vals, logger)\n",
    "        #elif \"soc\" in tss.columns:\n",
    "        return self.new_charge_n_discharging_from_soc_diff(tss)\n",
    "        #else:\n",
    "        #    raise ValueError(\"No charging status or soc column found to compute charging and discharging masks.\")\n",
    "\n",
    "    def new_charge_n_discharging_from_soc_diff(self, tss:DF) -> DF:\n",
    "        tss_grp = tss.groupby(self.id_col)\n",
    "        tss[\"ffilled_soc\"] = tss_grp[\"soc\"].ffill()\n",
    "        tss[\"soc_dir\"] = tss_grp[\"ffilled_soc\"].diff()\n",
    "        tss[\"soc_dir\"] = self.norm_soc_dir(tss[\"soc_dir\"])\n",
    "        # mitigate soc spikes effect on mask\n",
    "        tss[\"prev_dir\"] = tss_grp[\"soc_dir\"].ffill()\n",
    "        tss[\"prev_dir\"] = tss_grp[\"prev_dir\"].shift()\n",
    "        tss[\"next_dir\"] = tss_grp[\"soc_dir\"].bfill()\n",
    "        tss[\"next_dir\"] = tss_grp[\"next_dir\"].shift(-1)\n",
    "        tss[\"value_is_spike\"] = tss.eval(\"(next_dir == prev_dir) & (soc_dir != next_dir) & soc_dir.notna()\")\n",
    "        tss[\"soc_dir\"] = tss[\"soc_dir\"].mask(tss[\"value_is_spike\"], np.nan)\n",
    "        def rolling_mean_soc_dir(ts:DF) -> Series:\n",
    "            return (\n",
    "                ts\n",
    "                .rolling(window=TD(minutes=20), center=True, on=\"date\")\n",
    "                [\"soc_dir\"]\n",
    "                .mean()\n",
    "                .pipe(self.norm_soc_dir)\n",
    "                .reset_index(drop=True)\n",
    "            )\n",
    "\n",
    "        tss[\"smoothed_soc_dir\"] = tss_grp.transform(rolling_mean_soc_dir)\n",
    "        #tss[\"soc_dir\"] = (\n",
    "        #    tss[\"soc_dir\"]\n",
    "        #    .mask(tss[\"smoothed_soc_dir\"].gt(0) & tss[\"soc_dir\"].lt(0), np.nan)\n",
    "        #    .mask(tss[\"smoothed_soc_dir\"].lt(0) & tss[\"soc_dir\"].gt(0), np.nan)\n",
    "        #)\n",
    "\n",
    "        bfilled_dir = tss[\"soc_dir\"].bfill()\n",
    "        ffilled_dir = tss[\"soc_dir\"].ffill()\n",
    "        tss[\"soc_dir\"] = tss[\"soc_dir\"].mask(bfilled_dir == ffilled_dir, ffilled_dir)\n",
    "        tss = tss.eval(\"in_discharge = soc_dir == -1\")\n",
    "        tss = tss.eval(\"in_charge = soc_dir == 1\")\n",
    "        #tss = tss.drop(columns=[\"smoothed_soc_dir\", \"ffilled_soc\", \"value_is_spike\"])\n",
    "        return tss\n",
    "\n",
    "    def norm_soc_dir(self, soc_dir:Series) -> Series:\n",
    "        \"\"\"Normalize the soc direction to -1 for negative, NaN for zero, 1 for positive.\"\"\"\n",
    "        return soc_dir / soc_dir.abs()\n",
    "\n",
    "    def new_charge_n_discharging_from_charging_status(self, tss:DF, in_charge_vals:list, in_discharge_vals:list) -> DF:\n",
    "        self.logger.debug(f\"Computing charging and discharging vars using charging status dictionary.\")\n",
    "        return (\n",
    "            tss\n",
    "            .eval(f\"in_charge = charging_status in {in_charge_vals}\")\n",
    "            .eval(f\"in_discharge = charging_status in {in_discharge_vals}\")\n",
    "        )\n",
    "\n",
    "    def perf_masks_and_idx_from_condition_mask(self, tss:DF, masks:list[str], max_time_diff:TD=None) -> DF:\n",
    "        self.logger.info(f\"Trimming off trailing soc of {masks} masks.\")\n",
    "        for mask in masks:\n",
    "            tss = self.compute_idx_from_mask(tss, mask, max_time_diff)\n",
    "            trailing_soc = tss.groupby([self.id_col, f\"{mask}_idx\"])[\"soc\"].transform(\"last\")\n",
    "            leading_soc = tss.groupby([self.id_col, f\"{mask}_idx\"])[\"soc\"].transform(\"first\")\n",
    "            tss[f\"{mask}_perf\"] = tss[mask] & (tss[\"soc\"] != trailing_soc) & (tss[\"soc\"] != leading_soc)\n",
    "            tss = self.compute_idx_from_mask(tss, f\"{mask}_perf\", max_time_diff)\n",
    "        return tss\n",
    "\n",
    "    def compute_idx_from_mask(self, tss: DF, mask:str, max_time_diff:TD=None) -> DF:\n",
    "        self.logger.debug(f\"Computing {mask}_idx from {mask} mask.\")\n",
    "        idx_col_name = f\"{mask}_idx\"\n",
    "        shifted_mask = tss.groupby(self.id_col)[mask].shift(fill_value=False)\n",
    "        tss[\"new_period_start_mask\"] = shifted_mask.ne(tss[mask]) \n",
    "        if max_time_diff is not None:\n",
    "            tss[\"new_period_start_mask\"] |= (tss[\"time_diff\"] > max_time_diff)\n",
    "        tss[idx_col_name] = tss.groupby(self.id_col)[\"new_period_start_mask\"].cumsum().astype(\"uint16\")\n",
    "        tss.drop(columns=[\"new_period_start_mask\"], inplace=True)\n",
    "        return tss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tss_test = NewTimeSeriesProcessing(TEST_BRAND).new_process_raw_tss(raw_tss_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#tss = (\n",
    "#    tss\n",
    "#    .eval(\"charging_status = charging_status.str.lower()\")\n",
    "#    .eval(f\"in_charge_charging_status = charging_status in {IN_CHARGE_CHARGING_STATUS_VALS}\")\n",
    "#    .eval(f\"in_discharge_charging_status = charging_status in {IN_DISCHARGE_CHARGING_STATUS_VALS}\")\n",
    "#    .eval(\"in_charge_acc = in_charge == in_charge_charging_status\")\n",
    "#    .eval(\"in_discharge_acc = in_discharge == in_discharge_charging_status\")\n",
    "#)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "px.scatter(\n",
    "    tss_test.eval(\"smoothed_dir_soc = smoothed_soc_dir.fillna(0)\"), #.eval(\"charging_status = charging_status.fillna('Unknown')\"),\n",
    "    x=\"date\",\n",
    "    y=\"soc\",\n",
    "    color=\"in_charge_perf\",\n",
    "    color_continuous_scale=\"Rainbow\",\n",
    "    symbol=\"smoothed_soc_dir\",\n",
    "    hover_data=\"soc_dir\",\n",
    "    facet_row=\"vin\",\n",
    ").update_layout(height=1000, showlegend=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "We have a new implementation that is more scalable and performant, it took ~2 minute to process a 5 million lines dataframe with 13k unique VINs.  \n",
    "Some way some how I wasn't able to reimplement an soc NaN checking to prevent the soc diff from being computed on NaN values ffilled socs that come from from a data point to far away in time.  \n",
    "Also I couldn't reuse the rolling window logic from the legacy code to remove soc spikes.  \n",
    "While this is not a problem for the current data, it will be a problem for future data that will have more noise to it if we deal with Watea like data (like Ituran(-_-)).  "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "data_ev",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

