{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1",
   "metadata": {},
   "source": [
    "## Notebook pour tester le pipe de fleet-telemetry"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2",
   "metadata": {},
   "source": [
    "### Raw tss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from core.s3_utils import *\n",
    "from transform.raw_tss.fleet_telemetry_raw_tss import *\n",
    "import pandas as pd\n",
    "import plotly.express as px\n",
    "from transform.processed_tss.config import *\n",
    "from core.constants import *\n",
    "from transform.raw_tss.config import *\n",
    "from transform.processed_tss.ProcessedTimeSeries import ProcessedTimeSeries\n",
    "from transform.raw_results.tesla_fleet_telemetry import get_results as get_results_origin\n",
    "from transform.processed_results.main import get_processed_results\n",
    "from datetime import timedelta\n",
    "\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4",
   "metadata": {},
   "outputs": [],
   "source": [
    "S3_RAW_TSS_KEY_FORMAT = \"raw_ts/{brand}/time_series/raw_tss.parquet\"\n",
    "FLEET_TELEMETRY_RAW_TSS_KEY = S3_RAW_TSS_KEY_FORMAT.format(brand=\"tesla-fleet-telemetry\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5",
   "metadata": {},
   "outputs": [],
   "source": [
    "s3 = S3_Bucket()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_response_keys_to_parse(bucket:S3_Bucket) -> DF:\n",
    "    if bucket.check_file_exists(FLEET_TELEMETRY_RAW_TSS_KEY):\n",
    "        raw_tss_subset, _ = bucket.read_parquet_df_ter(FLEET_TELEMETRY_RAW_TSS_KEY, columns=[\"vin\", \"readable_date\"])\n",
    "    else:\n",
    "        raw_tss_subset = DEFAULT_TESLA_RAW_TSS_DF\n",
    "    last_parsed_date = (\n",
    "        raw_tss_subset\n",
    "        .groupby(\"vin\", observed=True)\n",
    "        # Use \"max\" instead of \"last\" as the keys are not sorted\n",
    "        .agg(last_parsed_date=pd.NamedAgg(\"readable_date\", \"max\"))\n",
    "    ).compute()\n",
    "    return (\n",
    "        bucket.list_responses_keys_of_brand(\"tesla-fleet-telemetry\")\n",
    "        .assign(date=lambda df: df[\"file\"].str[:-5].astype(\"datetime64[ns]\"))\n",
    "        .merge(last_parsed_date, \"outer\", \"vin\")\n",
    "        .query(\"last_parsed_date.isna() | date > last_parsed_date\")\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_raw_tss(bucket: S3_Bucket = S3_Bucket()) -> DF:\n",
    "    logger.debug(\"Getting raw tss from responses provided by tesla fleet telemetry.\")\n",
    "    keys = get_response_keys_to_parse(bucket)\n",
    "    if bucket.check_file_exists(FLEET_TELEMETRY_RAW_TSS_KEY):\n",
    "        raw_tss = bucket.read_parquet_df_ter(FLEET_TELEMETRY_RAW_TSS_KEY)\n",
    "        new_raw_tss = get_raw_tss_from_keys(keys, bucket)\n",
    "        return concat([raw_tss, new_raw_tss])\n",
    "    else:\n",
    "        new_raw_tss = get_raw_tss_from_keys(keys, bucket)\n",
    "        return new_raw_tss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8",
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_tss, path = s3.read_parquet_df_ter('raw_ts/tesla-fleet-telemetry/time_series/raw_tss.parquet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9",
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_tss.to_parquet(\"test_raw_tss.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10",
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_tss = dd.read_parquet('test_raw_tss.parquet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11",
   "metadata": {},
   "outputs": [],
   "source": [
    "last_parsed_date = raw_tss.groupby('vin', observed=True).agg(last_parsed_date=pd.NamedAgg(\"readable_date\", \"max\")).compute()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "keys_to_parse = s3.list_responses_keys_of_brand(\"tesla-fleet-telemetry\").assign(date=lambda df: df[\"file\"].str[:-5].astype(\"datetime64[ns]\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13",
   "metadata": {},
   "outputs": [],
   "source": [
    "keys_to_parse.date.max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14",
   "metadata": {},
   "outputs": [],
   "source": [
    "keys = keys_to_parse.merge(last_parsed_date, \"outer\", \"vin\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15",
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_tss['readable_date'].max().compute()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16",
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_tss = []\n",
    "grouped = keys.groupby(pd.Grouper(key='date', freq='W-MON'))\n",
    "grouped_items = list(grouped)[:1]\n",
    "for week, week_keys in track(grouped_items, description=\"Processing weekly groups\"):\n",
    "    week_date = week.date().strftime('%Y-%m-%d')\n",
    "    logger.debug(f\"Parsing the responses of the week {week_date}:\")\n",
    "    logger.debug(f\"{len(week_keys)} keys to parse for {week_keys['vin'].nunique()} vins.\")\n",
    "    logger.debug(f\"This represents {round(len(week_keys) / len(keys) * 100)}% of the total keys to parse.\")\n",
    "    responses = s3.read_multiple_json_files(week_keys[\"key\"].tolist(), max_workers=64)\n",
    "    logger.debug(f\"Read the responses.\")\n",
    "    with ThreadPoolExecutor(max_workers=64) as executor:\n",
    "        week_raw_tss = list(executor.map(DF.from_records, responses))\n",
    "    logger.debug(f\"Parsed the responses:\")\n",
    "    week_raw_tss = pd.concat([explode_data(df) for df in week_raw_tss])\n",
    "    logger.debug(f\"Concatenated the responses into a single DF.\")\n",
    "    raw_tss.append(week_raw_tss)\n",
    "    logger.debug(\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17",
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_tss"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18",
   "metadata": {},
   "source": [
    "### Processed tss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transform.processed_tss.ProcessedTimeSeries import ProcessedTimeSeries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_charge_n_discharge_masks(tss:DF) -> DF:\n",
    "        # We use a nullable boolean Series to represnet the rows where:\n",
    "        tss[\"nan_charging\"] = (\n",
    "            Series(pd.NA, index=tss.index, dtype=\"boolean\")# We are not sure of anything.\n",
    "            .mask(tss[\"charging_status\"].isin(IN_CHARGE_CHARGING_STATUS_VALS), True)# We are sure that the vehicle is in charge.\n",
    "            .mask(tss[\"charging_status\"].isin(IN_DISCHARGE_CHARGING_STATUS_VALS), False)# We are sure that the vehicle is not in charge.\n",
    "        )\n",
    "        # If a period of uncertainty (NaN) is surrounded by equal periods of certainties (True-NaN-True or False-NaN-False),\n",
    "        # We will fill them to the value of these certainties.\n",
    "        # However there are edge cases that have multiple days of uncertainties periods (I can't find the VIN but I'm sure you can ;-) )\n",
    "        # Interestingly enough the charge_energy_added variable does not get forwared that far and gets reset to zero. \n",
    "        # This would create outliers in our charge SoH estimation as we estimate the energy_gained as the diff between the last(0) and first value of charge_energy_added.\n",
    "        # So we set a maximal uncertainty period duration over which we don't fill it.\n",
    "        tss[\"nan_date\"] = tss[\"date\"].mask(tss[\"nan_charging\"].isna())\n",
    "        tss[[\"ffill_charging\", \"ffill_date\"]] = tss.groupby(\"vin\", observed=True)[[\"nan_charging\", \"nan_date\"]].ffill()\n",
    "        tss[[\"bfill_charging\", \"bfill_date\"]] = tss.groupby(\"vin\", observed=True)[[\"nan_charging\", \"nan_date\"]].bfill()\n",
    "        nan_period_duration:Series = tss.eval(\"bfill_date - ffill_date\")\n",
    "        fill_unknown_period = tss.eval(\"ffill_charging.eq(bfill_charging) & @nan_period_duration.le(@MAX_CHARGE_TD)\")\n",
    "        tss[\"nan_charging\"] = tss[\"nan_charging\"].mask(fill_unknown_period, tss[\"ffill_charging\"])\n",
    "        # As mentioned before, the SoC oscillates at [charge_limit_soc - ~3%, charge_limit_soc] so we set these periods to NaN as well.\n",
    "        tss[\"nan_charging\"] = tss[\"nan_charging\"].mask(tss[\"soc\"] >= (tss[\"charge_limit_soc\"] - 3))\n",
    "        # Then we seperate the Series into two, more explicit, columns.\n",
    "        tss[\"in_charge\"] = tss.eval(\"nan_charging.notna() & nan_charging\")\n",
    "        tss[\"in_discharge\"] = tss.eval(\"nan_charging.notna() & ~nan_charging\")\n",
    "        return tss.drop(columns=[\"nan_charging\", \"ffill_charging\", \"bfill_charging\", \"ffill_date\", \"bfill_date\"])\n",
    "    \n",
    "def compute_enenergy_added(tss:DF) -> DF:\n",
    "        tss['charge_energy_added'] = tss['dc_charge_energy_added'].where(\n",
    "            tss['dc_charge_energy_added'].notnull() & \n",
    "            (tss['dc_charge_energy_added'] > 0), \n",
    "            tss['ac_charge_energy_added'])\n",
    "        return tss\n",
    "    \n",
    "def compute_charge_idx_bis(tss):\n",
    "\n",
    "        tss = tss.pipe(compute_enenergy_added)\n",
    "        tss_na = tss.dropna(subset=['soc']).copy()\n",
    "        tss_na['soc_diff'] = tss_na.groupby('vin', observed=True)['soc'].diff()\n",
    "        tss_na['soc_diff_rolling'] = tss_na['soc_diff'].rolling(window=5, min_periods=1).mean()\n",
    "        # Determine trend\n",
    "        tss_na['trend'] = tss_na['soc_diff_rolling'].apply(lambda x: 1 if x > 0 else -1 if x < 0 else np.nan)\n",
    "        tss_na['trend'] = tss_na['trend'].ffill()\n",
    "\n",
    "        def detect_trend_change(group):\n",
    "\n",
    "            group['prev_trend'] = group['trend'].shift(1)\n",
    "            group['prev_prev_trend'] = group['trend'].shift(2)\n",
    "            \n",
    "            group['prev_date'] = group['date'].shift(1)\n",
    "            group['time_diff_min'] = (group['date'] - group['prev_date']).dt.total_seconds() / 60\n",
    "            group['time_gap'] = group['time_diff_min'] > 60  \n",
    "\n",
    "            # Faire une sépration charge_idx et discharge_idx\n",
    "            group['trend_change'] = (\n",
    "                (((group['trend'] != group['prev_trend']) & \n",
    "                  (group['prev_trend'] == group['prev_prev_trend']) ) |\n",
    "                group['time_gap'])\n",
    "            )\n",
    "            group.loc[group.index[0:2], 'trend_change'] = False\n",
    "            return group\n",
    "\n",
    "\n",
    "        tss_na = tss_na.groupby('vin', observed=True).apply(detect_trend_change).reset_index(drop=True)\n",
    "        \n",
    "        # Compute charge id\n",
    "        tss_na['in_charge_idx'] = tss_na.groupby('vin',  observed=True)['trend_change'].cumsum()\n",
    "        tss = tss.merge(tss_na[[\"soc\", \"date\", \"vin\", 'soc_diff', 'in_charge_idx', 'trend', 'prev_trend', 'prev_prev_trend', 'trend_change',]], \n",
    "                        on=[\"soc\", \"date\", \"vin\"], how=\"left\")\n",
    "        tss[[\"odometer\",\"in_charge_idx\"]] = tss[[\"odometer\", \"in_charge_idx\"]].ffill()\n",
    "        return tss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21",
   "metadata": {},
   "outputs": [],
   "source": [
    "from core.pandas_utils import safe_locate, safe_astype, str_lower_columns\n",
    "from transform.fleet_info.main import fleet_info "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22",
   "metadata": {},
   "outputs": [],
   "source": [
    "make = \"tesla-fleet-telemetry\"\n",
    "def run(tss):\n",
    "    tss = tss.rename(columns=RENAME_COLS_DICT, errors=\"ignore\")\n",
    "    tss = tss.pipe(safe_locate, col_loc=list(COL_DTYPES.keys()), logger=logger)\n",
    "    tss = tss.pipe(safe_astype, COL_DTYPES, logger=logger)\n",
    "    tss = tss.pipe(normalize_units_to_metric)\n",
    "    tss = tss.sort_values(by=[\"vin\", \"date\"])\n",
    "    tss = tss.pipe(str_lower_columns, COLS_TO_STR_LOWER)\n",
    "    tss = tss.pipe(compute_date_vars)\n",
    "    tss = tss.pipe(compute_charge_n_discharge_vars)\n",
    "    tss = tss.merge(fleet_info, on=\"vin\", how=\"left\")\n",
    "    tss = tss.eval(\"age = date.dt.tz_localize(None) - start_date.dt.tz_localize(None)\")\n",
    "    # It seems that the reset_index calls doesn't reset the \"vin\" into a category if the groupby's by argument was categorical.\n",
    "    # So we recall astype on the \"vin\"  in case it is supposed to be categorical.\n",
    "    tss = tss.astype({\"vin\": COL_DTYPES[\"vin\"]})\n",
    "    return tss\n",
    "\n",
    "def compute_charge_n_discharge_vars(tss:DF) -> DF:\n",
    "    return (\n",
    "        tss\n",
    "        # Compute the in_charge and in_discharge masks \n",
    "        .pipe(compute_charge_n_discharge_masks, IN_CHARGE_CHARGING_STATUS_VALS, IN_DISCHARGE_CHARGING_STATUS_VALS)\n",
    "        # Compute the correspding indices to perfrom split-apply-combine ops\n",
    "        .pipe(compute_idx_from_masks, [\"in_charge\", \"in_discharge\"])\n",
    "        # We recompute the masks by trimming off the points that have the first and last soc values\n",
    "        # This is done to reduce the noise in the output due to measurments noise.\n",
    "        .pipe(trim_leading_n_trailing_soc_off_masks, [\"in_charge\", \"in_discharge\"]) \n",
    "        .pipe(compute_idx_from_masks, [\"trimmed_in_charge\", \"trimmed_in_discharge\"])\n",
    "        .pipe(compute_cum_var, \"power\", \"cum_energy\")\n",
    "        .pipe(compute_cum_var, \"charger_power\", \"cum_charge_energy_added\")\n",
    "        .pipe(compute_status_col)\n",
    "    )\n",
    "\n",
    "def normalize_units_to_metric( tss:DF) -> DF:\n",
    "    tss[\"odometer\"] = tss[\"odometer\"] * ODOMETER_MILES_TO_KM.get(make, 1)\n",
    "    return tss\n",
    "from scipy.integrate import cumulative_trapezoid\n",
    "def compute_cum_var( tss: DF, var_col:str, cum_var_col:str) -> DF:\n",
    "    if not var_col in tss.columns:\n",
    "        logger.debug(f\"{var_col} not found, not computing {cum_var_col}.\")\n",
    "        return tss\n",
    "    logger.debug(f\"Computing {cum_var_col} from {var_col}.\")\n",
    "    tss[cum_var_col] = (\n",
    "        cumulative_trapezoid(\n",
    "            # Leave the keywords as default order is y x not x y (-_-)\n",
    "            # Make sure that date time units are in seconds before converting to int\n",
    "            x=tss[\"date\"].dt.as_unit(\"s\").astype(int),\n",
    "            y=tss[var_col].fillna(0).values,\n",
    "            initial=0,\n",
    "        )            \n",
    "        .astype(\"float32\")\n",
    "    )\n",
    "    tss[cum_var_col] *= KJ_TO_KWH # Convert from kj to kwh\n",
    "    # Reset value to zero at the start of each vehicle time series\n",
    "    # This is better than performing a groupby.apply with cumulative_trapezoid\n",
    "    tss[cum_var_col] -= tss.groupby(\"vin\", observed=True)[cum_var_col].transform(\"first\")\n",
    "    return tss\n",
    "\n",
    "def compute_date_vars( tss:DF) -> DF:\n",
    "    tss[\"time_diff\"] = tss.groupby(\"vin\", observed=False)[\"date\"].diff()\n",
    "    tss[\"sec_time_diff\"] = tss[\"time_diff\"].dt.total_seconds()\n",
    "    return tss\n",
    "\n",
    "def compute_charge_n_discharge_masks(tss:DF, in_charge_vals:list, in_discharge_vals:list) -> DF:\n",
    "    \"\"\"Computes the `in_charge` and `in_discharge` masks either from the charging_status column or from the evolution of the soc over time.\"\"\"\n",
    "    if make in CHARGE_MASK_WITH_CHARGING_STATUS_MAKES:\n",
    "        return charge_n_discharging_masks_from_charging_status(tss, in_charge_vals, in_discharge_vals)\n",
    "    if make in CHARGE_MASK_WITH_SOC_DIFFS_MAKES:\n",
    "        return charge_n_discharging_masks_from_soc_diff(tss)\n",
    "    raise ValueError(MAKE_NOT_SUPPORTED_ERROR.format(make=make))\n",
    "\n",
    "def charge_n_discharging_masks_from_soc_diff( tss:DF) -> DF:\n",
    "    tss_grp = tss.groupby(\"vin\", observed=True)\n",
    "    tss[\"soc_ffilled\"] = tss_grp[\"soc\"].ffill()\n",
    "    tss[\"soc_diff\"] = tss_grp[\"soc_ffilled\"].diff()\n",
    "    tss[\"soc_diff\"] /= tss[\"soc_diff\"].abs()\n",
    "    soc_diff_ffilled = tss_grp[\"soc_diff\"].ffill()\n",
    "    soc_diff_bfilled = tss_grp[\"soc_diff\"].bfill()\n",
    "    tss[\"in_charge\"] = soc_diff_ffilled.gt(0, fill_value=False) & soc_diff_bfilled.gt(0, fill_value=False)\n",
    "    tss[\"in_discharge\"] = soc_diff_ffilled.lt(0, fill_value=False) & soc_diff_bfilled.lt(0, fill_value=False)\n",
    "    return tss\n",
    "\n",
    "def charge_n_discharging_masks_from_charging_status( tss:DF, in_charge_vals:list, in_discharge_vals:list) -> DF:\n",
    "    assert \"charging_status\" in tss.columns, NO_CHARGING_STATUS_COL_ERROR\n",
    "    return (\n",
    "        tss\n",
    "        .eval(f\"in_charge = charging_status in {in_charge_vals}\")\n",
    "        .eval(f\"in_discharge = charging_status in {in_discharge_vals}\")\n",
    "    )\n",
    "\n",
    "def trim_leading_n_trailing_soc_off_masks( tss:DF, masks:list[str]) -> DF:\n",
    "    for mask in masks:\n",
    "        tss[\"naned_soc\"] = tss[\"soc\"].where(tss[mask])\n",
    "        soc_grp = tss.groupby([\"vin\", mask + \"_idx\"], observed=True)[\"naned_soc\"]\n",
    "        trailing_soc = soc_grp.transform(\"first\")\n",
    "        leading_soc = soc_grp.transform(\"last\")\n",
    "        tss[\"trailing_soc\"] = trailing_soc\n",
    "        tss[\"leading_soc\"] = leading_soc\n",
    "        tss[f\"trimmed_{mask}\"] = tss[mask] & (tss[\"soc\"] != trailing_soc) & (tss[\"soc\"] != leading_soc)\n",
    "    tss = tss.drop(columns=\"naned_soc\")\n",
    "    return tss\n",
    "max_td = TD(hours=1, minutes=30)\n",
    "def compute_idx_from_masks( tss: DF, masks:list[str]) -> DF:\n",
    "    for mask in masks:\n",
    "        idx_col_name = f\"{mask}_idx\"\n",
    "        shifted_mask = tss.groupby(\"vin\", observed=True)[mask].shift(fill_value=False)\n",
    "        tss[\"new_period_start_mask\"] = shifted_mask.ne(tss[mask]) \n",
    "        if max_td is not None:\n",
    "            tss[\"new_period_start_mask\"] |= (tss[\"time_diff\"] > max_td)\n",
    "        tss[idx_col_name] = tss.groupby(\"vin\", observed=True)[\"new_period_start_mask\"].cumsum().astype(\"uint16\")\n",
    "        tss.drop(columns=[\"new_period_start_mask\"], inplace=True)\n",
    "    return tss\n",
    "\n",
    "def compute_status_col( tss:DF) -> DF:\n",
    "    tss_grp = tss.groupby(\"vin\", observed=True)\n",
    "    status = tss[\"in_charge\"].map({True: \"charging\", False:\"discharging\", pd.NA:\"unknown\"})\n",
    "    tss[\"status\"] = status.mask(\n",
    "        tss[\"in_charge\"].eq(False, fill_value=True),\n",
    "        np.where(tss_grp[\"odometer\"].diff() > 0, \"moving\", \"idle_discharging\"),\n",
    "    )\n",
    "    return tss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23",
   "metadata": {},
   "outputs": [],
   "source": [
    "processed_tss = run(raw_tss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class TeslaProcessedTimeSeries(ProcessedTimeSeries):\n",
    "\n",
    "    def __init__(self, make:str=\"tesla\", id_col:str=\"vin\", log_level:str=\"INFO\", max_td:TD=MAX_TD, force_update:bool=False, **kwargs):\n",
    "        self.logger = getLogger(make)\n",
    "        set_level_of_loggers_with_prefix(log_level, make)\n",
    "        super().__init__(make, id_col, log_level, max_td, force_update, **kwargs)\n",
    "\n",
    "    def compute_charge_n_discharge_vars(self, tss:DF) -> DF:\n",
    "        return (\n",
    "            tss\n",
    "            .pipe(self.compute_charge_n_discharge_masks)\n",
    "            .pipe(self.compute_charge_idx_bis)\n",
    "            # .pipe(self.compute_idx_from_masks, [\"in_charge\"])\n",
    "            # .pipe(self.trim_leading_n_trailing_soc_off_masks, [\"in_charge\", \"in_discharge\"])\n",
    "            # # .pipe(self.compute_idx_from_masks, [\"trimmed_in_charge\", \"trimmed_in_discharge\"])\n",
    "        )\n",
    "\n",
    "    def compute_charge_n_discharge_masks(self, tss:DF) -> DF:\n",
    "        self.logger.debug(\"Computing tesla specific charge and discharge masks\")\n",
    "        # We use a nullable boolean Series to represnet the rows where:\n",
    "        tss[\"nan_charging\"] = (\n",
    "            Series(pd.NA, index=tss.index, dtype=\"boolean\")# We are not sure of anything.\n",
    "            .mask(tss[\"charging_status\"].isin(IN_CHARGE_CHARGING_STATUS_VALS), True)# We are sure that the vehicle is in charge.\n",
    "            .mask(tss[\"charging_status\"].isin(IN_DISCHARGE_CHARGING_STATUS_VALS), False)# We are sure that the vehicle is not in charge.\n",
    "        )\n",
    "        # If a period of uncertainty (NaN) is surrounded by equal periods of certainties (True-NaN-True or False-NaN-False),\n",
    "        # We will fill them to the value of these certainties.\n",
    "        # However there are edge cases that have multiple days of uncertainties periods (I can't find the VIN but I'm sure you can ;-) )\n",
    "        # Interestingly enough the charge_energy_added variable does not get forwared that far and gets reset to zero. \n",
    "        # This would create outliers in our charge SoH estimation as we estimate the energy_gained as the diff between the last(0) and first value of charge_energy_added.\n",
    "        # So we set a maximal uncertainty period duration over which we don't fill it.\n",
    "        tss[\"nan_date\"] = tss[\"date\"].mask(tss[\"nan_charging\"].isna())\n",
    "        tss[[\"ffill_charging\", \"ffill_date\"]] = tss.groupby(\"vin\", observed=True)[[\"nan_charging\", \"nan_date\"]].ffill()\n",
    "        tss[[\"bfill_charging\", \"bfill_date\"]] = tss.groupby(\"vin\", observed=True)[[\"nan_charging\", \"nan_date\"]].bfill()\n",
    "        nan_period_duration:Series = tss.eval(\"bfill_date - ffill_date\")\n",
    "        fill_unknown_period = tss.eval(\"ffill_charging.eq(bfill_charging) & @nan_period_duration.le(@MAX_CHARGE_TD)\")\n",
    "        tss[\"nan_charging\"] = tss[\"nan_charging\"].mask(fill_unknown_period, tss[\"ffill_charging\"])\n",
    "        # As mentioned before, the SoC oscillates at [charge_limit_soc - ~3%, charge_limit_soc] so we set these periods to NaN as well.\n",
    "        tss[\"nan_charging\"] = tss[\"nan_charging\"].mask(tss[\"soc\"] >= (tss[\"charge_limit_soc\"] - 3))\n",
    "        # Then we seperate the Series into two, more explicit, columns.\n",
    "        tss[\"in_charge\"] = tss.eval(\"nan_charging.notna() & nan_charging\")\n",
    "        tss[\"in_discharge\"] = tss.eval(\"nan_charging.notna() & ~nan_charging\")\n",
    "        return tss.drop(columns=[\"nan_charging\", \"ffill_charging\", \"bfill_charging\", \"ffill_date\", \"bfill_date\"])\n",
    "    \n",
    "    def compute_charge_n_discharge_masks_bis(self, tss:DF) -> DF:\n",
    "        self.logger.debug(\"Computing tesla specific charge and discharge masks\")\n",
    "\n",
    "        tss_na = tss.dropna(subset=['soc']).copy()\n",
    "\n",
    "        tss_na['soc_diff'] = tss_na.groupby('vin', observed=True)['soc'].diff()\n",
    "\n",
    "        tss_na['trend'] = tss_na['soc_diff'].apply(lambda x: 1 if x > 0 else -1 if x < 0 else 0)\n",
    "\n",
    "        #tss_na['trend_change'] = tss_na.groupby('vin', observed=True)['trend'].transform(lambda x: x != x.shift())\n",
    "        tss = tss.merge(tss_na[[\"soc\", \"date\", \"vin\", 'soc_diff', 'trend']], \n",
    "                        on=[\"soc\", \"date\", \"vin\"], how=\"left\")\n",
    "        tss[[\"trend\", \"soc\", \"odometer\",]].bfill(inplace=True)\n",
    "        tss[\"in_charge\"] = tss.eval('trend==1')\n",
    "        tss[\"in_discharge\"] = tss.eval('trend==-1')\n",
    "        return tss\n",
    "    \n",
    "    \n",
    "    def compute_enenergy_added(self, tss:DF) -> DF:\n",
    "        tss['charge_energy_added'] = tss['dc_charge_energy_added'].where(\n",
    "            tss['dc_charge_energy_added'].notnull() & \n",
    "            (tss['dc_charge_energy_added'] > 0), \n",
    "            tss['ac_charge_energy_added'])\n",
    "        return tss\n",
    "    \n",
    "    # def compute_charge_idx(self, tss:DF) -> DF:\n",
    "    #     self.logger.debug(\"Computing tesla specific charge index.\")\n",
    "    #     if self.make == 'tesla-fleet-telemetry':\n",
    "    #         tss = tss.pipe(self.compute_enenergy_added)\n",
    "    #     tss_grp = tss.groupby(\"vin\", observed=False)\n",
    "    #     tss[\"charge_energy_added\"] = tss_grp[\"charge_energy_added\"].ffill()\n",
    "    #     energy_added_over_time = tss_grp['charge_energy_added'].diff().div(tss[\"sec_time_diff\"].values)\n",
    "    #     # charge_energy_added is cummulative and forward filled, \n",
    "    #     # We check that the charge_energy_added decreases too fast to make sure that  correctly indentify two charging periods before and after a gap as two separate charging periods.\n",
    "    #     new_charge_mask = energy_added_over_time.lt(MIN_POWER_LOSS, fill_value=0) \n",
    "    #     # For the same reason, we ensure that there are no gaps bigger than MAX_CHARGE_TD in between to rows of the same charging period.\n",
    "    #     new_charge_mask |= tss[\"time_diff\"].gt(MAX_CHARGE_TD) \n",
    "    #     # And of course we also check that there is no change of status. \n",
    "    #     new_charge_mask |= (~tss_grp[\"in_charge\"].shift().bfill() & tss[\"in_charge\"]) \n",
    "    #     tss[\"in_charge_idx\"] = new_charge_mask.groupby(tss[\"vin\"], observed=True).cumsum()\n",
    "    #     print(tss[\"in_charge_idx\"].count() / len(tss))\n",
    "    #     tss[\"in_charge_idx\"] = tss[\"in_charge_idx\"].fillna(-1).astype(\"uint16\")\n",
    "    #     return tss\n",
    "    \n",
    "    def compute_charge_idx_bis(self, tss):\n",
    "\n",
    "        if self.make == 'tesla-fleet-telemetry':\n",
    "                    tss = tss.pipe(self.compute_enenergy_added)\n",
    "        tss_na = tss.dropna(subset=['soc']).copy()\n",
    "        tss_na['soc_diff'] = tss_na.groupby('vin', observed=True)['soc'].diff()\n",
    "        tss_na['soc_diff_rolling'] = tss_na['soc_diff'].rolling(window=5, min_periods=1).mean()\n",
    "        # Determine trend\n",
    "        tss_na['trend'] = tss_na['soc_diff_rolling'].apply(lambda x: 1 if x > 0 else -1 if x < 0 else np.nan)\n",
    "        tss_na['trend'] = tss_na['trend'].ffill()\n",
    "\n",
    "        def detect_trend_change(group):\n",
    "\n",
    "            group['prev_trend'] = group['trend'].shift(1)\n",
    "            group['prev_prev_trend'] = group['trend'].shift(2)\n",
    "            \n",
    "            group['prev_date'] = group['date'].shift(1)\n",
    "            group['time_diff_min'] = (group['date'] - group['prev_date']).dt.total_seconds() / 60\n",
    "            group['time_gap'] = group['time_diff_min'] > 60  \n",
    "\n",
    "            # Faire une sépration charge_idx et discharge_idx\n",
    "            group['trend_change'] = (\n",
    "                (((group['trend'] != group['prev_trend']) & \n",
    "                  (group['prev_trend'] == group['prev_prev_trend']) ) |\n",
    "                group['time_gap'])\n",
    "            )\n",
    "            group.loc[group.index[0:2], 'trend_change'] = False\n",
    "            return group\n",
    "\n",
    "\n",
    "        tss_na = tss_na.groupby('vin', observed=True).apply(detect_trend_change).reset_index(drop=True)\n",
    "        \n",
    "        # Compute charge id\n",
    "        tss_na['in_charge_idx'] = tss_na.groupby('vin',  observed=True)['trend_change'].cumsum()\n",
    "        tss = tss.merge(tss_na[[\"soc\", \"date\", \"vin\", 'soc_diff', 'in_charge_idx', 'trend', 'prev_trend', 'prev_prev_trend', 'trend_change',]], \n",
    "                        on=[\"soc\", \"date\", \"vin\"], how=\"left\")\n",
    "        tss[[\"odometer\",\"in_charge_idx\"]] = tss[[\"odometer\", \"in_charge_idx\"]].ffill()\n",
    "        return tss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25",
   "metadata": {},
   "outputs": [],
   "source": [
    "processed_tss = TeslaProcessedTimeSeries(\"tesla-fleet-telemetry\", force_update=True)\n",
    "processed_tss['in_charge_idx'] = processed_tss['in_charge_idx'].astype(str)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26",
   "metadata": {},
   "source": [
    "### raw results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27",
   "metadata": {},
   "outputs": [],
   "source": [
    "from core.stats_utils import series_start_end_diff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28",
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_results = (processed_tss.groupby([\"vin\", \"in_charge_idx\"], observed=True, as_index=False).agg(\n",
    "            ac_energy_added_min=pd.NamedAgg(\"ac_charge_energy_added\", \"min\"),\n",
    "            dc_energy_added_min=pd.NamedAgg(\"dc_charge_energy_added\", \"min\"),\n",
    "            ac_energy_added_end=pd.NamedAgg(\"ac_charge_energy_added\", \"last\"),\n",
    "            dc_energy_added_end=pd.NamedAgg(\"dc_charge_energy_added\", \"last\"),\n",
    "            soc_diff=pd.NamedAgg(\"soc\", series_start_end_diff),\n",
    "            inside_temp=pd.NamedAgg(\"inside_temp\", \"mean\"),\n",
    "            net_capacity=pd.NamedAgg(\"net_capacity\", \"first\"),\n",
    "            range=pd.NamedAgg(\"range\", \"first\"),\n",
    "            odometer=pd.NamedAgg(\"odometer\", \"first\"),\n",
    "            version=pd.NamedAgg(\"version\", \"first\"),\n",
    "            size=pd.NamedAgg(\"soc\", \"size\"),\n",
    "            model=pd.NamedAgg(\"model\", \"first\"),\n",
    "            date=pd.NamedAgg(\"date\", \"first\"),\n",
    "            ac_charging_power=pd.NamedAgg(\"ac_charging_power\", \"median\"),\n",
    "            dc_charging_power=pd.NamedAgg(\"dc_charging_power\", \"median\"),\n",
    "            tesla_code=pd.NamedAgg(\"tesla_code\", \"first\"),\n",
    "        )\n",
    "        .eval(\"charging_power = ac_charging_power + dc_charging_power\")\n",
    "        .eval(\"ac_energy_added = ac_energy_added_end  - ac_energy_added_min\")\n",
    "        .eval(\"dc_energy_added = dc_energy_added_end  - dc_energy_added_min\")\n",
    "        .assign(energy_added=lambda df: np.maximum(df[\"ac_energy_added\"], df[\"dc_energy_added\"]))\n",
    "        .eval(\"soh = energy_added / (soc_diff / 100.0 * net_capacity)\")\n",
    "        .eval(\"level_1 = soc_diff * (charging_power < 8) / 100\")\n",
    "        .eval(\"level_2 = soc_diff * (charging_power.between(8, 45)) / 100\")\n",
    "        .eval(\"level_3 = soc_diff * (charging_power > 45) / 100\")\n",
    "        .sort_values([\"tesla_code\", \"vin\", \"date\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29",
   "metadata": {},
   "outputs": [],
   "source": [
    "25.788389 / (40.029564 / 100 * 60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30",
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_results[(raw_results['vin']==\"LRWYGCFS6PC992837\")][['soh', 'odometer', 'soc_diff', \"energy_added\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pour repartir de ce qui est stocké\n",
    "#raw_results_origin = get_results_origin(force_update=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32",
   "metadata": {},
   "source": [
    "### Processed results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transform.processed_results.main import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34",
   "metadata": {},
   "outputs": [],
   "source": [
    "SOH_FILTER_EVAL = {\n",
    "     \"tesla-fleet-telemetry-30\": \"soh = soh.where(soc_diff > 30 & soh.between(0.75, 1.05))\",\n",
    "     \"tesla-fleet-telemetry-25\": \"soh = soh.where(soc_diff > 25 & soh.between(0.75, 1.05))\",\n",
    "     \"tesla-fleet-telemetry-20\": \"soh = soh.where(soc_diff > 20 & soh.between(0.75, 1.05))\",\n",
    "     \"tesla-fleet-telemetry-15\": \"soh = soh.where(soc_diff > 15 & soh.between(0.75, 1.05))\",\n",
    "     \"tesla-fleet-telemetry-8\": \"soh = soh.where(soc_diff > 8 & soh.between(0.75, 1.05))\",\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_processed_results(brand:str) -> DF:\n",
    "    logger.info(f\"{'Processing ' + brand + ' results.':=^{50}}\")\n",
    "    results =  (\n",
    "        raw_results\n",
    "        # Some raw estimations may have inf values, this will make mask_out_outliers_by_interquartile_range and force_monotonic_decrease fail\n",
    "        # So we replace them by NaNs.\n",
    "        .assign(soh=lambda df: df[\"soh\"].replace([np.inf, -np.inf], np.nan))\n",
    "        .sort_values([\"vin\", \"date\"])\n",
    "        .pipe(make_charge_levels_presentable)\n",
    "        .eval(SOH_FILTER_EVAL[brand])\n",
    "        .pipe(agg_results_by_update_frequency)\n",
    "        .groupby('vin', observed=True)\n",
    "        .apply(make_soh_presentable_per_vehicle, include_groups=False)\n",
    "        .reset_index(level=0)\n",
    "        #.pipe(filter_results_by_lines_bounds, VALID_SOH_POINTS_LINE_BOUNDS, logger=logger)\n",
    "        .sort_values([\"vin\", \"date\"])\n",
    "    )\n",
    "    results[\"soh\"] = results.groupby(\"vin\", observed=True)[\"soh\"].ffill()\n",
    "    results[\"soh\"] = results.groupby(\"vin\", observed=True)[\"soh\"].bfill()\n",
    "    results[\"odometer\"] = results.groupby(\"vin\", observed=True)[\"odometer\"].ffill()\n",
    "    results[\"odometer\"] = results.groupby(\"vin\", observed=True)[\"odometer\"].bfill()\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36",
   "metadata": {},
   "outputs": [],
   "source": [
    "processed_results_30 = get_processed_results('tesla-fleet-telemetry-30')\n",
    "processed_results_25 = get_processed_results('tesla-fleet-telemetry-25')\n",
    "processed_results_20 = get_processed_results('tesla-fleet-telemetry-20')\n",
    "processed_results_15 = get_processed_results('tesla-fleet-telemetry-15')\n",
    "processed_results_8 = get_processed_results('tesla-fleet-telemetry-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37",
   "metadata": {},
   "outputs": [],
   "source": [
    "px.scatter(raw_results[(raw_results['soh'] >.7) &(raw_results['soh'] < 1.05)].dropna(subset='soh'), x='odometer', y='soh', color='vin')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38",
   "metadata": {},
   "outputs": [],
   "source": [
    "px.scatter(processed_results_30[(processed_results_30['soh'] > .75) &(processed_results_30['soh'] < 1.05)].dropna(subset='soh'), x='odometer', y='soh', color='vin')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "data_ev",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

