{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1",
   "metadata": {},
   "source": [
    "## Notebook pour tester le pipe de fleet-telemetry"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2",
   "metadata": {},
   "source": [
    "### Raw tss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from core.s3_utils import *\n",
    "from transform.raw_tss.fleet_telemetry_raw_tss import *\n",
    "import pandas as pd\n",
    "import plotly.express as px\n",
    "from transform.processed_tss.config import *\n",
    "from core.constants import *\n",
    "from transform.raw_tss.config import *\n",
    "# from transform.processed_tss.ProcessedTimeSeries import ProcessedTimeSeries\n",
    "from transform.raw_results.tesla_fleet_telemetry import get_results as get_results_origin\n",
    "from transform.processed_results.main import get_processed_results\n",
    "from datetime import timedelta\n",
    "\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4",
   "metadata": {},
   "outputs": [],
   "source": [
    "S3_RAW_TSS_KEY_FORMAT = \"raw_ts/{brand}/time_series/raw_tss.parquet\"\n",
    "FLEET_TELEMETRY_RAW_TSS_KEY = S3_RAW_TSS_KEY_FORMAT.format(brand=\"tesla-fleet-telemetry\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5",
   "metadata": {},
   "outputs": [],
   "source": [
    "FLEET_TELEMETRY_RAW_TSS_KEY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6",
   "metadata": {},
   "outputs": [],
   "source": [
    "s3 = S3_Bucket()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7",
   "metadata": {},
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col, max as spark_max, substring\n",
    "from typing import List\n",
    "from pyspark.sql import SparkSession, DataFrame as SparkDF\n",
    "from pyspark.sql.functions import col, pandas_udf\n",
    "from typing import Iterator\n",
    "import pandas as pd\n",
    "\n",
    "def get_response_keys_to_parse_with_spark(bucket: S3_Bucket, spark: SparkSession) -> \"pyspark.sql.DataFrame\":\n",
    "    if bucket.check_file_exists(FLEET_TELEMETRY_RAW_TSS_KEY):\n",
    "        # raw_tss_path = bucket.get_s3_path(FLEET_TELEMETRY_RAW_TSS_KEY)\n",
    "        raw_tss_df = spark.read.parquet(FLEET_TELEMETRY_RAW_TSS_KEY).select(\"vin\", \"readable_date\")\n",
    "    else:\n",
    "        # Attention : ici, DEFAULT_TESLA_RAW_TSS_DF doit être converti en PySpark\n",
    "        raw_tss_df = spark.createDataFrame(DEFAULT_TESLA_RAW_TSS_DF)\n",
    "\n",
    "    # Obtenir la dernière date parsée par VIN\n",
    "    last_parsed_df = raw_tss_df.groupBy(\"vin\").agg(\n",
    "        spark_max(\"readable_date\").alias(\"last_parsed_date\")\n",
    "    )\n",
    "\n",
    "    # Liste des fichiers à parser\n",
    "    responses_df = bucket.list_responses_keys_of_brand(\"tesla-fleet-telemetry\", as_spark=True, spark=spark)\n",
    "\n",
    "    # Extraire la date depuis le nom de fichier (supposé être à la fin du nom sans extension .jsonl ou .parquet)\n",
    "    responses_df = responses_df.withColumn(\n",
    "        \"date\", col(\"file\").substr(1, length=col(\"file\"))[:-5].cast(\"timestamp\")\n",
    "    )\n",
    "\n",
    "    # Joindre avec les dernières dates\n",
    "    merged_df = responses_df.join(last_parsed_df, on=\"vin\", how=\"outer\")\n",
    "\n",
    "    # Garder les fichiers nouveaux ou non parsés\n",
    "    filtered_df = merged_df.filter(\n",
    "        (col(\"last_parsed_date\").isNull()) | (col(\"date\") > col(\"last_parsed_date\"))\n",
    "    )\n",
    "\n",
    "    return filtered_df\n",
    "\n",
    "\n",
    "def get_raw_tss_from_keys_with_spark(keys: SparkDF, bucket: S3_Bucket, spark: SparkSession, batch_size: int = 500) -> SparkDF:\n",
    "    # Trier par date pour reproductibilité\n",
    "    keys = keys.orderBy(\"date\")\n",
    "\n",
    "    # Conversion en partitions logiques avec repartition\n",
    "    num_partitions = max(1, keys.count() // batch_size)\n",
    "    keys = keys.repartition(num_partitions)\n",
    "\n",
    "    # Définir une fonction de traitement par partition (pandas_udf de type GROUPED_MAP)\n",
    "    @pandas_udf(\"vin string, timestamp timestamp, other_column_1 type, ...\", functionType=\"MAP_ITER\")\n",
    "    def parse_partition(batch_iter: Iterator[pd.DataFrame]) -> Iterator[pd.DataFrame]:\n",
    "        for pdf in batch_iter:\n",
    "            parsed_dfs = []\n",
    "            for _, row in pdf.iterrows():\n",
    "                key = row[\"key\"]\n",
    "                try:\n",
    "                    response = bucket.read_json(key)\n",
    "                    parsed_df = explode_data(pd.DataFrame.from_records(response))\n",
    "                    parsed_dfs.append(parsed_df)\n",
    "                except Exception as e:\n",
    "                    logger.warning(f\"Failed to parse {key}: {e}\")\n",
    "            if parsed_dfs:\n",
    "                yield pd.concat(parsed_dfs, ignore_index=True)\n",
    "\n",
    "    # Appliquer la fonction sur les partitions\n",
    "    raw_tss_df = keys.mapInPandas(parse_partition, schema=\"vin string, timestamp timestamp, ...\")\n",
    "\n",
    "    return raw_tss_df\n",
    "\n",
    "def get_raw_tss_with_spark(bucket: S3_Bucket, spark: SparkSession) -> \"pyspark.sql.DataFrame\":\n",
    "    logger.debug(\"Getting raw TSS from responses provided by Tesla fleet telemetry.\")\n",
    "    keys_df = get_response_keys_to_parse_with_spark(bucket, spark)\n",
    "\n",
    "    if bucket.check_file_exists(FLEET_TELEMETRY_RAW_TSS_KEY):\n",
    "        # raw_tss_path = bucket.get_s3_path(FLEET_TELEMETRY_RAW_TSS_KEY)\n",
    "        raw_tss_df = spark.read.parquet(\"s3://\"+FLEET_TELEMETRY_RAW_TSS_KEY)\n",
    "        new_raw_tss_df = get_raw_tss_from_keys_with_spark(keys_df, bucket, spark)\n",
    "        return raw_tss_df.unionByName(new_raw_tss_df)\n",
    "    else:\n",
    "        return get_raw_tss_from_keys_with_spark(keys_df, bucket, spark)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8",
   "metadata": {},
   "outputs": [],
   "source": [
    "S3_Bucket().get_creds_from_dot_env()#.read_json_file(\"response/tesla-fleet-telemetry/7SAXCCE67RF445292/2025-04-02.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9",
   "metadata": {},
   "outputs": [],
   "source": [
    "access_key = \"SCW9P6Q1T26F2JGSC1AS\"\n",
    "secret_key = \"c702e16a-5a48-45f3-8538-5783b5c58e44\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "import os\n",
    "from transform.raw_tss.fleet_telemetry_raw_tss import *\n",
    "os.environ[\"PYSPARK_SUBMIT_ARGS\"] = (\n",
    "    \"--packages org.apache.hadoop:hadoop-aws:3.3.4 pyspark-shell\"\n",
    "\n",
    ")\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"Scaleway S3 Read JSON\") \\\n",
    "    .config(\"spark.jars.packages\", \"org.apache.hadoop:hadoop-aws:3.3.4\") \\\n",
    "    .config(\"spark.hadoop.fs.s3a.endpoint\", \"https://s3.fr-par.scw.cloud\") \\\n",
    "    .config(\"spark.hadoop.fs.s3a.access.key\", access_key) \\\n",
    "    .config(\"spark.hadoop.fs.s3a.secret.key\", secret_key) \\\n",
    "    .config(\"spark.hadoop.fs.s3a.impl\", \"org.apache.hadoop.fs.s3a.S3AFileSystem\") \\\n",
    "    .config(\"spark.hadoop.fs.s3a.path.style.access\", \"true\") \\\n",
    "    .config(\"spark.hadoop.fs.s3a.aws.credentials.provider\", \"org.apache.hadoop.fs.s3a.SimpleAWSCredentialsProvider\") \\\n",
    "    .config(\"spark.executor.memory\", \"12g\") \\\n",
    "    .config(\"spark.driver.memory\", \"12g\") \\\n",
    "    .config(\"spark.driver.maxResultSize\", \"4g\") \\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11",
   "metadata": {},
   "outputs": [],
   "source": [
    "s3.check_spark_file_exists(\"raw_ts/tesla-fleet-telemetry/time_series/spark_raw_tss.parquet/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = spark.read.parquet(\"s3a://bib-platform-prod-data/raw_ts/tesla-fleet-telemetry/time_series/spark_raw_tss.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transform.raw_tss.spark_raw_tss import get_raw_tss as get_raw_tss_spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14",
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "from logging import getLogger\n",
    "import os\n",
    "import shutil\n",
    "\n",
    "from scipy.integrate import cumulative_trapezoid\n",
    "import pandas as pd\n",
    "from core.constants import *\n",
    "\n",
    "from core.spark_utils import *\n",
    "from core.pandas_utils import *\n",
    "from core.caching_utils import CachedETLSpark\n",
    "from core.logging_utils import set_level_of_loggers_with_prefix\n",
    "from core.console_utils import main_decorator\n",
    "from transform.processed_tss.config import *\n",
    "from transform.raw_tss.main import get_raw_tss\n",
    "from transform.fleet_info.main import fleet_info\n",
    "from pyspark.sql import DataFrame as DF, Window\n",
    "from pyspark.sql.functions import (\n",
    "    col, lag, unix_timestamp, when, lit, last, \n",
    "    expr, coalesce, sum as spark_sum\n",
    ")\n",
    "from pyspark.sql.types import FloatType, TimestampType, DoubleType\n",
    "from transform.fleet_info.main import fleet_info\n",
    "from core.spark_utils import *\n",
    "from transform.processed_tss.config import *\n",
    "from core.s3_utils import S3_Bucket\n",
    "\n",
    "\n",
    "# Here we have implemented the ETL as a class as most raw time series go through the same processing step.\n",
    "# To have a processing step specific to a data provider/manufacturer, simply implement a subclass of ProcessedTimeSeries and update update_all_tss.\n",
    "class ProcessedTimeSeries(CachedETLSpark):\n",
    "    # Declare that the following variable names are not dataframe(parent class) columns\n",
    "    _metadata = ['make', \"logger\", \"id_col\", \"max_td\"]\n",
    "\n",
    "    def __init__(self, make:str, id_col:str=\"vin\", log_level:str=\"INFO\", max_td:TD=MAX_TD, force_update:bool=False, spark: SparkSession = None,  **kwargs):\n",
    "        self.make = make\n",
    "        logger_name = f\"transform.processed_tss.{make}\"\n",
    "        self.logger = getLogger(logger_name)\n",
    "        set_level_of_loggers_with_prefix(log_level, logger_name)\n",
    "        self.id_col = id_col\n",
    "        self.max_td = max_td\n",
    "        self.spark = spark\n",
    "        super().__init__(S3_PROCESSED_TSS_KEY_FORMAT.format(make=make), \"s3\", force_update=force_update, spark=spark, **kwargs)\n",
    "    # No need to call run, it will be called in CachedETL init.\n",
    "    def run(self):\n",
    "        self.logger.info(f\"{'Processing ' + self.make + ' raw tss.':=^{50}}\")\n",
    "        tss = get_raw_tss_spark(spark)\n",
    "        print('load data')\n",
    "        tss = rename_and_select(tss, rename_spark_column, col_to_select)\n",
    "        tss = safe_astype_spark(tss)\n",
    "        tss = self.normalize_units_to_metric(tss)\n",
    "        tss = tss.orderBy([\"vin\", \"date\"])\n",
    "        tss = self.compute_date_vars(tss)\n",
    "        tss = self.compute_charge_n_discharge_vars(tss)\n",
    "        tss = tss.join(spark.createDataFrame(fleet_info), 'vin', 'left')\n",
    "        print(\"process done\")\n",
    "        return tss\n",
    "\n",
    "\n",
    "    def normalize_units_to_metric(self, tss):\n",
    "        tss = tss.withColumn(\"odometer\", col(\"odometer\") * 1.609)\n",
    "        return tss\n",
    "\n",
    "    \n",
    "    def compute_cum_var(self, tss, var_col: str, cum_var_col: str):\n",
    "        if var_col not in tss.columns:\n",
    "            self.logger.debug(f\"{var_col} not found, not computing {cum_var_col}.\")\n",
    "            return tss\n",
    "\n",
    "        self.logger.debug(f\"Computing {cum_var_col} from {var_col} using Arrow + Pandas UDF.\")\n",
    "\n",
    "        # Schéma de retour attendu → adapte le type si nécessaire\n",
    "        schema = tss.schema.add(cum_var_col, DoubleType())\n",
    "\n",
    "        @pandas_udf(schema, functionType=\"grouped_map\")\n",
    "        def integrate_trapezoid(df: pd.DataFrame) -> pd.DataFrame:\n",
    "            # Ordonne les données par date (important !)\n",
    "            df = df.sort_values(\"date\").copy()\n",
    "\n",
    "            # Conversion x en secondes (numpy int64)\n",
    "            x = df[\"date\"].astype('int64') // 10**9  # Convertit ns → s\n",
    "            y = df[var_col].fillna(0).astype(\"float64\")\n",
    "\n",
    "            # Intégration cumulée\n",
    "            cum = cumulative_trapezoid(y=y.values, x=x.values, initial=0) * KJ_TO_KWH\n",
    "\n",
    "            # Ajuste pour que ça commence à zéro\n",
    "            cum = cum - cum[0]\n",
    "\n",
    "            df[cum_var_col] = cum\n",
    "            return df\n",
    "\n",
    "        return tss.groupBy(self.id_col).apply(integrate_trapezoid)\n",
    "\n",
    "    def compute_date_vars(self, tss: DF) -> DF:\n",
    "        # Créer une fenêtre par vin, ordonnée par date\n",
    "        window_spec = Window.partitionBy(\"vin\").orderBy(\"date\")\n",
    "        \n",
    "        # Calculer le lag de date (valeur précédente)\n",
    "        tss = tss.withColumn(\"prev_date\", lag(col(\"date\")).over(window_spec))\n",
    "        \n",
    "        # Différence en secondes entre les deux timestamps\n",
    "        tss = tss.withColumn(\n",
    "            \"sec_time_diff\",\n",
    "            (unix_timestamp(col(\"date\")) - unix_timestamp(col(\"prev_date\"))).cast(\"double\")\n",
    "        )\n",
    "        \n",
    "        return tss\n",
    "\n",
    "    def compute_charge_n_discharge_masks(self, tss:DF, in_charge_vals:list, in_discharge_vals:list) -> DF:\n",
    "        \"\"\"Computes the `in_charge` and `in_discharge` masks either from the charging_status column or from the evolution of the soc over time.\"\"\"\n",
    "        self.logger.debug(\"Computing charging and discharging masks.\")\n",
    "        if self.make in CHARGE_MASK_WITH_CHARGING_STATUS_MAKES:\n",
    "            return self.charge_n_discharging_masks_from_charging_status(tss, in_charge_vals, in_discharge_vals)\n",
    "        if self.make in CHARGE_MASK_WITH_SOC_DIFFS_MAKES:\n",
    "            return self.charge_n_discharging_masks_from_soc_diff(tss)\n",
    "        raise ValueError(MAKE_NOT_SUPPORTED_ERROR.format(make=self.make))\n",
    "\n",
    "    def charge_n_discharging_masks_from_soc_diff(self, tss):\n",
    "        w = Window.partitionBy(self.id_col).orderBy(\"date\").rowsBetween(Window.unboundedPreceding, 0)\n",
    "\n",
    "        # Forward fill soc\n",
    "        tss = tss.withColumn(\"soc_ffilled\", last(\"soc\", ignorenulls=True).over(w))\n",
    "\n",
    "        # Window for diff calculation\n",
    "        w_diff = Window.partitionBy(self.id_col).orderBy(\"date\")\n",
    "\n",
    "        soc_prev = lag(\"soc_ffilled\").over(w_diff)\n",
    "        soc_diff = col(\"soc_ffilled\") - soc_prev\n",
    "\n",
    "        # Normalisation du signe → {-1, 0, 1}\n",
    "        soc_sign = when(soc_diff.isNull(), lit(0)).otherwise(soc_diff / abs(soc_diff))\n",
    "\n",
    "        tss = tss.withColumn(\"soc_diff\", soc_sign)\n",
    "\n",
    "        # Forward fill and backward fill equivalents\n",
    "        tss = tss.withColumn(\"soc_diff_ffill\", last(\"soc_diff\", ignorenulls=True).over(w))\n",
    "        w_rev = Window.partitionBy(self.id_col).orderBy(col(\"date\").desc()).rowsBetween(Window.unboundedPreceding, 0)\n",
    "        tss = tss.withColumn(\"soc_diff_bfill\", last(\"soc_diff\", ignorenulls=True).over(w_rev))\n",
    "\n",
    "        # Définition des masques\n",
    "        tss = tss.withColumn(\"in_charge\", (col(\"soc_diff_ffill\") > 0) & (col(\"soc_diff_bfill\") > 0))\n",
    "        tss = tss.withColumn(\"in_discharge\", (col(\"soc_diff_ffill\") < 0) & (col(\"soc_diff_bfill\") < 0))\n",
    "\n",
    "        return tss\n",
    "\n",
    "    def charge_n_discharging_masks_from_charging_status(self, tss:DF, in_charge_vals:list, in_discharge_vals:list) -> DF:\n",
    "        self.logger.debug(f\"Computing charging and discharging vars using charging status dictionary.\")\n",
    "        assert \"charging_status\" in tss.columns, NO_CHARGING_STATUS_COL_ERROR\n",
    "        return (\n",
    "            tss\n",
    "            .eval(f\"in_charge = charging_status in {in_charge_vals}\")\n",
    "            .eval(f\"in_discharge = charging_status in {in_discharge_vals}\")\n",
    "        )\n",
    "\n",
    "    def trim_leading_n_trailing_soc_off_masks(self, tss:DF, masks:list[str]) -> DF:\n",
    "        self.logger.debug(f\"Computing trimmed masks of{masks}.\")\n",
    "        for mask in masks:\n",
    "            tss[\"naned_soc\"] = tss[\"soc\"].where(tss[mask])\n",
    "            soc_grp = tss.groupby([\"vin\", mask + \"_idx\"], observed=True)[\"naned_soc\"]\n",
    "            trailing_soc = soc_grp.transform(\"first\")\n",
    "            leading_soc = soc_grp.transform(\"last\")\n",
    "            tss[\"trailing_soc\"] = trailing_soc\n",
    "            tss[\"leading_soc\"] = leading_soc\n",
    "            tss[f\"trimmed_{mask}\"] = tss[mask] & (tss[\"soc\"] != trailing_soc) & (tss[\"soc\"] != leading_soc)\n",
    "        tss = tss.drop(columns=\"naned_soc\")\n",
    "        return tss\n",
    "    \n",
    "    \n",
    "    def compute_idx_from_masks(self, tss, masks: list[str]):\n",
    "        \"\"\"\n",
    "        Spark version of compute_idx_from_masks.\n",
    "        \n",
    "        Args:\n",
    "            tss (DataFrame): Spark DataFrame.\n",
    "            masks (list): List of boolean column names to compute idx on.\n",
    "        \n",
    "        Returns:\n",
    "            DataFrame: Transformed Spark DataFrame.\n",
    "        \"\"\"\n",
    "        self.logger.info(f\"Computing {masks} idx from masks.\")\n",
    "        \n",
    "        for mask in masks:\n",
    "            idx_col_name = f\"{mask}_idx\"\n",
    "\n",
    "            w = Window.partitionBy(self.id_col).orderBy(\"time\")  # adapte 'time' à ta colonne temporelle\n",
    "\n",
    "            # Décalage de mask par groupe\n",
    "            shifted_mask = lag(col(mask), 1).over(w)\n",
    "\n",
    "            # new_period_start_mask = shifted_mask != mask\n",
    "            new_period_start_mask = (shifted_mask.isNull() | (shifted_mask != col(mask)))\n",
    "\n",
    "            # Si max_td est défini, on ajoute aussi condition sur time_diff\n",
    "            if self.max_td is not None:\n",
    "                new_period_start_mask = new_period_start_mask | (col(\"time_diff\") > lit(self.max_td))\n",
    "\n",
    "            # Génère l'index via cumul\n",
    "            tss = tss.withColumn(\n",
    "                \"new_period_start_mask\", when(new_period_start_mask, lit(1)).otherwise(lit(0))\n",
    "            )\n",
    "\n",
    "            tss = tss.withColumn(\n",
    "                idx_col_name, spark_sum(\"new_period_start_mask\").over(w)\n",
    "            ).drop(\"new_period_start_mask\")\n",
    "\n",
    "        return tss\n",
    "\n",
    "    def compute_status_col(self, tss):\n",
    "        self.logger.debug(\"Computing status column.\")\n",
    "\n",
    "        # Fenêtre ordonnée par date pour chaque VIN\n",
    "        w = Window.partitionBy(\"vin\").orderBy(\"date\")\n",
    "\n",
    "        # Décalage pour calculer diff(odometer)\n",
    "        prev_odo = lag(\"odometer\").over(w)\n",
    "        delta_odo = col(\"odometer\") - prev_odo\n",
    "\n",
    "        # Première base de status\n",
    "        status = when(col(\"in_charge\") == True, lit(\"charging\")) \\\n",
    "                .when(col(\"in_charge\") == False, lit(\"discharging\")) \\\n",
    "                .otherwise(lit(\"unknown\"))\n",
    "\n",
    "        # Raffinement → si in_charge == False → \"moving\" ou \"idle_discharging\"\n",
    "        status = when(col(\"in_charge\") == True, lit(\"charging\")) \\\n",
    "                .when(col(\"in_charge\") == False, \n",
    "                    when(delta_odo > 0, lit(\"moving\"))\n",
    "                    .otherwise(lit(\"idle_discharging\"))\n",
    "                    ) \\\n",
    "                .otherwise(lit(\"unknown\"))\n",
    "\n",
    "        return tss.withColumn(\"status\", status)\n",
    "\n",
    "    @classmethod\n",
    "    def update_all_tss(cls, **kwargs):\n",
    "        for make in ALL_MAKES:\n",
    "            if make in [\"tesla\", \"tesla-fleet-telemery\"]:\n",
    "                cls = TeslaProcessedTimeSeries\n",
    "            else:\n",
    "                cls = ProcessedTimeSeries\n",
    "            cls(make, force_update=True, **kwargs)\n",
    "            \n",
    "            \n",
    "class TeslaProcessedTimeSeries(ProcessedTimeSeries):\n",
    "    def __init__(self, make:str=\"tesla-fleet-telemery\", id_col:str=\"vin\", log_level:str=\"INFO\", max_td:TD=MAX_TD, force_update:bool=False, spark=None, **kwargs):\n",
    "        self.logger = getLogger(make)\n",
    "        set_level_of_loggers_with_prefix(log_level, make)\n",
    "        super().__init__(make, id_col, log_level, max_td, force_update, spark=spark, **kwargs)\n",
    "\n",
    "        \n",
    "    def compute_charge_n_discharge_vars(self, tss:DF) -> DF:\n",
    "        tss = self.compute_charge_n_discharge_masks(tss, IN_CHARGE_CHARGING_STATUS_VALS, IN_DISCHARGE_CHARGING_STATUS_VALS)\n",
    "        tss = self.compute_charge_idx_bis(tss)\n",
    "        return tss\n",
    "\n",
    "    def compute_charge_n_discharge_masks(self, tss:DF, in_charge_vals:list, in_discharge_vals:list) -> DF:\n",
    "        \"\"\"Computes the `in_charge` and `in_discharge` masks either from the charging_status column or from the evolution of the soc over time.\"\"\"\n",
    "        if self.make in CHARGE_MASK_WITH_CHARGING_STATUS_MAKES:\n",
    "            return self.charge_n_discharging_masks_from_charging_status(tss, in_charge_vals, in_discharge_vals)\n",
    "\n",
    "    def charge_n_discharging_masks_from_charging_status(self, tss: DF, in_charge_vals: list, in_discharge_vals: list) -> DF:\n",
    "        assert \"charging_status\" in tss.columns, NO_CHARGING_STATUS_COL_ERROR\n",
    "        \n",
    "        # Masques booléens Spark\n",
    "        tss = tss.withColumn(\n",
    "            \"in_charge\",\n",
    "            when(col(\"charging_status\").isin(in_charge_vals), lit(True)).otherwise(lit(False))\n",
    "        )\n",
    "\n",
    "        tss = tss.withColumn(\n",
    "            \"in_discharge\",\n",
    "            when(col(\"charging_status\").isin(in_discharge_vals), lit(True)).otherwise(lit(False))\n",
    "        )\n",
    "    \n",
    "        return tss\n",
    "\n",
    "    def compute_energy_added(self, tss: DF) -> DF:\n",
    "        tss = tss.withColumn(\n",
    "            \"charge_energy_added\",\n",
    "            when(\n",
    "                col(\"dc_charge_energy_added\").isNotNull() & (col(\"dc_charge_energy_added\") > 0),\n",
    "                col(\"dc_charge_energy_added\")\n",
    "            ).otherwise(col(\"ac_charge_energy_added\"))\n",
    "        )\n",
    "        return tss\n",
    "\n",
    "    \n",
    "    def compute_charge_idx_bis(self, tss: DF) -> DF:\n",
    "        \n",
    "        tss = self.compute_energy_added(tss)\n",
    "        \n",
    "        # 1. Filtrer les lignes où soc n'est pas null\n",
    "        tss_na = tss.filter(col(\"soc\").isNotNull())\n",
    "\n",
    "        # 2. Créer une fenêtre ordonnée par date par VIN\n",
    "        vin_window = Window.partitionBy(\"vin\").orderBy(\"date\")\n",
    "\n",
    "        # 3. Calcul des différences\n",
    "        tss_na = tss_na \\\n",
    "            .withColumn(\"soc_diff\", col(\"soc\") - lag(\"soc\", 1).over(vin_window)) \\\n",
    "            .withColumn(\"trend\", when(col(\"soc_diff\") > 0, lit(1))\n",
    "                                .when(col(\"soc_diff\") < 0, lit(-1))\n",
    "                                .otherwise(lit(0))) \\\n",
    "            .withColumn(\"prev_trend\", lag(\"trend\", 1).over(vin_window)) \\\n",
    "            .withColumn(\"prev_prev_trend\", lag(\"trend\", 2).over(vin_window)) \\\n",
    "            .withColumn(\"prev_date\", lag(\"date\", 1).over(vin_window)) \\\n",
    "            .withColumn(\"time_diff_min\", \n",
    "                        (unix_timestamp(col(\"date\")) - unix_timestamp(col(\"prev_date\"))) / 60) \\\n",
    "            .withColumn(\"time_gap\", col(\"time_diff_min\") > 60) \\\n",
    "            .withColumn(\"trend_change\",\n",
    "                        when(\n",
    "                            ((col(\"trend\") != col(\"prev_trend\")) & \n",
    "                            (col(\"prev_trend\") == col(\"prev_prev_trend\"))) | \n",
    "                            col(\"time_gap\"), \n",
    "                            lit(1)\n",
    "                        ).otherwise(lit(0)))\n",
    "\n",
    "        # 4. Initialiser les premières lignes à 0\n",
    "        tss_na = tss_na.withColumn(\n",
    "            \"trend_change\",\n",
    "            when(col(\"date\") == lag(\"date\", 1).over(vin_window), lit(0)).otherwise(col(\"trend_change\"))\n",
    "        )\n",
    "\n",
    "        # 5. Cumulative sum (session index)\n",
    "        tss_na = tss_na.withColumn(\"in_charge_idx\", spark_sum(\"trend_change\").over(vin_window.rowsBetween(Window.unboundedPreceding, 0)))\n",
    "\n",
    "        # 6. Join avec le DataFrame original\n",
    "        tss = tss.join(\n",
    "            tss_na.select(\"vin\", \"date\", \"soc\", \"soc_diff\", \"in_charge_idx\"),\n",
    "            on=[\"vin\", \"date\", \"soc\"],\n",
    "            how=\"left\"\n",
    "        )\n",
    "\n",
    "        # 7. Forward-fill `odometer` et `in_charge_idx` (non-natif en Spark, mais on peut approximer)\n",
    "        fill_window = Window.partitionBy(\"vin\").orderBy(\"date\").rowsBetween(Window.unboundedPreceding, 0)\n",
    "        tss = tss \\\n",
    "            .withColumn(\"odometer\", coalesce(col(\"odometer\"), expr(\"last(odometer, true)\").over(fill_window))) \\\n",
    "            .withColumn(\"in_charge_idx\", coalesce(col(\"in_charge_idx\"), expr(\"last(in_charge_idx, true)\").over(fill_window)))\n",
    "\n",
    "        return tss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15",
   "metadata": {},
   "outputs": [],
   "source": [
    "rename_spark_column = {\n",
    "        \"readable_date\": \"date\",\n",
    "        \"Odometer\" : \"odometer\",\n",
    "        \"ACChargingEnergyIn\": \"ac_charge_energy_added\",\n",
    "        \"Soc\": \"soc\",\n",
    "        \"CarType\": 'model',\n",
    "        \"DCChargingEnergyIn\": \"dc_charge_energy_added\",\n",
    "        \"BatteryLevel\": \"battery_level\",\n",
    "        \"ACChargingPower\": \"ac_charging_power\",\n",
    "        \"DCChargingPower\": \"dc_charging_power\",\n",
    "        \"DetailedChargeState\": \"charging_status\",\n",
    "        \n",
    "}\n",
    "col_to_select = [\n",
    "    'vin', 'date', 'odometer', 'soc', \n",
    "    \"battery_level\",\n",
    "    \"ac_charge_energy_added\",\n",
    "    \"dc_charge_energy_added\",\n",
    "    \"ac_charging_power\",\n",
    "    \"dc_charging_power\",\n",
    "    \"charging_status\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16",
   "metadata": {},
   "outputs": [],
   "source": [
    "res = TeslaProcessedTimeSeries(make='tesla-fleet-telemetry', force_update=False, spark=spark)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17",
   "metadata": {},
   "outputs": [],
   "source": [
    "res.data.select('vin').distinct().count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_pd = res.data.withColumn(\"vin\", col(\"vin\") == \"5YJSA7E52RF541858\").toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_pd.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20",
   "metadata": {},
   "outputs": [],
   "source": [
    "import plotly.express as px"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_pd.dropna(subset='soc').shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_pd.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_pd['in_charge_idx'] = df_pd['in_charge_idx'].astype('str')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_pd[df_pd[\"vin\"]==True]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25",
   "metadata": {},
   "outputs": [],
   "source": [
    "rename_spark_column = {\n",
    "        \"readable_date\": \"date\",\n",
    "        \"Odometer\" : \"odometer\",\n",
    "        \"ACChargingEnergyIn\": \"ac_charge_energy_added\",\n",
    "        \"Soc\": \"soc\",\n",
    "        \"CarType\": 'model',\n",
    "        \"DCChargingEnergyIn\": \"dc_charge_energy_added\",\n",
    "        \"BatteryLevel\": \"battery_level\",\n",
    "        \"ACChargingPower\": \"ac_charging_power\",\n",
    "        \"DCChargingPower\": \"dc_charging_power\",\n",
    "        \"DetailedChargeState\": \"charging_status\",\n",
    "}\n",
    "col_to_select = [\n",
    "    'vin', 'date', 'odometer', 'soc', \n",
    "    \"battery_level\",\n",
    "    \"ac_charge_energy_added\",\n",
    "    \"dc_charge_energy_added\",\n",
    "    \"ac_charging_power\",\n",
    "    \"dc_charging_power\",\n",
    "    \"charging_status\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26",
   "metadata": {},
   "outputs": [],
   "source": [
    "ProcessedTimeSeries.update_all_tss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = res.run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.select('vin').distinct().count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29",
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30",
   "metadata": {},
   "outputs": [],
   "source": [
    "res.vin.nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transform.raw_results.raw_results_spark import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32",
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_res = get_results(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33",
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_res.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34",
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_res_oandas = raw_res.toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35",
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_actual = s3.read_parquet_df('raw_results/tesla-fleet-telemetry.parquet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36",
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_actual[[\"vin\", \"date\", \"soh\", \"version\", ]].sort_values(['vin', 'date'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37",
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_res_oandas[[\"vin\", \"date\", \"soh\", \"version\",\"in_charge_idx\"]].sort_values(['vin', 'date'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38",
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_res_oandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39",
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_res_oandas.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40",
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_res_oandas[[\"vin\", \"date\", \"soh\"]].sort_values(['vin', 'date']).dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_Pandas = df.toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_spark_seesion(access_key, secret_key):\n",
    "    os.environ[\"PYSPARK_SUBMIT_ARGS\"] = (\n",
    "    \"--packages org.apache.hadoop:hadoop-aws:3.3.4 pyspark-shell\"\n",
    ")\n",
    "    spark = SparkSession.builder \\\n",
    "    .appName(\"Scaleway S3 Read JSON\") \\\n",
    "    .config(\"spark.jars.packages\", \"org.apache.hadoop:hadoop-aws:3.3.4\") \\\n",
    "    .config(\"spark.hadoop.fs.s3a.endpoint\", \"https://s3.fr-par.scw.cloud\") \\\n",
    "    .config(\"spark.hadoop.fs.s3a.access.key\", access_key) \\\n",
    "    .config(\"spark.hadoop.fs.s3a.secret.key\", secret_key) \\\n",
    "    .config(\"spark.hadoop.fs.s3a.impl\", \"org.apache.hadoop.fs.s3a.S3AFileSystem\") \\\n",
    "    .config(\"spark.hadoop.fs.s3a.path.style.access\", \"true\") \\\n",
    "    .config(\"spark.hadoop.fs.s3a.aws.credentials.provider\", \"org.apache.hadoop.fs.s3a.SimpleAWSCredentialsProvider\") \\\n",
    "    .getOrCreate()\n",
    "    return spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.types import StructType, StructField, StringType, TimestampType , MapType\n",
    "from pyspark.sql.functions import col, max as spark_max, to_timestamp, expr, udf,  explode, when\n",
    "from pyspark.sql import DataFrame\n",
    "from pyspark.sql.types import StructType, StructField, StringType, DoubleType, BooleanType, IntegerType, ArrayType\n",
    "from functools import reduce\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_parquet_spark(spark, key: str, columns: list[str] | None = None):\n",
    "        full_path = f\"s3a://bib-platform-prod-data/{key}\"\n",
    "        df = spark.read.parquet(full_path)\n",
    "        if columns is not None:\n",
    "            df = df.select(*columns)\n",
    "        return df\n",
    "    \n",
    "def get_response_keys_to_parse_spark(spark, bucket:S3_Bucket):\n",
    "    if bucket.check_file_exists(FLEET_TELEMETRY_RAW_TSS_KEY):\n",
    "        raw_tss_subset = read_parquet_spark(spark, FLEET_TELEMETRY_RAW_TSS_KEY, columns=[\"vin\", \"readable_date\"])\n",
    "    else:\n",
    "        schema = StructType([\n",
    "    StructField(\"vin\", StringType(), True),\n",
    "    StructField(\"readable_date\", TimestampType(), True),\n",
    "])\n",
    "        raw_tss_subset = spark.createDataFrame([], schema)\n",
    "    \n",
    "    last_parsed_date = (\n",
    "        raw_tss_subset\n",
    "        .groupby([\"vin\"])\n",
    "        .agg({\"readable_date\": \"max\"}).withColumnRenamed(\"max(readable_date)\", \"last_parsed_date\")\n",
    "    )\n",
    "    response_keys_df = bucket.list_responses_keys_of_brand(\"tesla-fleet-telemetry\")\n",
    "    response_keys_df = spark.createDataFrame(response_keys_df)\n",
    "    response_keys_df = response_keys_df.withColumn(\n",
    "        \"date\",\n",
    "        to_timestamp(expr(\"substring(file, 1, length(file) - 5)\"))\n",
    "    )\n",
    "\n",
    "    return (\n",
    "        response_keys_df\n",
    "        .join(last_parsed_date, on=\"vin\", how=\"outer\")\n",
    "        .filter((col(\"last_parsed_date\").isNull()) | (col(\"date\") > col(\"last_parsed_date\")))\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45",
   "metadata": {},
   "outputs": [],
   "source": [
    "path = f\"s3a://bib-platform-prod-data/response/tesla-fleet-telemetry/5YJ3E7EB1KF334219/2025-05-21.json\"\n",
    "path_parquet = \"s3a://bib-platform-prod-data/raw_ts/tesla-fleet-telemetry/time_series/raw_tss.parquet\"\n",
    "# df = spark.read.parquet(path_parquet)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46",
   "metadata": {},
   "outputs": [],
   "source": [
    "response = get_response_keys_to_parse_spark(spark, s3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47",
   "metadata": {},
   "outputs": [],
   "source": [
    "resp_pandas = response.toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48",
   "metadata": {},
   "outputs": [],
   "source": [
    "resp_pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49",
   "metadata": {},
   "outputs": [],
   "source": [
    "def explode_data_spark(response: dict):\n",
    "    flattened_data = []\n",
    "    for entry in response if isinstance(response, list) else [response]:\n",
    "        vin = entry.get('vin')\n",
    "        timestamp = entry.get('timestamp')\n",
    "        readable_date = entry.get('readable_date')\n",
    "\n",
    "        base = {\n",
    "            'vin': vin,\n",
    "            'timestamp': timestamp,\n",
    "            'readable_date': readable_date,\n",
    "        }\n",
    "\n",
    "        row_data = {}\n",
    "        for item in entry.get(\"data\", []):\n",
    "            key = item.get(\"key\")\n",
    "            value_dict = item.get(\"value\", {})\n",
    "            if not value_dict:\n",
    "                continue\n",
    "            value = list(value_dict.values())[0]  # Récupère la valeur quel que soit le type\n",
    "            row_data[key] = value\n",
    "\n",
    "        flattened_data.append({**base, **row_data})\n",
    "    # Conversion Pandas → Spark\n",
    "    df_pd = pd.DataFrame(flattened_data)\n",
    "    df_pd = df_pd.replace(False, 'false').replace(True, 'true')\n",
    "    for col in df_pd.columns:\n",
    "        df_pd[col] = df_pd[col].astype(str)\n",
    "    df_spark = spark.createDataFrame(df_pd)\n",
    "    return df_spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_raw_tss_from_keys_spark(keys: DataFrame, bucket: S3_Bucket) -> DataFrame:\n",
    "    \"\"\"\n",
    "    keys: Spark DataFrame with columns 'key', 'vin', 'date' (where 'date' is a timestamp)\n",
    "    bucket: S3_Bucket with method read_multiple_json_files(keys: List[str], ...) -> List[dict]\n",
    "    Returns a Spark DataFrame with parsed and exploded telemetry data\n",
    "    \"\"\"\n",
    "    # Convert Spark DataFrame to Pandas for grouping\n",
    "    keys_pd = keys.select(\"key\", \"vin\", \"date\").toPandas()\n",
    "\n",
    "    # Grouper par semaine (comme avant)\n",
    "    grouped = keys_pd.groupby(pd.Grouper(key=\"date\", freq=\"W-MON\"))\n",
    "    grouped_items = list(grouped)\n",
    "    for week, week_keys in track(grouped_items, description=\"Processing weekly groups\"):\n",
    "        week_date = week.date().strftime('%Y-%m-%d')\n",
    "        logger.debug(f\"Parsing the responses of the week {week_date}:\")\n",
    "        logger.debug(f\"{len(week_keys)} keys to parse for {week_keys['vin'].nunique()} vins.\")\n",
    "        logger.debug(f\"This represents {round(len(week_keys) / len(keys_pd) * 100)}% of the total keys to parse.\")\n",
    "\n",
    "        # Lire les JSON depuis S3\n",
    "        responses = bucket.read_multiple_json_files(week_keys[\"key\"].tolist(), max_workers=64)\n",
    "\n",
    "        logger.debug(f\"Read the responses.\")\n",
    "        # Parser et exploser les JSON avec ThreadPoolExecutor\n",
    "        with ThreadPoolExecutor(max_workers=64) as executor:\n",
    "            week_raw_tss = list(executor.map(explode_data_spark, responses)) # retourne des pandas ou Spark DFs ?\n",
    "        #week_raw_tss = [explode_data_spark(responses)]\n",
    "\n",
    "    #print(len(week_raw_tss))\n",
    "    # Union finale\n",
    "    if week_raw_tss:\n",
    "        return reduce(DataFrame.unionAll, week_raw_tss)\n",
    "    else:\n",
    "        # Retourne un Spark DF vide si aucun résultat\n",
    "        schema = spark.createDataFrame([], week_raw_tss[0].schema) if 'concatenated' in locals() else spark.createDataFrame([], schema=StructType([]))\n",
    "        return schema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_response = [{\"vin\":\"5YJ3E7EB1KF334219\",\"timestamp\":1748318343277,\"readable_date\":\"2025-05-27 03:59:03\",\n",
    "                  \"data\":[{\"key\":\"InsideTemp\",\"value\":{\"stringValue\":\"19.100000880658627\"}},\n",
    "                          {\"key\":\"FastChargerType\",\"value\":{\"invalid\":True}},\n",
    "                          {\"key\":\"PackCurrent\",\"value\":{\"stringValue\":\"0\"}},\n",
    "                          {\"key\":\"ModuleTempMax\",\"value\":{\"stringValue\":\"26\"}},\n",
    "                          {\"key\":\"ChargeCurrentRequest\",\"value\":{\"stringValue\":\"16\"}},\n",
    "                          {\"key\":\"ChargePortColdWeatherMode\",\"value\":{\"stringValue\":\"false\"}},\n",
    "                          {\"key\":\"DCChargingEnergyIn\",\"value\":{\"stringValue\":\"17.419999610632658\"}},\n",
    "                          {\"key\":\"DCDCEnable\",\"value\":{\"stringValue\":\"false\"}},\n",
    "                          {\"key\":\"ChargerPhases\",\"value\":{\"invalid\":True}},\n",
    "                          {\"key\":\"PreconditioningEnabled\",\"value\":{\"stringValue\":\"false\"}},\n",
    "                          {\"key\":\"ChargeCurrentRequestMax\",\"value\":{\"stringValue\":\"16\"}},\n",
    "                          {\"key\":\"BatteryLevel\",\"value\":{\"stringValue\":\"49.2429022082019\"}},\n",
    "                          {\"key\":\"ACChargingPower\",\"value\":{\"stringValue\":\"0\"}},\n",
    "                          {\"key\":\"EstBatteryRange\",\"value\":{\"stringValue\":\"113.16217657792016\"}},\n",
    "                          {\"key\":\"BmsFullchargecomplete\",\"value\":{\"stringValue\":\"false\"}},\n",
    "                          {\"key\":\"ChargeAmps\",\"value\":{\"stringValue\":\"0\"}},\n",
    "                          {\"key\":\"LifetimeEnergyUsed\",\"value\":{\"stringValue\":\"26604.12326362799\"}},\n",
    "                          {\"key\":\"HvacACEnabled\",\"value\":{\"booleanValue\":False}},\n",
    "                          {\"key\":\"BatteryHeaterOn\",\"value\":{\"stringValue\":\"false\"}},\n",
    "                          {\"key\":\"IsolationResistance\",\"value\":{\"stringValue\":\"4280\"}},\n",
    "                          {\"key\":\"DetailedChargeState\",\"value\":{\"detailedChargeStateValue\":\"DetailedChargeStateDisconnected\"}},\n",
    "                          {\"key\":\"BrickVoltageMin\",\"value\":{\"stringValue\":\"3.836000182200223\"}},\n",
    "                          {\"key\":\"BrickVoltageMax\",\"value\":{\"stringValue\":\"3.838000182295218\"}},\n",
    "                          {\"key\":\"EstimatedHoursToChargeTermination\",\"value\":{\"invalid\":True}},\n",
    "                          {\"key\":\"ChargePort\",\"value\":{\"stringValue\":\"CCS\"}},\n",
    "                          {\"key\":\"ChargeState\",\"value\":{\"stringValue\":\"Idle\"}},\n",
    "                          {\"key\":\"HvacPower\",\"value\":{\"hvacPowerValue\":\"HvacPowerStateOff\"}},\n",
    "                          {\"key\":\"EfficiencyPackage\",\"value\":{\"stringValue\":\"Default\"}},\n",
    "                          {\"key\":\"HvacAutoMode\",\"value\":{\"hvacAutoModeValue\":\"HvacAutoModeStateOn\"}},\n",
    "                          {\"key\":\"SentryMode\",\"value\":{\"stringValue\":\"Off\"}},\n",
    "                          {\"key\":\"HvacFanSpeed\",\"value\":{\"intValue\":2}},\n",
    "                          {\"key\":\"BMSState\",\"value\":{\"stringValue\":\"Standby\"}},\n",
    "                          {\"key\":\"FastChargerPresent\",\"value\":{\"stringValue\":\"false\"}},\n",
    "                          {\"key\":\"ModuleTempMin\",\"value\":{\"stringValue\":\"24.5\"}},\n",
    "                          {\"key\":\"Odometer\",\"value\":{\"stringValue\":\"64016.35690902214\"}},\n",
    "                          {\"key\":\"Soc\",\"value\":{\"stringValue\":\"48.958990536277604\"}},\n",
    "                          {\"key\":\"DefrostMode\",\"value\":{\"defrostModeValue\":\"DefrostModeStateOff\"}},\n",
    "                          {\"key\":\"ChargeEnableRequest\",\"value\":{\"stringValue\":\"true\"}},\n",
    "                          {\"key\":\"ACChargingEnergyIn\",\"value\":{\"stringValue\":\"17.83427804352963\"}},\n",
    "                          {\"key\":\"ChargeRateMilePerHour\",\"value\":{\"doubleValue\":0}},\n",
    "                          {\"key\":\"ChargingCableType\",\"value\":{\"invalid\":True}},\n",
    "                          {\"key\":\"VehicleSpeed\",\"value\":{\"invalid\":True}},\n",
    "                          {\"key\":\"OutsideTemp\",\"value\":{\"stringValue\":\"12\"}},\n",
    "                          {\"key\":\"RatedRange\",\"value\":{\"stringValue\":\"126.69387471919158\"}},\n",
    "                          {\"key\":\"EuropeVehicle\",\"value\":{\"booleanValue\":True}},\n",
    "                          {\"key\":\"PackVoltage\",\"value\":{\"stringValue\":\"368.1199917718768\"}},\n",
    "                          {\"key\":\"IdealBatteryRange\",\"value\":{\"stringValue\":\"126.69387471919158\"}},\n",
    "                          {\"key\":\"ClimateKeeperMode\",\"value\":{\"climateKeeperModeValue\":\"ClimateKeeperModeStateOff\"}},\n",
    "                          {\"key\":\"RearDefrostEnabled\",\"value\":{\"booleanValue\":False}},\n",
    "                          {\"key\":\"CarType\",\"value\":{\"stringValue\":\"Model3\"}},\n",
    "                          {\"key\":\"DefrostForPreconditioning\",\"value\":{\"booleanValue\":False}},\n",
    "                          {\"key\":\"EnergyRemaining\",\"value\":{\"stringValue\":\"32.65999926999211\"}},\n",
    "                          {\"key\":\"ChargeLimitSoc\",\"value\":{\"stringValue\":\"62\"}},\n",
    "                          {\"key\":\"DCChargingPower\",\"value\":{\"stringValue\":\"0\"}}],\"meta\":{},\"createdAt\":\"2025-05-27T03:59:03.277232687Z\"},\n",
    "                 {\"vin\":\"5YJ3E7EB1KF334219\",\"timestamp\":1748318344277,\"readable_date\":\"2025-05-27 03:59:04\",\n",
    "                  \"data\":[{\"key\":\"ChargerVoltage\",\"value\":{\"doubleValue\":1.716}}],\"meta\":{},\"createdAt\":\"2025-05-27T03:59:04.277224934Z\"}]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import to_date, trunc, date_format\n",
    "from rich.progress import track\n",
    "\n",
    "def get_raw_tss_from_keys_spark(keys: DataFrame, bucket: S3_Bucket) -> DataFrame:\n",
    "\n",
    "    df = keys.withColumn(\"week_start\", trunc(to_date(\"date\"), \"week\"))  # Spark considère la semaine commençant le dimanche\n",
    "\n",
    "    # Corriger pour que la semaine commence le lundi\n",
    "    from pyspark.sql.functions import expr\n",
    "    df = df.withColumn(\"week_start\", expr(\"date_sub(trunc(date, 'week'), 6)\"))\n",
    "    all_raw = []\n",
    "    # Obtenir les semaines uniques\n",
    "    weeks = df.select(\"week_start\").distinct().orderBy(\"week_start\").collect()\n",
    "    for row in track(weeks, description=\"Processing weekly groups\"):\n",
    "        week_keys = df.filter(df.week_start == row[\"week_start\"])\n",
    "        key_list = [r[\"key\"] for r in week_keys.select(\"key\").distinct().collect()]\n",
    "        responses = bucket.read_multiple_json_files(key_list, max_workers=64)\n",
    "        with ThreadPoolExecutor(max_workers=64) as executor:\n",
    "            week_raw_tss = list(executor.map(explode_data_spark, responses))\n",
    "            all_raw = all_raw + week_raw_tss\n",
    "        \n",
    "    if all_raw:\n",
    "        return reduce(lambda df1, df2: df1.unionByName(df2, allowMissingColumns=True), all_raw)\n",
    "    else:\n",
    "        return pd.DataFrame()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53",
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_tss = get_raw_tss_from_keys_spark(response, s3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54",
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_tss.toPandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55",
   "metadata": {},
   "source": [
    "### Process TTS Spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56",
   "metadata": {},
   "outputs": [],
   "source": [
    "rename_spark_column = {\n",
    "        \"readable_date\": \"date\",\n",
    "        \"Odometer\" : \"odometer\",\n",
    "        \"ACChargingEnergyIn\": \"ac_charge_energy_added\",\n",
    "        \"Soc\": \"soc\",\n",
    "        \"CarType\": 'model',\n",
    "        \"DCChargingEnergyIn\": \"dc_charge_energy_added\",\n",
    "        \"BatteryLevel\": \"battery_level\",\n",
    "        \"ACChargingPower\": \"ac_charging_power\",\n",
    "        \"DCChargingPower\": \"dc_charging_power\",\n",
    "        \"DetailedChargeState\": \"charging_status\",\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57",
   "metadata": {},
   "outputs": [],
   "source": [
    "col_to_select = [\n",
    "    'vin', 'date', 'odometer', 'soc', \n",
    "    \"battery_level\",\n",
    "    \"ac_charge_energy_added\",\n",
    "    \"dc_charge_energy_added\",\n",
    "    \"ac_charging_power\",\n",
    "    \"dc_charging_power\",\n",
    "    \"charging_status\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col\n",
    "from pyspark.sql.types import FloatType, TimestampType\n",
    "def rename_and_select(tss, rename_col, col_to_select):\n",
    "    return tss.withColumnsRenamed(rename_col).select(col_to_select)\n",
    "\n",
    "def safe_astype(tss):\n",
    "    return tss.withColumn(\"odometer\", col(\"odometer\").cast(FloatType())) \\\n",
    "    .withColumn(\"soc\", col(\"soc\").cast(FloatType())) \\\n",
    "    .withColumn(\"battery_level\", col(\"battery_level\").cast(FloatType())) \\\n",
    "    .withColumn(\"ac_charge_energy_added\", col(\"ac_charge_energy_added\").cast(FloatType())) \\\n",
    "    .withColumn(\"dc_charge_energy_added\", col(\"dc_charge_energy_added\").cast(FloatType())) \\\n",
    "    .withColumn(\"ac_charging_power\", col(\"ac_charging_power\").cast(FloatType())) \\\n",
    "    .withColumn(\"dc_charging_power\", col(\"dc_charging_power\").cast(FloatType())) \\\n",
    "    .withColumn(\"date\", col(\"date\").cast(TimestampType()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59",
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_tss_filter_type = raw_tss_filter.withColumn(\"odometer\", col(\"odometer\").cast(FloatType())) \\\n",
    "    .withColumn(\"soc\", col(\"soc\").cast(FloatType())) \\\n",
    "    .withColumn(\"battery_level\", col(\"battery_level\").cast(FloatType())) \\\n",
    "    .withColumn(\"ac_charge_energy_added\", col(\"ac_charge_energy_added\").cast(FloatType())) \\\n",
    "    .withColumn(\"dc_charge_energy_added\", col(\"dc_charge_energy_added\").cast(FloatType())) \\\n",
    "    .withColumn(\"ac_charging_power\", col(\"ac_charging_power\").cast(FloatType())) \\\n",
    "    .withColumn(\"dc_charging_power\", col(\"dc_charging_power\").cast(FloatType())) \\\n",
    "    .withColumn(\"date\", col(\"date\").cast(TimestampType()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60",
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_units_to_metric(tss):\n",
    "    tss = tss.withColumn(\"odometer\", col(\"odometer\") * 1.609)\n",
    "    return tss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61",
   "metadata": {},
   "outputs": [],
   "source": [
    "normalize_units_to_metric(raw_tss_filter_type)\n",
    "raw_tss_filter_type_sorted = raw_tss_filter_type.sort(['vin', \"date\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import Window\n",
    "from pyspark.sql.functions import col, lag, unix_timestamp\n",
    "def compute_date_vars(tss: DF) -> DF:\n",
    "    # Créer une fenêtre par vin, ordonnée par date\n",
    "    window_spec = Window.partitionBy(\"vin\").orderBy(\"date\")\n",
    "    \n",
    "    # Calculer le lag de date (valeur précédente)\n",
    "    tss = tss.withColumn(\"prev_date\", lag(col(\"date\")).over(window_spec))\n",
    "    \n",
    "    # Différence en secondes entre les deux timestamps\n",
    "    tss = tss.withColumn(\n",
    "        \"sec_time_diff\",\n",
    "        (unix_timestamp(col(\"date\")) - unix_timestamp(col(\"prev_date\"))).cast(\"double\")\n",
    "    )\n",
    "    \n",
    "    return tss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63",
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_tss_filter_type_sorted_date_vars = compute_date_vars(raw_tss_filter_type_sorted)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import DataFrame as DF, Window\n",
    "from pyspark.sql.functions import (\n",
    "    col, lag, unix_timestamp, when, lit,\n",
    "    expr, coalesce, sum as _sum\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65",
   "metadata": {},
   "outputs": [],
   "source": [
    "IN_CHARGE_CHARGING_STATUS_VALS = [\n",
    "    'charging', # Tesla\n",
    "    # 'nopower', # Tesla\n",
    "    'chargingactive',\n",
    "    'slow_charging',\n",
    "    'fast_charging',\n",
    "    'initialization',\n",
    "    \"in-progress\",\n",
    "    # fleet-telemetry\n",
    "    'detailedchargestatecharging', \n",
    "    'detailedchargestatestarting'\n",
    "]\n",
    "\n",
    "IN_DISCHARGE_CHARGING_STATUS_VALS = [\n",
    "    'charging_error',\n",
    "    'nocharging',\n",
    "    'chargingerror',\n",
    "    'cable_unplugged',\n",
    "    'disconnected', # Tesla\n",
    "     # fleet-telemetry\n",
    "    \"detailedchargestatedisconnected\",\n",
    "    \"detailedchargestatenopower\",\n",
    "    \"detailedchargestatestopped\",\n",
    "    \"detailedchargestatecomplete\",\n",
    "  \n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66",
   "metadata": {},
   "outputs": [],
   "source": [
    "def charge_n_discharging_masks_from_charging_status(tss: DF, in_charge_vals: list, in_discharge_vals: list) -> DF:\n",
    "    assert \"charging_status\" in tss.columns, NO_CHARGING_STATUS_COL_ERROR\n",
    "    \n",
    "    # Masques booléens Spark\n",
    "    tss = tss.withColumn(\n",
    "        \"in_charge\",\n",
    "        when(col(\"charging_status\").isin(in_charge_vals), lit(True)).otherwise(lit(False))\n",
    "    )\n",
    "\n",
    "    tss = tss.withColumn(\n",
    "        \"in_discharge\",\n",
    "        when(col(\"charging_status\").isin(in_discharge_vals), lit(True)).otherwise(lit(False))\n",
    "    )\n",
    "    \n",
    "    return tss\n",
    "\n",
    "def compute_charge_n_discharge_masks(tss:DF, in_charge_vals:list, in_discharge_vals:list) -> DF:\n",
    "    \"\"\"Computes the `in_charge` and `in_discharge` masks either from the charging_status column or from the evolution of the soc over time.\"\"\"\n",
    "    if \"tesla-fleet-telemetry\" in CHARGE_MASK_WITH_CHARGING_STATUS_MAKES:\n",
    "        return charge_n_discharging_masks_from_charging_status(tss, in_charge_vals, in_discharge_vals)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_energy_added(tss: DF) -> DF:\n",
    "    tss = tss.withColumn(\n",
    "        \"charge_energy_added\",\n",
    "        when(\n",
    "            col(\"dc_charge_energy_added\").isNotNull() & (col(\"dc_charge_energy_added\") > 0),\n",
    "            col(\"dc_charge_energy_added\")\n",
    "        ).otherwise(col(\"ac_charge_energy_added\"))\n",
    "    )\n",
    "    return tss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def compute_charge_idx_bis(tss: DF) -> DF:\n",
    "    \n",
    "    tss = compute_energy_added(tss)\n",
    "    \n",
    "    # 1. Filtrer les lignes où soc n'est pas null\n",
    "    tss_na = tss.filter(col(\"soc\").isNotNull())\n",
    "\n",
    "    # 2. Créer une fenêtre ordonnée par date par VIN\n",
    "    vin_window = Window.partitionBy(\"vin\").orderBy(\"date\")\n",
    "\n",
    "    # 3. Calcul des différences\n",
    "    tss_na = tss_na \\\n",
    "        .withColumn(\"soc_diff\", col(\"soc\") - lag(\"soc\", 1).over(vin_window)) \\\n",
    "        .withColumn(\"trend\", when(col(\"soc_diff\") > 0, lit(1))\n",
    "                              .when(col(\"soc_diff\") < 0, lit(-1))\n",
    "                              .otherwise(lit(0))) \\\n",
    "        .withColumn(\"prev_trend\", lag(\"trend\", 1).over(vin_window)) \\\n",
    "        .withColumn(\"prev_prev_trend\", lag(\"trend\", 2).over(vin_window)) \\\n",
    "        .withColumn(\"prev_date\", lag(\"date\", 1).over(vin_window)) \\\n",
    "        .withColumn(\"time_diff_min\", \n",
    "                    (unix_timestamp(col(\"date\")) - unix_timestamp(col(\"prev_date\"))) / 60) \\\n",
    "        .withColumn(\"time_gap\", col(\"time_diff_min\") > 60) \\\n",
    "        .withColumn(\"trend_change\",\n",
    "                    when(\n",
    "                        ((col(\"trend\") != col(\"prev_trend\")) & \n",
    "                         (col(\"prev_trend\") == col(\"prev_prev_trend\"))) | \n",
    "                        col(\"time_gap\"), \n",
    "                        lit(1)\n",
    "                    ).otherwise(lit(0)))\n",
    "\n",
    "    # 4. Initialiser les premières lignes à 0\n",
    "    tss_na = tss_na.withColumn(\n",
    "        \"trend_change\",\n",
    "        when(col(\"date\") == lag(\"date\", 1).over(vin_window), lit(0)).otherwise(col(\"trend_change\"))\n",
    "    )\n",
    "\n",
    "    # 5. Cumulative sum (session index)\n",
    "    tss_na = tss_na.withColumn(\"in_charge_idx\", _sum(\"trend_change\").over(vin_window.rowsBetween(Window.unboundedPreceding, 0)))\n",
    "\n",
    "    # 6. Join avec le DataFrame original\n",
    "    tss = tss.join(\n",
    "        tss_na.select(\"vin\", \"date\", \"soc\", \"soc_diff\", \"in_charge_idx\"),\n",
    "        on=[\"vin\", \"date\", \"soc\"],\n",
    "        how=\"left\"\n",
    "    )\n",
    "\n",
    "    # 7. Forward-fill `odometer` et `in_charge_idx` (non-natif en Spark, mais on peut approximer)\n",
    "    fill_window = Window.partitionBy(\"vin\").orderBy(\"date\").rowsBetween(Window.unboundedPreceding, 0)\n",
    "    tss = tss \\\n",
    "        .withColumn(\"odometer\", coalesce(col(\"odometer\"), expr(\"last(odometer, true)\").over(fill_window))) \\\n",
    "        .withColumn(\"in_charge_idx\", coalesce(col(\"in_charge_idx\"), expr(\"last(in_charge_idx, true)\").over(fill_window)))\n",
    "\n",
    "    return tss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_charge_n_discharge_vars(tss:DF) -> DF:\n",
    "    tss = compute_charge_n_discharge_masks(tss, IN_CHARGE_CHARGING_STATUS_VALS,  IN_DISCHARGE_CHARGING_STATUS_VALS)\n",
    "    tss = compute_charge_idx_bis(tss)\n",
    "    return tss\n",
    "        # .pipe(self.compute_idx_from_masks, [\"in_charge\"])\n",
    "        # .pipe(self.trim_leading_n_trailing_soc_off_masks, [\"in_charge\", \"in_discharge\"])\n",
    "        # # .pipe(self.compute_idx_from_masks, [\"trimmed_in_charge\", \"trimmed_in_discharge\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70",
   "metadata": {},
   "outputs": [],
   "source": [
    "process_tss = compute_charge_n_discharge_vars(raw_tss_filter_type_sorted_date_vars)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run():\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transform.fleet_info.main import fleet_info\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run(tss) -> DF:\n",
    "        tss = rename_and_select(tss, rename_spark_column, col_to_select)\n",
    "        tss = safe_astype(tss)\n",
    "        tss = normalize_units_to_metric(tss)\n",
    "        tss = tss.orderBy([\"vin\", \"date\"])\n",
    "        #tss = str_lower_columns(tss, COLS_TO_STR_LOWER)\n",
    "        tss = compute_date_vars(tss)\n",
    "        tss = compute_charge_n_discharge_vars(tss)\n",
    "        #tss = tss.merge(fleet_info, on=\"vin\", how=\"left\")\n",
    "        \n",
    "        tss = tss.join(spark.createDataFrame(fleet_info), 'vin', 'left')\n",
    "        #tss = tss.eval(\"age = date.dt.tz_localize(None) - start_date.dt.tz_localize(None)\")\n",
    "        #tss = tss.withColumn(\"age\")\n",
    "        # It seems that the reset_index calls doesn't reset the id_col into a category if the groupby's by argument was categorical.\n",
    "        # So we recall astype on the id_col  in case it is supposed to be categorical.\n",
    "        return tss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "#response.write.mode(\"overwrite\").parquet(\"s3a://bib-platform-prod-data/raw_ts/tesla-fleet-telemetry/time_series/test.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75",
   "metadata": {},
   "outputs": [],
   "source": [
    "run(raw_tss)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76",
   "metadata": {},
   "source": [
    "# pass"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77",
   "metadata": {},
   "source": [
    "### Processed tss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transform.processed_tss.ProcessedTimeSeries import ProcessedTimeSeries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_charge_n_discharge_masks(tss:DF) -> DF:\n",
    "        # We use a nullable boolean Series to represnet the rows where:\n",
    "        tss[\"nan_charging\"] = (\n",
    "            Series(pd.NA, index=tss.index, dtype=\"boolean\")# We are not sure of anything.\n",
    "            .mask(tss[\"charging_status\"].isin(IN_CHARGE_CHARGING_STATUS_VALS), True)# We are sure that the vehicle is in charge.\n",
    "            .mask(tss[\"charging_status\"].isin(IN_DISCHARGE_CHARGING_STATUS_VALS), False)# We are sure that the vehicle is not in charge.\n",
    "        )\n",
    "        # If a period of uncertainty (NaN) is surrounded by equal periods of certainties (True-NaN-True or False-NaN-False),\n",
    "        # We will fill them to the value of these certainties.\n",
    "        # However there are edge cases that have multiple days of uncertainties periods (I can't find the VIN but I'm sure you can ;-) )\n",
    "        # Interestingly enough the charge_energy_added variable does not get forwared that far and gets reset to zero. \n",
    "        # This would create outliers in our charge SoH estimation as we estimate the energy_gained as the diff between the last(0) and first value of charge_energy_added.\n",
    "        # So we set a maximal uncertainty period duration over which we don't fill it.\n",
    "        tss[\"nan_date\"] = tss[\"date\"].mask(tss[\"nan_charging\"].isna())\n",
    "        tss[[\"ffill_charging\", \"ffill_date\"]] = tss.groupby(\"vin\", observed=True)[[\"nan_charging\", \"nan_date\"]].ffill()\n",
    "        tss[[\"bfill_charging\", \"bfill_date\"]] = tss.groupby(\"vin\", observed=True)[[\"nan_charging\", \"nan_date\"]].bfill()\n",
    "        nan_period_duration:Series = tss.eval(\"bfill_date - ffill_date\")\n",
    "        fill_unknown_period = tss.eval(\"ffill_charging.eq(bfill_charging) & @nan_period_duration.le(@MAX_CHARGE_TD)\")\n",
    "        tss[\"nan_charging\"] = tss[\"nan_charging\"].mask(fill_unknown_period, tss[\"ffill_charging\"])\n",
    "        # As mentioned before, the SoC oscillates at [charge_limit_soc - ~3%, charge_limit_soc] so we set these periods to NaN as well.\n",
    "        tss[\"nan_charging\"] = tss[\"nan_charging\"].mask(tss[\"soc\"] >= (tss[\"charge_limit_soc\"] - 3))\n",
    "        # Then we seperate the Series into two, more explicit, columns.\n",
    "        tss[\"in_charge\"] = tss.eval(\"nan_charging.notna() & nan_charging\")\n",
    "        tss[\"in_discharge\"] = tss.eval(\"nan_charging.notna() & ~nan_charging\")\n",
    "        return tss.drop(columns=[\"nan_charging\", \"ffill_charging\", \"bfill_charging\", \"ffill_date\", \"bfill_date\"])\n",
    "    \n",
    "def compute_energy_added(tss:DF) -> DF:\n",
    "        tss['charge_energy_added'] = tss['dc_charge_energy_added'].where(\n",
    "            tss['dc_charge_energy_added'].notnull() & \n",
    "            (tss['dc_charge_energy_added'] > 0), \n",
    "            tss['ac_charge_energy_added'])\n",
    "        return tss\n",
    "    \n",
    "def compute_charge_idx_bis(tss):\n",
    "\n",
    "        tss = tss.pipe(compute_energy_added)\n",
    "        tss_na = tss.dropna(subset=['soc']).copy()\n",
    "        tss_na['soc_diff'] = tss_na.groupby('vin', observed=True)['soc'].diff()\n",
    "        tss_na['soc_diff_rolling'] = tss_na['soc_diff'].rolling(window=5, min_periods=1).mean()\n",
    "        # Determine trend\n",
    "        tss_na['trend'] = tss_na['soc_diff_rolling'].apply(lambda x: 1 if x > 0 else -1 if x < 0 else np.nan)\n",
    "        tss_na['trend'] = tss_na['trend'].ffill()\n",
    "\n",
    "        def detect_trend_change(group):\n",
    "\n",
    "            group['prev_trend'] = group['trend'].shift(1)\n",
    "            group['prev_prev_trend'] = group['trend'].shift(2)\n",
    "            \n",
    "            group['prev_date'] = group['date'].shift(1)\n",
    "            group['time_diff_min'] = (group['date'] - group['prev_date']).dt.total_seconds() / 60\n",
    "            group['time_gap'] = group['time_diff_min'] > 60  \n",
    "\n",
    "            # Faire une sépration charge_idx et discharge_idx\n",
    "            group['trend_change'] = (\n",
    "                (((group['trend'] != group['prev_trend']) & \n",
    "                  (group['prev_trend'] == group['prev_prev_trend']) ) |\n",
    "                group['time_gap'])\n",
    "            )\n",
    "            group.loc[group.index[0:2], 'trend_change'] = False\n",
    "            return group\n",
    "\n",
    "\n",
    "        tss_na = tss_na.groupby('vin', observed=True).apply(detect_trend_change).reset_index(drop=True)\n",
    "        \n",
    "        # Compute charge id\n",
    "        tss_na['in_charge_idx'] = tss_na.groupby('vin',  observed=True)['trend_change'].cumsum()\n",
    "        tss = tss.merge(tss_na[[\"soc\", \"date\", \"vin\", 'soc_diff', 'in_charge_idx', 'trend', 'prev_trend', 'prev_prev_trend', 'trend_change',]], \n",
    "                        on=[\"soc\", \"date\", \"vin\"], how=\"left\")\n",
    "        tss[[\"odometer\",\"in_charge_idx\"]] = tss[[\"odometer\", \"in_charge_idx\"]].ffill()\n",
    "        return tss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80",
   "metadata": {},
   "outputs": [],
   "source": [
    "from core.pandas_utils import safe_locate, safe_astype, str_lower_columns\n",
    "from transform.fleet_info.main import fleet_info "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81",
   "metadata": {},
   "outputs": [],
   "source": [
    "make = \"tesla-fleet-telemetry\"\n",
    "def run(tss):\n",
    "    tss = tss.rename(columns=RENAME_COLS_DICT, errors=\"ignore\")\n",
    "    tss = tss.pipe(safe_locate, col_loc=list(COL_DTYPES.keys()), logger=logger)\n",
    "    tss = tss.pipe(safe_astype, COL_DTYPES, logger=logger)\n",
    "    tss = tss.pipe(normalize_units_to_metric)\n",
    "    tss = tss.sort_values(by=[\"vin\", \"date\"])\n",
    "    tss = tss.pipe(str_lower_columns, COLS_TO_STR_LOWER)\n",
    "    tss = tss.pipe(compute_date_vars)\n",
    "    tss = tss.pipe(compute_charge_n_discharge_vars)\n",
    "    tss = tss.merge(fleet_info, on=\"vin\", how=\"left\")\n",
    "    tss = tss.eval(\"age = date.dt.tz_localize(None) - start_date.dt.tz_localize(None)\")\n",
    "    # It seems that the reset_index calls doesn't reset the \"vin\" into a category if the groupby's by argument was categorical.\n",
    "    # So we recall astype on the \"vin\"  in case it is supposed to be categorical.\n",
    "    tss = tss.astype({\"vin\": COL_DTYPES[\"vin\"]})\n",
    "    return tss\n",
    "\n",
    "def compute_charge_n_discharge_vars(tss:DF) -> DF:\n",
    "    return (\n",
    "        tss\n",
    "        # Compute the in_charge and in_discharge masks \n",
    "        .pipe(compute_charge_n_discharge_masks, IN_CHARGE_CHARGING_STATUS_VALS, IN_DISCHARGE_CHARGING_STATUS_VALS)\n",
    "        # Compute the correspding indices to perfrom split-apply-combine ops\n",
    "        .pipe(compute_idx_from_masks, [\"in_charge\", \"in_discharge\"])\n",
    "        # We recompute the masks by trimming off the points that have the first and last soc values\n",
    "        # This is done to reduce the noise in the output due to measurments noise.\n",
    "        .pipe(trim_leading_n_trailing_soc_off_masks, [\"in_charge\", \"in_discharge\"]) \n",
    "        .pipe(compute_idx_from_masks, [\"trimmed_in_charge\", \"trimmed_in_discharge\"])\n",
    "        .pipe(compute_cum_var, \"power\", \"cum_energy\")\n",
    "        .pipe(compute_cum_var, \"charger_power\", \"cum_charge_energy_added\")\n",
    "        .pipe(compute_status_col)\n",
    "    )\n",
    "\n",
    "def normalize_units_to_metric( tss:DF) -> DF:\n",
    "    tss[\"odometer\"] = tss[\"odometer\"] * ODOMETER_MILES_TO_KM.get(make, 1)\n",
    "    return tss\n",
    "from scipy.integrate import cumulative_trapezoid\n",
    "def compute_cum_var( tss: DF, var_col:str, cum_var_col:str) -> DF:\n",
    "    if not var_col in tss.columns:\n",
    "        logger.debug(f\"{var_col} not found, not computing {cum_var_col}.\")\n",
    "        return tss\n",
    "    logger.debug(f\"Computing {cum_var_col} from {var_col}.\")\n",
    "    tss[cum_var_col] = (\n",
    "        cumulative_trapezoid(\n",
    "            # Leave the keywords as default order is y x not x y (-_-)\n",
    "            # Make sure that date time units are in seconds before converting to int\n",
    "            x=tss[\"date\"].dt.as_unit(\"s\").astype(int),\n",
    "            y=tss[var_col].fillna(0).values,\n",
    "            initial=0,\n",
    "        )            \n",
    "        .astype(\"float32\")\n",
    "    )\n",
    "    tss[cum_var_col] *= KJ_TO_KWH # Convert from kj to kwh\n",
    "    # Reset value to zero at the start of each vehicle time series\n",
    "    # This is better than performing a groupby.apply with cumulative_trapezoid\n",
    "    tss[cum_var_col] -= tss.groupby(\"vin\", observed=True)[cum_var_col].transform(\"first\")\n",
    "    return tss\n",
    "\n",
    "def compute_date_vars( tss:DF) -> DF:\n",
    "    tss[\"time_diff\"] = tss.groupby(\"vin\", observed=False)[\"date\"].diff()\n",
    "    tss[\"sec_time_diff\"] = tss[\"time_diff\"].dt.total_seconds()\n",
    "    return tss\n",
    "\n",
    "def compute_charge_n_discharge_masks(tss:DF, in_charge_vals:list, in_discharge_vals:list) -> DF:\n",
    "    \"\"\"Computes the `in_charge` and `in_discharge` masks either from the charging_status column or from the evolution of the soc over time.\"\"\"\n",
    "    if make in CHARGE_MASK_WITH_CHARGING_STATUS_MAKES:\n",
    "        return charge_n_discharging_masks_from_charging_status(tss, in_charge_vals, in_discharge_vals)\n",
    "    if make in CHARGE_MASK_WITH_SOC_DIFFS_MAKES:\n",
    "        return charge_n_discharging_masks_from_soc_diff(tss)\n",
    "    raise ValueError(MAKE_NOT_SUPPORTED_ERROR.format(make=make))\n",
    "\n",
    "def charge_n_discharging_masks_from_soc_diff( tss:DF) -> DF:\n",
    "    tss_grp = tss.groupby(\"vin\", observed=True)\n",
    "    tss[\"soc_ffilled\"] = tss_grp[\"soc\"].ffill()\n",
    "    tss[\"soc_diff\"] = tss_grp[\"soc_ffilled\"].diff()\n",
    "    tss[\"soc_diff\"] /= tss[\"soc_diff\"].abs()\n",
    "    soc_diff_ffilled = tss_grp[\"soc_diff\"].ffill()\n",
    "    soc_diff_bfilled = tss_grp[\"soc_diff\"].bfill()\n",
    "    tss[\"in_charge\"] = soc_diff_ffilled.gt(0, fill_value=False) & soc_diff_bfilled.gt(0, fill_value=False)\n",
    "    tss[\"in_discharge\"] = soc_diff_ffilled.lt(0, fill_value=False) & soc_diff_bfilled.lt(0, fill_value=False)\n",
    "    return tss\n",
    "\n",
    "def charge_n_discharging_masks_from_charging_status( tss:DF, in_charge_vals:list, in_discharge_vals:list) -> DF:\n",
    "    assert \"charging_status\" in tss.columns, NO_CHARGING_STATUS_COL_ERROR\n",
    "    return (\n",
    "        tss\n",
    "        .eval(f\"in_charge = charging_status in {in_charge_vals}\")\n",
    "        .eval(f\"in_discharge = charging_status in {in_discharge_vals}\")\n",
    "    )\n",
    "\n",
    "def trim_leading_n_trailing_soc_off_masks( tss:DF, masks:list[str]) -> DF:\n",
    "    for mask in masks:\n",
    "        tss[\"naned_soc\"] = tss[\"soc\"].where(tss[mask])\n",
    "        soc_grp = tss.groupby([\"vin\", mask + \"_idx\"], observed=True)[\"naned_soc\"]\n",
    "        trailing_soc = soc_grp.transform(\"first\")\n",
    "        leading_soc = soc_grp.transform(\"last\")\n",
    "        tss[\"trailing_soc\"] = trailing_soc\n",
    "        tss[\"leading_soc\"] = leading_soc\n",
    "        tss[f\"trimmed_{mask}\"] = tss[mask] & (tss[\"soc\"] != trailing_soc) & (tss[\"soc\"] != leading_soc)\n",
    "    tss = tss.drop(columns=\"naned_soc\")\n",
    "    return tss\n",
    "max_td = TD(hours=1, minutes=30)\n",
    "def compute_idx_from_masks( tss: DF, masks:list[str]) -> DF:\n",
    "    for mask in masks:\n",
    "        idx_col_name = f\"{mask}_idx\"\n",
    "        shifted_mask = tss.groupby(\"vin\", observed=True)[mask].shift(fill_value=False)\n",
    "        tss[\"new_period_start_mask\"] = shifted_mask.ne(tss[mask]) \n",
    "        if max_td is not None:\n",
    "            tss[\"new_period_start_mask\"] |= (tss[\"time_diff\"] > max_td)\n",
    "        tss[idx_col_name] = tss.groupby(\"vin\", observed=True)[\"new_period_start_mask\"].cumsum().astype(\"uint16\")\n",
    "        tss.drop(columns=[\"new_period_start_mask\"], inplace=True)\n",
    "    return tss\n",
    "\n",
    "def compute_status_col( tss:DF) -> DF:\n",
    "    tss_grp = tss.groupby(\"vin\", observed=True)\n",
    "    status = tss[\"in_charge\"].map({True: \"charging\", False:\"discharging\", pd.NA:\"unknown\"})\n",
    "    tss[\"status\"] = status.mask(\n",
    "        tss[\"in_charge\"].eq(False, fill_value=True),\n",
    "        np.where(tss_grp[\"odometer\"].diff() > 0, \"moving\", \"idle_discharging\"),\n",
    "    )\n",
    "    return tss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82",
   "metadata": {},
   "outputs": [],
   "source": [
    "processed_tss = run(raw_tss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class TeslaProcessedTimeSeries(ProcessedTimeSeries):\n",
    "\n",
    "    def __init__(self, make:str=\"tesla\", id_col:str=\"vin\", log_level:str=\"INFO\", max_td:TD=MAX_TD, force_update:bool=False, **kwargs):\n",
    "        self.logger = getLogger(make)\n",
    "        set_level_of_loggers_with_prefix(log_level, make)\n",
    "        super().__init__(make, id_col, log_level, max_td, force_update, **kwargs)\n",
    "\n",
    "    def compute_charge_n_discharge_vars(self, tss:DF) -> DF:\n",
    "        return (\n",
    "            tss\n",
    "            .pipe(self.compute_charge_n_discharge_masks)\n",
    "            .pipe(self.compute_charge_idx_bis)\n",
    "            # .pipe(self.compute_idx_from_masks, [\"in_charge\"])\n",
    "            # .pipe(self.trim_leading_n_trailing_soc_off_masks, [\"in_charge\", \"in_discharge\"])\n",
    "            # # .pipe(self.compute_idx_from_masks, [\"trimmed_in_charge\", \"trimmed_in_discharge\"])\n",
    "        )\n",
    "\n",
    "    def compute_charge_n_discharge_masks(self, tss:DF) -> DF:\n",
    "        self.logger.debug(\"Computing tesla specific charge and discharge masks\")\n",
    "        # We use a nullable boolean Series to represnet the rows where:\n",
    "        tss[\"nan_charging\"] = (\n",
    "            Series(pd.NA, index=tss.index, dtype=\"boolean\")# We are not sure of anything.\n",
    "            .mask(tss[\"charging_status\"].isin(IN_CHARGE_CHARGING_STATUS_VALS), True)# We are sure that the vehicle is in charge.\n",
    "            .mask(tss[\"charging_status\"].isin(IN_DISCHARGE_CHARGING_STATUS_VALS), False)# We are sure that the vehicle is not in charge.\n",
    "        )\n",
    "        # If a period of uncertainty (NaN) is surrounded by equal periods of certainties (True-NaN-True or False-NaN-False),\n",
    "        # We will fill them to the value of these certainties.\n",
    "        # However there are edge cases that have multiple days of uncertainties periods (I can't find the VIN but I'm sure you can ;-) )\n",
    "        # Interestingly enough the charge_energy_added variable does not get forwared that far and gets reset to zero. \n",
    "        # This would create outliers in our charge SoH estimation as we estimate the energy_gained as the diff between the last(0) and first value of charge_energy_added.\n",
    "        # So we set a maximal uncertainty period duration over which we don't fill it.\n",
    "        tss[\"nan_date\"] = tss[\"date\"].mask(tss[\"nan_charging\"].isna())\n",
    "        tss[[\"ffill_charging\", \"ffill_date\"]] = tss.groupby(\"vin\", observed=True)[[\"nan_charging\", \"nan_date\"]].ffill()\n",
    "        tss[[\"bfill_charging\", \"bfill_date\"]] = tss.groupby(\"vin\", observed=True)[[\"nan_charging\", \"nan_date\"]].bfill()\n",
    "        nan_period_duration:Series = tss.eval(\"bfill_date - ffill_date\")\n",
    "        fill_unknown_period = tss.eval(\"ffill_charging.eq(bfill_charging) & @nan_period_duration.le(@MAX_CHARGE_TD)\")\n",
    "        tss[\"nan_charging\"] = tss[\"nan_charging\"].mask(fill_unknown_period, tss[\"ffill_charging\"])\n",
    "        # As mentioned before, the SoC oscillates at [charge_limit_soc - ~3%, charge_limit_soc] so we set these periods to NaN as well.\n",
    "        tss[\"nan_charging\"] = tss[\"nan_charging\"].mask(tss[\"soc\"] >= (tss[\"charge_limit_soc\"] - 3))\n",
    "        # Then we seperate the Series into two, more explicit, columns.\n",
    "        tss[\"in_charge\"] = tss.eval(\"nan_charging.notna() & nan_charging\")\n",
    "        tss[\"in_discharge\"] = tss.eval(\"nan_charging.notna() & ~nan_charging\")\n",
    "        return tss.drop(columns=[\"nan_charging\", \"ffill_charging\", \"bfill_charging\", \"ffill_date\", \"bfill_date\"])\n",
    "    \n",
    "    def compute_charge_n_discharge_masks_bis(self, tss:DF) -> DF:\n",
    "        self.logger.debug(\"Computing tesla specific charge and discharge masks\")\n",
    "\n",
    "        tss_na = tss.dropna(subset=['soc']).copy()\n",
    "\n",
    "        tss_na['soc_diff'] = tss_na.groupby('vin', observed=True)['soc'].diff()\n",
    "\n",
    "        tss_na['trend'] = tss_na['soc_diff'].apply(lambda x: 1 if x > 0 else -1 if x < 0 else 0)\n",
    "\n",
    "        #tss_na['trend_change'] = tss_na.groupby('vin', observed=True)['trend'].transform(lambda x: x != x.shift())\n",
    "        tss = tss.merge(tss_na[[\"soc\", \"date\", \"vin\", 'soc_diff', 'trend']], \n",
    "                        on=[\"soc\", \"date\", \"vin\"], how=\"left\")\n",
    "        tss[[\"trend\", \"soc\", \"odometer\",]].bfill(inplace=True)\n",
    "        tss[\"in_charge\"] = tss.eval('trend==1')\n",
    "        tss[\"in_discharge\"] = tss.eval('trend==-1')\n",
    "        return tss\n",
    "    \n",
    "    \n",
    "    def compute_energy_added(self, tss:DF) -> DF:\n",
    "        tss['charge_energy_added'] = tss['dc_charge_energy_added'].where(\n",
    "            tss['dc_charge_energy_added'].notnull() & \n",
    "            (tss['dc_charge_energy_added'] > 0), \n",
    "            tss['ac_charge_energy_added'])\n",
    "        return tss\n",
    "    \n",
    "    # def compute_charge_idx(self, tss:DF) -> DF:\n",
    "    #     self.logger.debug(\"Computing tesla specific charge index.\")\n",
    "    #     if self.make == 'tesla-fleet-telemetry':\n",
    "    #         tss = tss.pipe(self.compute_energy_added)\n",
    "    #     tss_grp = tss.groupby(\"vin\", observed=False)\n",
    "    #     tss[\"charge_energy_added\"] = tss_grp[\"charge_energy_added\"].ffill()\n",
    "    #     energy_added_over_time = tss_grp['charge_energy_added'].diff().div(tss[\"sec_time_diff\"].values)\n",
    "    #     # charge_energy_added is cummulative and forward filled, \n",
    "    #     # We check that the charge_energy_added decreases too fast to make sure that  correctly indentify two charging periods before and after a gap as two separate charging periods.\n",
    "    #     new_charge_mask = energy_added_over_time.lt(MIN_POWER_LOSS, fill_value=0) \n",
    "    #     # For the same reason, we ensure that there are no gaps bigger than MAX_CHARGE_TD in between to rows of the same charging period.\n",
    "    #     new_charge_mask |= tss[\"time_diff\"].gt(MAX_CHARGE_TD) \n",
    "    #     # And of course we also check that there is no change of status. \n",
    "    #     new_charge_mask |= (~tss_grp[\"in_charge\"].shift().bfill() & tss[\"in_charge\"]) \n",
    "    #     tss[\"in_charge_idx\"] = new_charge_mask.groupby(tss[\"vin\"], observed=True).cumsum()\n",
    "    #     print(tss[\"in_charge_idx\"].count() / len(tss))\n",
    "    #     tss[\"in_charge_idx\"] = tss[\"in_charge_idx\"].fillna(-1).astype(\"uint16\")\n",
    "    #     return tss\n",
    "    \n",
    "    def compute_charge_idx_bis(self, tss):\n",
    "\n",
    "        if self.make == 'tesla-fleet-telemetry':\n",
    "                    tss = tss.pipe(self.compute_energy_added)\n",
    "        tss_na = tss.dropna(subset=['soc']).copy()\n",
    "        tss_na['soc_diff'] = tss_na.groupby('vin', observed=True)['soc'].diff()\n",
    "        tss_na['soc_diff_rolling'] = tss_na['soc_diff'].rolling(window=5, min_periods=1).mean()\n",
    "        # Determine trend\n",
    "        tss_na['trend'] = tss_na['soc_diff_rolling'].apply(lambda x: 1 if x > 0 else -1 if x < 0 else np.nan)\n",
    "        tss_na['trend'] = tss_na['trend'].ffill()\n",
    "\n",
    "        def detect_trend_change(group):\n",
    "\n",
    "            group['prev_trend'] = group['trend'].shift(1)\n",
    "            group['prev_prev_trend'] = group['trend'].shift(2)\n",
    "            \n",
    "            group['prev_date'] = group['date'].shift(1)\n",
    "            group['time_diff_min'] = (group['date'] - group['prev_date']).dt.total_seconds() / 60\n",
    "            group['time_gap'] = group['time_diff_min'] > 60  \n",
    "\n",
    "            # Faire une sépration charge_idx et discharge_idx\n",
    "            group['trend_change'] = (\n",
    "                (((group['trend'] != group['prev_trend']) & \n",
    "                  (group['prev_trend'] == group['prev_prev_trend']) ) |\n",
    "                group['time_gap'])\n",
    "            )\n",
    "            group.loc[group.index[0:2], 'trend_change'] = False\n",
    "            return group\n",
    "\n",
    "\n",
    "        tss_na = tss_na.groupby('vin', observed=True).apply(detect_trend_change).reset_index(drop=True)\n",
    "        \n",
    "        # Compute charge id\n",
    "        tss_na['in_charge_idx'] = tss_na.groupby('vin',  observed=True)['trend_change'].cumsum()\n",
    "        tss = tss.merge(tss_na[[\"soc\", \"date\", \"vin\", 'soc_diff', 'in_charge_idx', 'trend', 'prev_trend', 'prev_prev_trend', 'trend_change',]], \n",
    "                        on=[\"soc\", \"date\", \"vin\"], how=\"left\")\n",
    "        tss[[\"odometer\",\"in_charge_idx\"]] = tss[[\"odometer\", \"in_charge_idx\"]].ffill()\n",
    "        return tss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84",
   "metadata": {},
   "outputs": [],
   "source": [
    "processed_tss = TeslaProcessedTimeSeries(\"tesla-fleet-telemetry\", force_update=True)\n",
    "processed_tss['in_charge_idx'] = processed_tss['in_charge_idx'].astype(str)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85",
   "metadata": {},
   "source": [
    "### raw results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86",
   "metadata": {},
   "outputs": [],
   "source": [
    "from core.stats_utils import series_start_end_diff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87",
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_results = (processed_tss.groupby([\"vin\", \"in_charge_idx\"], observed=True, as_index=False).agg(\n",
    "            ac_energy_added_min=pd.NamedAgg(\"ac_charge_energy_added\", \"min\"),\n",
    "            dc_energy_added_min=pd.NamedAgg(\"dc_charge_energy_added\", \"min\"),\n",
    "            ac_energy_added_end=pd.NamedAgg(\"ac_charge_energy_added\", \"last\"),\n",
    "            dc_energy_added_end=pd.NamedAgg(\"dc_charge_energy_added\", \"last\"),\n",
    "            soc_diff=pd.NamedAgg(\"soc\", series_start_end_diff),\n",
    "            inside_temp=pd.NamedAgg(\"inside_temp\", \"mean\"),\n",
    "            net_capacity=pd.NamedAgg(\"net_capacity\", \"first\"),\n",
    "            range=pd.NamedAgg(\"range\", \"first\"),\n",
    "            odometer=pd.NamedAgg(\"odometer\", \"first\"),\n",
    "            version=pd.NamedAgg(\"version\", \"first\"),\n",
    "            size=pd.NamedAgg(\"soc\", \"size\"),\n",
    "            model=pd.NamedAgg(\"model\", \"first\"),\n",
    "            date=pd.NamedAgg(\"date\", \"first\"),\n",
    "            ac_charging_power=pd.NamedAgg(\"ac_charging_power\", \"median\"),\n",
    "            dc_charging_power=pd.NamedAgg(\"dc_charging_power\", \"median\"),\n",
    "            tesla_code=pd.NamedAgg(\"tesla_code\", \"first\"),\n",
    "        )\n",
    "        .eval(\"charging_power = ac_charging_power + dc_charging_power\")\n",
    "        .eval(\"ac_energy_added = ac_energy_added_end  - ac_energy_added_min\")\n",
    "        .eval(\"dc_energy_added = dc_energy_added_end  - dc_energy_added_min\")\n",
    "        .assign(energy_added=lambda df: np.maximum(df[\"ac_energy_added\"], df[\"dc_energy_added\"]))\n",
    "        .eval(\"soh = energy_added / (soc_diff / 100.0 * net_capacity)\")\n",
    "        .eval(\"level_1 = soc_diff * (charging_power < 8) / 100\")\n",
    "        .eval(\"level_2 = soc_diff * (charging_power.between(8, 45)) / 100\")\n",
    "        .eval(\"level_3 = soc_diff * (charging_power > 45) / 100\")\n",
    "        .sort_values([\"tesla_code\", \"vin\", \"date\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88",
   "metadata": {},
   "outputs": [],
   "source": [
    "25.788389 / (40.029564 / 100 * 60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89",
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_results[(raw_results['vin']==\"LRWYGCFS6PC992837\")][['soh', 'odometer', 'soc_diff', \"energy_added\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pour repartir de ce qui est stocké\n",
    "#raw_results_origin = get_results_origin(force_update=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91",
   "metadata": {},
   "source": [
    "### Processed results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transform.processed_results.main import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93",
   "metadata": {},
   "outputs": [],
   "source": [
    "SOH_FILTER_EVAL = {\n",
    "     \"tesla-fleet-telemetry-30\": \"soh = soh.where(soc_diff > 30 & soh.between(0.75, 1.05))\",\n",
    "     \"tesla-fleet-telemetry-25\": \"soh = soh.where(soc_diff > 25 & soh.between(0.75, 1.05))\",\n",
    "     \"tesla-fleet-telemetry-20\": \"soh = soh.where(soc_diff > 20 & soh.between(0.75, 1.05))\",\n",
    "     \"tesla-fleet-telemetry-15\": \"soh = soh.where(soc_diff > 15 & soh.between(0.75, 1.05))\",\n",
    "     \"tesla-fleet-telemetry-8\": \"soh = soh.where(soc_diff > 8 & soh.between(0.75, 1.05))\",\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_processed_results(brand:str) -> DF:\n",
    "    logger.info(f\"{'Processing ' + brand + ' results.':=^{50}}\")\n",
    "    results =  (\n",
    "        raw_results\n",
    "        # Some raw estimations may have inf values, this will make mask_out_outliers_by_interquartile_range and force_monotonic_decrease fail\n",
    "        # So we replace them by NaNs.\n",
    "        .assign(soh=lambda df: df[\"soh\"].replace([np.inf, -np.inf], np.nan))\n",
    "        .sort_values([\"vin\", \"date\"])\n",
    "        .pipe(make_charge_levels_presentable)\n",
    "        .eval(SOH_FILTER_EVAL[brand])\n",
    "        .pipe(agg_results_by_update_frequency)\n",
    "        .groupby('vin', observed=True)\n",
    "        .apply(make_soh_presentable_per_vehicle, include_groups=False)\n",
    "        .reset_index(level=0)\n",
    "        #.pipe(filter_results_by_lines_bounds, VALID_SOH_POINTS_LINE_BOUNDS, logger=logger)\n",
    "        .sort_values([\"vin\", \"date\"])\n",
    "    )\n",
    "    results[\"soh\"] = results.groupby(\"vin\", observed=True)[\"soh\"].ffill()\n",
    "    results[\"soh\"] = results.groupby(\"vin\", observed=True)[\"soh\"].bfill()\n",
    "    results[\"odometer\"] = results.groupby(\"vin\", observed=True)[\"odometer\"].ffill()\n",
    "    results[\"odometer\"] = results.groupby(\"vin\", observed=True)[\"odometer\"].bfill()\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95",
   "metadata": {},
   "outputs": [],
   "source": [
    "processed_results_30 = get_processed_results('tesla-fleet-telemetry-30')\n",
    "processed_results_25 = get_processed_results('tesla-fleet-telemetry-25')\n",
    "processed_results_20 = get_processed_results('tesla-fleet-telemetry-20')\n",
    "processed_results_15 = get_processed_results('tesla-fleet-telemetry-15')\n",
    "processed_results_8 = get_processed_results('tesla-fleet-telemetry-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96",
   "metadata": {},
   "outputs": [],
   "source": [
    "px.scatter(raw_results[(raw_results['soh'] >.7) &(raw_results['soh'] < 1.05)].dropna(subset='soh'), x='odometer', y='soh', color='vin')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97",
   "metadata": {},
   "outputs": [],
   "source": [
    "px.scatter(processed_results_30[(processed_results_30['soh'] > .75) &(processed_results_30['soh'] < 1.05)].dropna(subset='soh'), x='odometer', y='soh', color='vin')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "data_ev",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

