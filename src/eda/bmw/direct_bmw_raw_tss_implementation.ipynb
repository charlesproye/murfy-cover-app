{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Raw tss from direct bmw responses\n",
    "The goal of this notebook is to demonstrate how to parse the raw tss from direct bmw responses."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from functools import reduce\n",
    "\n",
    "from core.pandas_utils import *\n",
    "from core.singleton_s3_bucket import bucket"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Experimantation\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's first take a look at an example response."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "EXAMPLE_KEY = \"response/BMW/WBY71AW000FM68170/2024-12-02.json\"\n",
    "response = bucket.read_json_file(EXAMPLE_KEY)\n",
    "response"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's list all the responses that we will have to parse."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "responses = bucket.list_responses_keys_of_brand(\"BMW\")\n",
    "responses"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The responses are lists of dicts where the dicts are not a line but a single element.    \n",
    "Example:  \n",
    "```json\n",
    "  'date_of_value': '2024-10-13T18:16:01Z'},\n",
    " {'key': 'charging_status',\n",
    "  'value': 'NOCHARGING',\n",
    "  'unit': None,\n",
    "  'info': None,\n",
    "  'date_of_value': '2024-10-13T17:19:33Z'},\n",
    " {'key': 'mileage',\n",
    "  'value': '149510.0',\n",
    "  'unit': 'km',\n",
    "  'info': None,\n",
    "  'date_of_value': '2024-10-13T17:19:33Z'},\n",
    " {'key': 'charging_ac_ampere',\n",
    "  'value': '0',\n",
    "  'unit': 'A',\n",
    "  'info': None,\n",
    "  'date_of_value': '2024-10-13T17:19:33Z'},\n",
    " {'key': 'charging_ac_voltage',\n",
    "  'value': '0.0',\n",
    "  'unit': 'V',\n",
    "  'info': None,\n",
    "  'date_of_value': '2024-10-13T17:19:33Z'},\n",
    "```\n",
    "When parsing it into a dataframe we will have to pivot the dataframe.    \n",
    "The structure is convinient because it allows us to first concatenate the lists and then pivot once."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's take the responses of a single vin and parse it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "responses_dicts = responses.query(\"vin == 'WBY1Z610407A12415'\")[\"key\"].apply(bucket.read_json_file)\n",
    "display(responses_dicts)\n",
    "cat_responses_dicts = reduce(lambda cat_rep, rep_2: cat_rep + rep_2[\"data\"], responses_dicts, [])\n",
    "display(cat_responses_dicts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see what we get when we parse it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unpivoted_df = DF.from_dict(cat_responses_dicts).drop(columns=[\"unit\", \"info\"])\n",
    "unpivoted_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that there are duplicate dates and keys."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unpivoted_df.drop_duplicates(subset=[\"date_of_value\", \"value\"])[\"date_of_value\"].value_counts(sort=True, ascending=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we were to pivot it we would raise an error because there are multiple dates for the same key.  \n",
    "This is most likely due to the fact that responses close to each other in time sometime contain the same element.  \n",
    "Therefore, we need to drop duplicates before pivoting.  \n",
    "\n",
    "Here we take the most common date and view the duplicates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unpivoted_df.query(\"date_of_value == '2024-11-06T15:48:10Z'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can sse that there are duplicates, if you look at the output of the above cell you can see that the values equals so we can drop them without loosing any data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unpivoted_df.drop_duplicates(subset=[\"date_of_value\", \"key\", \"value\"]).query(\"date_of_value == '2024-11-06T15:48:10Z'\")[[\"date_of_value\", \"key\"]].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unpivoted_df.drop_duplicates(subset=[\"date_of_value\", \"key\", \"value\"]).query(\"date_of_value == '2024-11-06T15:48:10Z'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's check the count of dates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unpivoted_df[\"date_of_value\"].count() / len(unpivoted_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that there are a lot of NaT date values which would cause an equivalent loss of data.  \n",
    "To remedy this we will ffill and bfill the dates before pivoting.  \n",
    "We can allow our self to do this since the elements that are close in the list should be close in time.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unpivoted_df[\"date_of_value\"].ffill().bfill().count() / len(unpivoted_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = (\n",
    "    unpivoted_df\n",
    "    .eval(\"date_of_value = date_of_value.ffill().bfill()\")\n",
    "    .drop_duplicates(subset=[\"date_of_value\", \"key\"])\n",
    "    .pivot(index=\"date_of_value\", columns=\"key\", values=\"value\")\n",
    ")\n",
    "\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sanity check"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's check that the frequency of the data is correct.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "min_date = df.reset_index()[\"date_of_value\"].pipe(pd.to_datetime, format=\"mixed\").min()\n",
    "max_date = df.reset_index()[\"date_of_value\"].pipe(pd.to_datetime, format=\"mixed\").max()\n",
    "\n",
    "duration = (max_date - min_date).total_seconds()\n",
    "freq = len(df) / duration\n",
    "freq * 3600"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The frequency is fairly low...."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's check the number of notna values per row."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.count(axis=1).describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Final implementation\n",
    "\n",
    "Let's implement this for all the vins."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_responses(responses:DF) -> DF:\n",
    "    print(\"reading responses of\", responses.name, end=\"\")\n",
    "    responses_dicts = responses[\"key\"].apply(bucket.read_json_file)\n",
    "    print(\", concatenating...\", end=\"\")\n",
    "    cat_responses_dicts = reduce(lambda cat_rep, rep_2: cat_rep + rep_2[\"data\"], responses_dicts, [])\n",
    "    print(\"Parsing reps.\")\n",
    "    return (\n",
    "        DF.from_dict(cat_responses_dicts)\n",
    "        .drop(columns=[\"unit\", \"info\"])\n",
    "        .eval(\"date_of_value = date_of_value.ffill().bfill()\")\n",
    "        .drop_duplicates(subset=[\"date_of_value\", \"key\"])\n",
    "        .pivot(index=\"date_of_value\", columns=\"key\", values=\"value\")\n",
    "        .assign(vin=responses.name)\n",
    "    )\n",
    "\n",
    "raw_tss = (\n",
    "    responses\n",
    "    .groupby(\"vin\")\n",
    "    .apply(parse_responses, include_groups=False)\n",
    ")\n",
    "\n",
    "raw_tss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sanity_check(raw_tss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_tss.drop(columns=[\"vin\"]).reset_index(drop=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "The frequency of the data is still a bit too low.  \n",
    "We need to fix the NaT dates.  \n",
    "And we should be good üëç."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "data_ev",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

