{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Raw results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from core.s3.s3_utils import S3Service\n",
    "from core.s3.settings import S3Settings\n",
    "from core.spark_utils import create_spark_session\n",
    "from pyspark.sql.functions import col\n",
    "\n",
    "settings = S3Settings()\n",
    "\n",
    "bucket = S3Service(settings)\n",
    "\n",
    "spark_session = create_spark_session(\n",
    "        settings.S3_KEY,\n",
    "        settings.S3_SECRET\n",
    ")\n",
    "\n",
    "\n",
    "# Lire avec mergeSchema activé\n",
    "df_spark = bucket.read_parquet_df_spark(spark_session, \"raw_results/spark_tesla-fleet-telemetry.parquet/vin=5YJSA7E52RF541858/part-00089-25860fa2-18e0-4817-94cb-8fd8ad12f12f.c000.snappy.parquet\")\n",
    "df_spark_vin = df_spark.toPandas()\n",
    "\n",
    "date_mask = (\n",
    "    (df_spark_vin['date'] >= '2025-04-24') & \n",
    "    (df_spark_vin['date'] <= '2025-05-01')\n",
    ")\n",
    "\n",
    "# Appliquer le filtre\n",
    "df_filtered = df_spark_vin[date_mask]\n",
    "\n",
    "# Lire avec mergeSchema activé\n",
    "df_no_spark = bucket.read_parquet_df(\"raw_results/tesla-fleet-telemetry.parquet\")\n",
    "\n",
    "\n",
    "date_mask = (\n",
    "    (df_no_spark['date'] >= '2025-04-24') & \n",
    "    (df_no_spark['date'] <= '2025-05-01') &\n",
    "    (df_no_spark.vin == '5YJSA7E52RF541858')\n",
    ")\n",
    "# Appliquer le filtre\n",
    "df_filtered_no_spark = df_no_spark[date_mask]\n",
    "df_filtered_no_spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_filtered[['date', 'in_charge_idx']].sort_values('date', ascending=True).tail(100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df_filtered.in_charge_idx.min())\n",
    "print(df_filtered.in_charge_idx.max())\n",
    "print(df_filtered.shape)\n",
    "\n",
    "print(df_filtered_no_spark.in_charge_idx.min())\n",
    "print(df_filtered_no_spark.in_charge_idx.max())\n",
    "print(df_filtered_no_spark.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_filtered_no_spark[['date', 'soc_diff']]['soc_diff'].sum()\n",
    "df_filtered_no_spark['level_1'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(left[left.level_1 >0].level_1.sum())\n",
    "print(right[right.level_1 >0].level_1.sum())\n",
    "\n",
    "print(left[left.level_2 >0].level_2.sum())\n",
    "print(right[right.level_2 >0].level_2.sum())\n",
    "\n",
    "print(left[left.level_3 >0].level_3.sum())\n",
    "print(right[right.level_3 >0].level_3.sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(left[left.charging_power >0].charging_power.sum())\n",
    "print(right[right.charging_power >0].charging_power.sum())\n",
    "\n",
    "print(left[left.level_2 >0].level_2.sum())\n",
    "print(right[right.level_2 >0].level_2.sum())\n",
    "\n",
    "print(left[left.level_3 >0].level_3.sum())\n",
    "print(right[right.level_3 >0].level_3.sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "left = df_filtered[['date', 'ac_charging_power','dc_charging_power','charging_power']]\n",
    "right = df_filtered_no_spark[['date', 'ac_charging_power','dc_charging_power','charging_power']]\n",
    "\n",
    "left.merge(right, on=['date'], how='outer', suffixes=('_spark', '_no_spark'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Processed tss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lire avec mergeSchema activé\n",
    "ptss_spark = bucket.read_parquet_df_spark(spark_session, \"processed_ts/tesla-fleet-telemetry/time_series/processed_tss_spark.parquet/vin=5YJSA7E52RF541858/part-00000-0554e60c-8879-4176-b4b5-6568ff87865f.c000.snappy.parquet\")\n",
    "ptss_spark = ptss_spark.to_pandas_on_spark()\n",
    "\n",
    "date_mask = (\n",
    "    (ptss_spark['date'] >= '2025-04-24') & \n",
    "    (ptss_spark['date'] <= '2025-05-08')\n",
    ")\n",
    "\n",
    "# Appliquer le filtre\n",
    "ptss_filtered = ptss_spark[date_mask]\n",
    "\n",
    "\n",
    "\"\"\"# Lire avec mergeSchema activé\n",
    "ptss_no_spark = bucket.read_parquet_df(\"processed_ts/tesla-fleet-telemetry/time_series/processed_tss.parquet\", filters=[(\"vin\", \"==\", \"5YJSA7E52RF541858\")])\n",
    "\n",
    "date_mask = (\n",
    "    (ptss_no_spark['date'] >= '2025-4-24') & \n",
    "    (ptss_no_spark['date'] <= '2025-05-08') &\n",
    "    (ptss_no_spark.vin == '5YJSA7E52RF541858')\n",
    ")\n",
    "# Appliquer le filtre\n",
    "ptss_filtered_no_spark = ptss_no_spark[date_mask]\n",
    "ptss_filtered_no_spark\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ptss_filtered_no_spark_socdiff = ptss_filtered_no_spark.dropna(subset=['soc']).copy()\n",
    "ptss_filtered_no_spark_socdiff['soc_diff'] = ptss_filtered_no_spark_socdiff.groupby('vin', observed=True)['soc'].diff() \n",
    "ptss_filtered_no_spark = ptss_filtered_no_spark.merge(ptss_filtered_no_spark_socdiff[[\"soc\", \"date\", \"vin\", 'soc_diff']], \n",
    "                        on=[\"soc\", \"date\", \"vin\"], how=\"left\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ptss_filtered[['date', 'in_charge_idx', 'soc_diff', 'in_charge', 'soc']].toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "\n",
    "df_plot = ptss_filtered_no_spark[['date', 'in_charge_idx', 'soc_diff', 'in_charge', 'soc']]\n",
    "\n",
    "# Créer une palette rouge-vert alternée\n",
    "unique_charges = sorted(df_plot['in_charge_idx'].unique())\n",
    "colors = ['black', 'gray'] * (len(unique_charges) // 2 + 1)\n",
    "\n",
    "# Créer le graphique interactif\n",
    "fig = go.Figure()\n",
    "\n",
    "for i, charge_idx in enumerate(unique_charges):\n",
    "    subset = df_plot[df_plot['in_charge_idx'] == charge_idx]\n",
    "    fig.add_trace(go.Scatter(\n",
    "        x=subset['date'],\n",
    "        y=subset['soc'],\n",
    "        mode='markers',\n",
    "        name=f'Charge {charge_idx}',\n",
    "        marker=dict(\n",
    "            color=colors[i],\n",
    "            size=8,\n",
    "            opacity=0.7\n",
    "        ),\n",
    "        hovertemplate='<b>Date:</b> %{x}<br>' +\n",
    "                     '<b>SOC:</b> %{y}<br>' +\n",
    "                     '<b>Charge:</b> ' + str(charge_idx) + '<br>' +\n",
    "                     '<extra></extra>'\n",
    "    ))\n",
    "\n",
    "# Personnaliser le graphique\n",
    "fig.update_layout(\n",
    "    title='Soc par date - Rouge/Vert alternés par session de charge (Zoomable)',\n",
    "    xaxis_title='Date',\n",
    "    yaxis_title='SOC',\n",
    "    width=1200,\n",
    "    height=600,\n",
    "    hovermode='closest'\n",
    ")\n",
    "\n",
    "# Activer le zoom et la sélection\n",
    "fig.update_layout(\n",
    "    dragmode='zoom',  # Mode zoom par défaut\n",
    "    selectdirection='any'\n",
    ")\n",
    "\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "\n",
    "df_plot = ptss_filtered[['date', 'in_charge_idx', 'soc_diff', 'in_charge', 'soc']].to_pandas()\n",
    "\n",
    "# Créer une palette rouge-vert alternée\n",
    "unique_charges = sorted(df_plot['in_charge_idx'].unique())\n",
    "\n",
    "\n",
    "colors = ['red', 'green'] * (len(unique_charges) // 2 + 1)\n",
    "\n",
    "# Créer le graphique interactif\n",
    "fig = go.Figure()\n",
    "\n",
    "for i, charge_idx in enumerate(unique_charges):\n",
    "    subset = df_plot[df_plot['in_charge_idx'] == charge_idx]\n",
    "    fig.add_trace(go.Scatter(\n",
    "        x=subset['date'],\n",
    "        y=subset['soc'],\n",
    "        mode='markers',\n",
    "        name=f'Charge {charge_idx}',\n",
    "        marker=dict(\n",
    "            color=colors[i],\n",
    "            size=8,\n",
    "            opacity=0.7\n",
    "        ),\n",
    "        hovertemplate='<b>Date:</b> %{x}<br>' +\n",
    "                     '<b>SOC:</b> %{y}<br>' +\n",
    "                     '<b>Charge:</b> ' + str(charge_idx) + '<br>' +\n",
    "                     '<extra></extra>'\n",
    "    ))\n",
    "\n",
    "# Personnaliser le graphique\n",
    "fig.update_layout(\n",
    "    title='Soc par date - Rouge/Vert alternés par session de charge (Zoomable)',\n",
    "    xaxis_title='Date',\n",
    "    yaxis_title='SOC',\n",
    "    width=1200,\n",
    "    height=600,\n",
    "    hovermode='closest'\n",
    ")\n",
    "\n",
    "# Activer le zoom et la sélection\n",
    "fig.update_layout(\n",
    "    dragmode='zoom',  # Mode zoom par défaut\n",
    "    selectdirection='any'\n",
    ")\n",
    "\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ptss_filtered.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pickletools import float8\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "\n",
    "def label_phases_with_pause(df, threshold=0.4, max_pause_minutes=10):\n",
    "    df = df.copy()\n",
    "    df['date'] = pd.to_datetime(df['date'])\n",
    "    df = df.sort_values('date').reset_index(drop=True)\n",
    "\n",
    "    df['phase'] = np.nan\n",
    "    cum_delta = 0\n",
    "    start_idx = None\n",
    "    current_trend = None\n",
    "    current_phase = None\n",
    "    in_phase = False\n",
    "\n",
    "    for i in range(1, len(df)):\n",
    "        diff = df.loc[i, 'soc_diff']\n",
    "\n",
    "        if pd.isna(diff):\n",
    "            continue\n",
    "\n",
    "        time_gap = (df.loc[i, 'date'] - df.loc[i - 1, 'date']).total_seconds() / 60\n",
    "\n",
    "        if time_gap > max_pause_minutes:\n",
    "            # Reset de la phase\n",
    "            cum_delta = 1 if np.sign(df.loc[i+1, 'soc_diff']) == 1 else  -1\n",
    "            current_trend = 1 if np.sign(df.loc[i+1, 'soc_diff']) == 1 else  -1\n",
    "            current_phase = None\n",
    "            in_phase = False\n",
    "            start_idx = i - 1\n",
    "\n",
    "        direction = np.sign(diff)\n",
    "\n",
    "        if current_trend is None or (direction != current_trend and diff != 0):\n",
    "            current_trend = direction if diff != 0 else current_trend\n",
    "            cum_delta = diff\n",
    "            start_idx = i - 1\n",
    "            current_phase = None\n",
    "            in_phase = False\n",
    "            continue\n",
    "\n",
    "        cum_delta += diff\n",
    "\n",
    "        if not in_phase and abs(cum_delta) >= threshold:\n",
    "            current_phase = \"charging\" if current_trend > 0 else \"discharging\"\n",
    "            in_phase = True\n",
    "            df.loc[start_idx:i, 'phase'] = current_phase\n",
    "        elif in_phase:\n",
    "            df.loc[i, 'phase'] = current_phase\n",
    "\n",
    "    df['phase'] = df['phase'].ffill()\n",
    "\n",
    "    if df['phase'].isnull().any():\n",
    "        first_valid = df['phase'].dropna().iloc[0]\n",
    "        df['phase'] = df['phase'].fillna(first_valid)\n",
    "\n",
    "    return df\n",
    "\n",
    "\n",
    "df_plot = ptss_filtered[['date', 'in_charge_idx', 'soc_diff', 'in_charge', 'soc', 'sec_time_diff']].to_pandas()\n",
    "df_plot = label_phases_with_pause(df_plot)\n",
    "\n",
    "df_plot = df_plot.sort_values('date').reset_index(drop=True)\n",
    "df_plot['phase_change'] = (df_plot['phase'] != df_plot['phase'].shift(1))  # True quand la phase change\n",
    "df_plot['phase_id'] = df_plot['phase_change'].cumsum()               # Incrémente à chaque changement\n",
    "\n",
    "\n",
    "\n",
    "import plotly.graph_objects as go\n",
    "\n",
    "# Créer le graphique interactif avec couleurs par phase\n",
    "fig = go.Figure()\n",
    "\n",
    "# Définir les couleurs pour chaque phase\n",
    "phase_colors = {\n",
    "    'charging': 'green',\n",
    "    'discharging': 'red', \n",
    "    'idle': 'gray'\n",
    "}\n",
    "\n",
    "# Tracer chaque phase avec sa couleur\n",
    "for phase in df_plot['phase'].unique():\n",
    "    subset = df_plot[df_plot['phase'] == phase]\n",
    "    fig.add_trace(go.Scatter(\n",
    "        x=subset['date'],\n",
    "        y=subset['soc'],\n",
    "        mode='markers',\n",
    "        name=f'{phase.capitalize()}',\n",
    "        marker=dict(\n",
    "            color=phase_colors[phase],\n",
    "            size=8,\n",
    "            opacity=0.7\n",
    "        ),\n",
    "        hovertemplate='<b>Date:</b> %{x}<br>' +\n",
    "                     '<b>SOC:</b> %{y}<br>' +\n",
    "                     '<b>Phase:</b> ' + phase + '<br>' +\n",
    "                     '<b>Charge Index:</b> %{customdata}<br>' +\n",
    "                     '<extra></extra>',\n",
    "        customdata=subset['in_charge_idx']\n",
    "    ))\n",
    "\n",
    "# Ajouter une ligne transparente avec les phase_id\n",
    "fig.add_trace(go.Scatter(\n",
    "    x=df_plot['date'],\n",
    "    y=df_plot['phase_id'],\n",
    "    mode='lines',\n",
    "    name='Phase ID',\n",
    "    line=dict(\n",
    "        color='black',\n",
    "        width=2,\n",
    "        backoff=0.3\n",
    "    ),\n",
    "    yaxis='y2',  # Utiliser un second axe Y\n",
    "    hovertemplate='<b>Date:</b> %{x}<br>' +\n",
    "                 '<b>Phase ID:</b> %{y}<br>' +\n",
    "                 '<extra></extra>'\n",
    "))\n",
    "\n",
    "# Personnaliser le graphique avec deux axes Y\n",
    "fig.update_layout(\n",
    "    title='Soc par date - Coloré par phase avec Phase ID',\n",
    "    xaxis_title='Date',\n",
    "    yaxis_title='SOC',\n",
    "    yaxis2=dict(\n",
    "        title='Phase ID',\n",
    "        overlaying='y',\n",
    "        side='right',\n",
    "        showgrid=False\n",
    "    ),\n",
    "    width=1200,\n",
    "    height=600,\n",
    "    hovermode='closest'\n",
    ")\n",
    "\n",
    "# Activer le zoom et la sélection\n",
    "fig.update_layout(\n",
    "    dragmode='zoom',\n",
    "    selectdirection='any'\n",
    ")\n",
    "\n",
    "fig.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "print('Transform part', datetime.now())\n",
    "\n",
    "def label_phases_with_pause(df, threshold=0.5, max_pause_minutes=20):\n",
    "    print('Sort', datetime.now())\n",
    "    df = df.sort_values('date').reset_index(drop=True)\n",
    "\n",
    "    df['phase'] = np.nan\n",
    "    cum_delta = 0\n",
    "    start_idx = None\n",
    "    current_trend = None\n",
    "    current_phase = None\n",
    "    in_phase = False\n",
    "\n",
    "    print('Loop', datetime.now())\n",
    "    for i in range(1, len(df)):\n",
    "        diff = df.loc[i, 'soc_diff']\n",
    "\n",
    "        if pd.isna(diff):\n",
    "            continue\n",
    "\n",
    "        time_gap = (df.loc[i, 'date'] - df.loc[i - 1, 'date']).total_seconds() / 60\n",
    "\n",
    "        if time_gap > max_pause_minutes:\n",
    "            # Reset de la phase\n",
    "            cum_delta = 1 if np.sign(df.loc[i+1, 'soc_diff']) == 1 else  1\n",
    "            current_trend = np.sign(diff) if diff != 0 else 0\n",
    "            current_phase = None\n",
    "            in_phase = False\n",
    "            start_idx = i - 1\n",
    "\n",
    "        direction = np.sign(diff)\n",
    "\n",
    "        if current_trend is None or (direction != current_trend and diff != 0):\n",
    "            current_trend = direction if diff != 0 else current_trend\n",
    "            cum_delta = diff\n",
    "            start_idx = i - 1\n",
    "            current_phase = None\n",
    "            in_phase = False\n",
    "            continue\n",
    "\n",
    "        cum_delta += diff\n",
    "\n",
    "        if not in_phase and abs(cum_delta) >= threshold:\n",
    "            current_phase = \"charging\" if current_trend > 0 else \"discharging\"\n",
    "            in_phase = True\n",
    "            df.loc[start_idx:i, 'phase'] = current_phase\n",
    "        elif in_phase:\n",
    "            df.loc[i, 'phase'] = current_phase\n",
    "\n",
    "    print('Ajsut cols', datetime.now())\n",
    "    df['phase'] = df['phase'].ffill()\n",
    "    df['phase_id'] = (df['phase'] != df['phase'].shift(1)).cumsum()   \n",
    "\n",
    "    if df['phase'].isnull().any():\n",
    "        first_valid = df['phase'].dropna().iloc[0]\n",
    "        df['phase'] = df['phase'].fillna(first_valid)\n",
    "\n",
    "    return df\n",
    "\n",
    "\n",
    "\"\"\"def idle_status(df,threshold_charge:float=0.1, threshold_discharge:float=0.05, gap_mn:float=30):\n",
    "    df_analysis = df.copy()\n",
    "    df = df_analysis.sort_values('date').reset_index(drop=True)\n",
    "    df['readable_date'] = df['date']\n",
    "    df = df.set_index('readable_date')\n",
    "\n",
    "    df['cumsum_diff'] = df['soc_diff'].rolling(\n",
    "        window=f'{gap_mn}T', min_periods=1\n",
    "    ).sum()\n",
    "\n",
    "    df = df.reset_index(drop=True)\n",
    "    \n",
    "    # Parcourir chaque point\n",
    "    for i in range(len(df)):\n",
    "        current_phase = df.loc[i, 'phase']\n",
    "        mn_since_phase = df.loc[i, 'cumsum_mn_since_phase']\n",
    "        if current_phase == 'charging':\n",
    "            current_cumsum = df.loc[i, 'cumsum_diff']\n",
    "        else:\n",
    "            current_cumsum = df.loc[i, 'cumsum_diff']\n",
    "        # Si c'est une phase discharging et que le cumsum est au-dessus du seuil\n",
    "        if mn_since_phase > gap_mn:\n",
    "            if current_phase == 'discharging' and current_cumsum > - threshold_discharge:\n",
    "                # Trouver l'index de début (30 minutes en arrière)\n",
    "                current_time = df.loc[i, 'date']\n",
    "                start_time = current_time - pd.Timedelta(minutes=gap_mn)\n",
    "                \n",
    "                # Trouver tous les points dans la fenêtre de 30 minutes précédentes\n",
    "                mask = ((df['date'] >= start_time) & (df['date'] <= current_time) & (df['phase'] == 'discharging') )#& (df.cumsum_diff > threshold_charge))\n",
    "                \n",
    "                # Remplacer par 'idle'\n",
    "                df.loc[mask, 'phase'] = 'idle'\n",
    "                \n",
    "            # Si c'est une phase charging et que le cumsum est en-dessous du seuil\n",
    "            elif current_phase == 'charging' and current_cumsum < threshold_charge:\n",
    "                # Trouver l'index de début (30 minutes en arrière)\n",
    "                current_time = df.loc[i, 'date']\n",
    "                start_time = current_time - pd.Timedelta(minutes=gap_mn)\n",
    "                \n",
    "                # Trouver tous les points dans la fenêtre de 30 minutes précédentes\n",
    "                mask = ((df['date'] >= start_time) & (df['date'] <= current_time) & (df['phase'] == 'charging') )# & (df.cumsum_diff < threshold_charge))\n",
    "                \n",
    "                # Remplacer par 'idle'\n",
    "                df.loc[mask, 'phase'] = 'idle'\n",
    "\n",
    "    return df\"\"\"\n",
    "\n",
    "def idle_status(df, threshold_charge: float = 0.1, threshold_discharge: float = 0.05, gap_mn: float = 30):\n",
    "    df_analysis = df.copy()\n",
    "    df_analysis = df_analysis.sort_values('date').reset_index(drop=True)\n",
    "    \n",
    "    df_analysis['readable_date'] = df_analysis['date']\n",
    "    df_analysis = df_analysis.set_index('readable_date')\n",
    "\n",
    "    # Rolling window pour le cumul de soc_diff sur la période\n",
    "    rolling_soc = df_analysis['soc_diff'].rolling(f'{gap_mn}T', min_periods=1).sum()\n",
    "    df_analysis['cumsum_diff'] = rolling_soc\n",
    "\n",
    "    df_analysis = df_analysis.reset_index(drop=True)\n",
    "\n",
    "    # Préparation des masques logiques\n",
    "    is_discharge = df_analysis['phase'] == 'discharging'\n",
    "    is_charge = df_analysis['phase'] == 'charging'\n",
    "    enough_time = df_analysis['cumsum_mn_since_phase'] > gap_mn\n",
    "\n",
    "    # Création des conditions pour idle\n",
    "    idle_discharge_mask = is_discharge & enough_time & (df_analysis['cumsum_diff'] > -threshold_discharge)\n",
    "    idle_charge_mask = is_charge & enough_time & (df_analysis['cumsum_diff'] < threshold_charge)\n",
    "\n",
    "    # Marquer les lignes à remplacer par 'idle'\n",
    "    df_analysis.loc[idle_discharge_mask, 'phase'] = 'idle'\n",
    "    df_analysis.loc[idle_charge_mask, 'phase'] = 'idle'\n",
    "\n",
    "    return df_analysis\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "print('TO pandas', datetime.now())\n",
    "df_plot = ptss_filtered.to_pandas()\n",
    "print('Label phase', datetime.now())\n",
    "df_plot = label_phases_with_pause(df_plot)\n",
    "\n",
    "df_plot = df_plot.sort_values('date').reset_index(drop=True) # True quand la phase change          # Incrémente à chaque changement\n",
    "# df_plot['cumsum_since_phase'] = df_plot.groupby('phase_id')['soc_diff'].cumsum()\n",
    "print('Cumsum', datetime.now())\n",
    "df_plot['cumsum_mn_since_phase'] = df_plot.groupby('phase_id')['sec_time_diff'].cumsum() / 60\n",
    "print('Idle', datetime.now())\n",
    "df_plot = idle_status(df_plot)\n",
    "\n",
    "df_plot['phase_id'] = (df_plot['phase'] != df_plot['phase'].shift(1)).cumsum()   \n",
    "\n",
    "\n",
    "print('Plot part', datetime.now())\n",
    "\n",
    "\n",
    "import plotly.graph_objects as go\n",
    "\n",
    "# Créer le graphique interactif avec couleurs par phase\n",
    "fig = go.Figure()\n",
    "\n",
    "# Définir les couleurs pour chaque phase\n",
    "phase_colors = {\n",
    "    'charging': 'green',\n",
    "    'discharging': 'red', \n",
    "    'idle': 'gray'\n",
    "}\n",
    "\n",
    "# Tracer chaque phase avec sa couleur\n",
    "for phase in df_plot['phase'].unique():\n",
    "    subset = df_plot[df_plot['phase'] == phase]\n",
    "    fig.add_trace(go.Scatter(\n",
    "        x=subset['date'],\n",
    "        y=subset['soc'],\n",
    "        mode='markers',\n",
    "        name=f'{phase.capitalize()}',\n",
    "        marker=dict(\n",
    "            color=phase_colors[phase],\n",
    "            size=8,\n",
    "            opacity=0.7\n",
    "        ),\n",
    "        hovertemplate='<b>Date:</b> %{x}<br>' +\n",
    "                     '<b>SOC:</b> %{y}<br>' +\n",
    "                     '<b>Phase:</b> ' + phase + '<br>' +\n",
    "                     '<b>Charge Index:</b> %{customdata}<br>' +\n",
    "                     '<extra></extra>',\n",
    "        customdata=subset['in_charge_idx']\n",
    "    ))\n",
    "\n",
    "# Ajouter une ligne transparente avec les phase_id\n",
    "fig.add_trace(go.Scatter(\n",
    "    x=df_plot['date'],\n",
    "    y=df_plot['phase_id'],\n",
    "    mode='lines',\n",
    "    name='Phase ID',\n",
    "    line=dict(\n",
    "        color='black',\n",
    "        width=2,\n",
    "        backoff=0.3\n",
    "    ),\n",
    "    yaxis='y2',  # Utiliser un second axe Y\n",
    "    hovertemplate='<b>Date:</b> %{x}<br>' +\n",
    "                 '<b>Phase ID:</b> %{y}<br>' +\n",
    "                 '<extra></extra>'\n",
    "))\n",
    "\n",
    "# Personnaliser le graphique avec deux axes Y\n",
    "fig.update_layout(\n",
    "    title='Soc par date - Coloré par phase avec Phase ID',\n",
    "    xaxis_title='Date',\n",
    "    yaxis_title='SOC',\n",
    "    yaxis2=dict(\n",
    "        title='Phase ID',\n",
    "        overlaying='y',\n",
    "        side='right',\n",
    "        showgrid=False\n",
    "    ),\n",
    "    width=1200,\n",
    "    height=600,\n",
    "    hovermode='closest'\n",
    ")\n",
    "\n",
    "# Activer le zoom et la sélection\n",
    "fig.update_layout(\n",
    "    dragmode='zoom',\n",
    "    selectdirection='any'\n",
    ")\n",
    "\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Spak"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_plot[['date', 'soc', 'soc_diff', 'cumsum_diff', 'cumsum_since_phase', 'cumsum_mn_since_phase', 'phase', 'phase_id']][(df_plot.soc.notna()) & (df_plot.date >= '2025-05-07 12:45:00') & (df_plot.date <= '2025-05-07 19:00:00')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Charger ta data d'exemple (adapter selon ton df)\n",
    "df = df_plot.copy()\n",
    "df['date'] = pd.to_datetime(df['date'])\n",
    "df = df.sort_values('date').reset_index(drop=True)\n",
    "\n",
    "max_pause_minutes = 60\n",
    "\n",
    "for i in range(1, len(df)):\n",
    "    delta_min = (df.loc[i, 'date'] - df.loc[i-1, 'date']).total_seconds() / 60\n",
    "    if delta_min > max_pause_minutes:\n",
    "        print(f\"Pause détectée > {max_pause_minutes} minutes entre index {i-1} et {i}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_plot[(df_plot.soc.notna()) & (df_plot.date >= '2025-04-28 09:00:00') & (df_plot.date <= '2025-04-30 11:40:00')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(ptss_filtered_no_spark.columns)\n",
    "ptss_filtered_no_spark.in_charge_idx.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ptss_filtered_no_spark_socdiff = ptss_filtered_no_spark.dropna(subset=['soc']).copy()\n",
    "ptss_filtered_no_spark_socdiff['soc_diff'] = ptss_filtered_no_spark_socdiff.groupby('vin', observed=True)['soc'].diff() \n",
    "ptss_filtered_no_spark = ptss_filtered_no_spark.merge(ptss_filtered_no_spark_socdiff[[\"soc\", \"date\", \"vin\", 'soc_diff']], \n",
    "                        on=[\"soc\", \"date\", \"vin\"], how=\"left\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "left = ptss_filtered[['in_charge_idx', 'date', 'soc_diff']].to_pandas()\n",
    "right = ptss_filtered_no_spark[['in_charge_idx', 'date', 'soc_diff']]\n",
    "\n",
    "test = left.merge(right, on=['date'], how='outer', suffixes=('_spark', '_no_spark'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test[(~((test.soc_diff_spark.isna()) & (test.soc_diff_no_spark.isna()))) & (test.soc_diff_spark != test.soc_diff_no_spark)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# print(left.groupby('in_charge_idx').ac_charging_power.median().sum())\n",
    "# print(right.groupby('in_charge_idx').ac_charging_power.median().sum())\n",
    "\n",
    "# print(left.groupby('in_charge_idx').dc_charging_power.median().sum())\n",
    "# print(right.groupby('in_charge_idx').dc_charging_power.median().sum())\n",
    "\n",
    "\n",
    "print(left.groupby('in_charge_idx').soc_diff.sum())\n",
    "print(right.groupby('in_charge_idx').soc_diff.sum())\n",
    "\n",
    "print(left[left.soc_diff > 0].groupby('in_charge_idx').soc_diff.sum().sum())\n",
    "print(right[right.soc_diff > 0].groupby('in_charge_idx').soc_diff.sum().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ptss_filtered[ptss_filtered.date == '2025-05-01 20:47:42']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = left.merge(right, on=['date'], how='left', suffixes=('_spark', '_no_spark'))\n",
    "test[(test.ac_charging_power_spark != test.ac_charging_power_no_spark) & ((test.ac_charging_power_spark.notna()) | (test.ac_charging_power_no_spark.notna()))]\n",
    "#test[(test.dc_charging_power_spark != test.dc_charging_power_no_spark) & ((test.dc_charging_power_spark.notna()) | (test.dc_charging_power_no_spark.notna()))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(ptss_filtered.in_charge_idx.min())\n",
    "print(ptss_filtered.in_charge_idx.max())\n",
    "print(ptss_filtered.shape)\n",
    "\n",
    "print(ptss_filtered_no_spark.in_charge_idx.min())\n",
    "print(ptss_filtered_no_spark.in_charge_idx.max())\n",
    "print(ptss_filtered_no_spark.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RTSS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col, lit\n",
    "\n",
    "tss_spark = bucket.read_parquet_df_spark(spark_session, \"raw_ts/tesla-fleet-telemetry/time_series/spark_raw_tss.parquet/vin=5YJSA7E52RF541858/part-00000-7596fcef-b984-4438-8839-d7a16a9b11d7.c000.snappy.parquet\")\n",
    "tss_spark = tss_spark.withColumn(\"vin\", lit('5YJSA7E52RF541858')).filter(col(\"readable_date\") >= '2025-04-24').filter(col(\"readable_date\") <= '2025-05-01')\n",
    "\"\"\"tss_spark = tss_spark.to_pandas_on_spark()\n",
    "\n",
    "date_mask = (\n",
    "    (tss_spark['readable_date'] >= '2025-05-01') & \n",
    "    (tss_spark['readable_date'] <= '2025-05-08')\n",
    ")\n",
    "\n",
    "# Appliquer le filtre\n",
    "tss_filtered = tss_spark[date_mask]\n",
    "tss_filtered\n",
    "\n",
    "tss_no_spark = bucket.read_parquet_df(\"raw_ts/tesla-fleet-telemetry/time_series/raw_tss.parquet\", filters=[(\"vin\", \"==\", \"5YJSA7E52RF541858\")])\n",
    "\n",
    "date_mask = (\n",
    "    (tss_no_spark['readable_date'] >= '2025-05-01') & \n",
    "    (tss_no_spark['readable_date'] <= '2025-05-08') &\n",
    "    (tss_no_spark.vin == '5YJSA7E52RF541858')\n",
    ")\n",
    "# Appliquer le filtre\n",
    "tss_filtered_no_spark = tss_no_spark[date_mask]\n",
    "tss_filtered_no_spark\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rtss = tss_spark.toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rtss.DetailedChargeState.value_counts()\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = rtss[rtss.readable_date == '2025-05-01 20:47:42']\n",
    "\n",
    "non_null_columns = df.columns[df.notna().any()].tolist()\n",
    "print(\"Colonnes avec au moins une valeur non nulle:\")\n",
    "print(non_null_columns)\n",
    "\n",
    "# Afficher le DataFrame avec seulement ces colonnes\n",
    "df[non_null_columns]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transform.processed_tss.config import *\n",
    "from pyspark.sql import DataFrame as DF, Window\n",
    "from pyspark.sql.types import DoubleType\n",
    "from pyspark.sql import SparkSession\n",
    "from scipy.integrate import cumulative_trapezoid\n",
    "import pandas as pd\n",
    "from core.console_utils import main_decorator\n",
    "from core.constants import KJ_TO_KWH\n",
    "from core.caching_utils import CachedETLSpark\n",
    "from core.logging_utils import set_level_of_loggers_with_prefix\n",
    "from core.spark_utils import (\n",
    "    safe_astype_spark_with_error_handling,\n",
    "    create_spark_session,\n",
    "    timedelta_to_interval,\n",
    ")\n",
    "from transform.fleet_info.main import fleet_info\n",
    "from transform.processed_tss.config import *\n",
    "from transform.raw_tss.RawTss import RawTss\n",
    "from core.s3_utils import S3_Bucket\n",
    "\n",
    "from pyspark.sql.functions import (\n",
    "    col,\n",
    "    lag,\n",
    "    lead,\n",
    "    unix_timestamp,\n",
    "    when,\n",
    "    lit,\n",
    "    last,\n",
    "    first,\n",
    "    expr,\n",
    "    coalesce,\n",
    "    sum as spark_sum,\n",
    "    pandas_udf,\n",
    ")\n",
    "\n",
    "def normalize_units_to_metric(tss):\n",
    "    tss = tss.withColumn(\"odometer\", col(\"odometer\") * 1.609)\n",
    "    return tss\n",
    "\n",
    "\n",
    "def compute_date_vars(tss: DF) -> DF:\n",
    "    # Créer une fenêtre par vin, ordonnée par date\n",
    "    window_spec = Window.partitionBy(\"vin\").orderBy(\"date\")\n",
    "\n",
    "    # Calculer le lag de date (valeur précédente)\n",
    "    tss = tss.withColumn(\"prev_date\", lag(col(\"date\")).over(window_spec))\n",
    "\n",
    "    # Différence en secondes entre les deux timestamps\n",
    "    tss = tss.withColumn(\n",
    "        \"sec_time_diff\",\n",
    "        (unix_timestamp(col(\"date\")) - unix_timestamp(col(\"prev_date\"))).cast(\n",
    "            \"double\"\n",
    "        ),\n",
    "    )\n",
    "\n",
    "    return tss\n",
    "\n",
    "def compute_charge_n_discharge_masks(\n",
    "    tss: DF, in_charge_vals: list, in_discharge_vals: list\n",
    ") -> DF:\n",
    "    \"\"\"Computes the `in_charge` and `in_discharge` masks either from the charging_status column or from the evolution of the soc over time.\"\"\"\n",
    "\n",
    "    if 'tesla-fleet-telemetry' in CHARGE_MASK_WITH_CHARGING_STATUS_MAKES:\n",
    "        return charge_n_discharging_masks_from_charging_status(\n",
    "            tss, in_charge_vals, in_discharge_vals\n",
    "        )\n",
    "    print(CHARGE_MASK_WITH_SOC_DIFFS_MAKES)\n",
    "    if 'tesla-fleet-telemetry' in CHARGE_MASK_WITH_SOC_DIFFS_MAKES:\n",
    "        print('Here1')\n",
    "        return charge_n_discharging_masks_from_soc_diff(tss)\n",
    "    raise ValueError(MAKE_NOT_SUPPORTED_ERROR.format(make='tesla-fleet-telemetry'))\n",
    "\n",
    "def charge_n_discharging_masks_from_soc_diff(tss):\n",
    "    w = (\n",
    "        Window.partitionBy(\"vin\")\n",
    "        .orderBy(\"date\")\n",
    "        .rowsBetween(Window.unboundedPreceding, 0)\n",
    "    )\n",
    "\n",
    "    # Forward fill soc\n",
    "    tss = tss.withColumn(\"soc_ffilled\", last(\"soc\", ignorenulls=True).over(w))\n",
    "    print(tss.columns)\n",
    "\n",
    "    # Window for diff calculation\n",
    "    w_diff = Window.partitionBy(\"vin\").orderBy(\"date\")\n",
    "\n",
    "    soc_prev = lag(\"soc_ffilled\").over(w_diff)\n",
    "    soc_diff = col(\"soc_ffilled\") - soc_prev\n",
    "\n",
    "    print('soc_ffilled' in tss.columns)\n",
    "\n",
    "    # Normalisation du signe → {-1, 0, 1}\n",
    "    soc_sign = when(soc_diff.isNull(), lit(0)).otherwise(soc_diff / abs(soc_diff))\n",
    "\n",
    "    tss = tss.withColumn(\"soc_diff\", soc_sign)\n",
    "\n",
    "    # Forward fill and backward fill equivalents\n",
    "    tss = tss.withColumn(\n",
    "        \"soc_diff_ffill\", last(\"soc_diff\", ignorenulls=True).over(w)\n",
    "    )\n",
    "    w_rev = (\n",
    "        Window.partitionBy(\"vin\")\n",
    "        .orderBy(col(\"date\").desc())\n",
    "        .rowsBetween(Window.unboundedPreceding, 0)\n",
    "    )\n",
    "    tss = tss.withColumn(\n",
    "        \"soc_diff_bfill\", last(\"soc_diff\", ignorenulls=True).over(w_rev)\n",
    "    )\n",
    "\n",
    "    # Définition des masques\n",
    "    tss = tss.withColumn(\n",
    "        \"in_charge\", (col(\"soc_diff_ffill\") > 0) & (col(\"soc_diff_bfill\") > 0)\n",
    "    )\n",
    "    tss = tss.withColumn(\n",
    "        \"in_discharge\", (col(\"soc_diff_ffill\") < 0) & (col(\"soc_diff_bfill\") < 0)\n",
    "    )\n",
    "\n",
    "    return tss\n",
    "\n",
    "def charge_n_discharging_masks_from_charging_status(\n",
    "    tss: DF, in_charge_vals: list, in_discharge_vals: list\n",
    ") -> DF:\n",
    "\n",
    "    assert \"charging_status\" in tss.columns, NO_CHARGING_STATUS_COL_ERROR\n",
    "    return tss.withColumn(\n",
    "        \"in_charge\", col(\"charging_status\").isin(in_charge_vals)\n",
    "    ).withColumn(\"in_discharge\", col(\"charging_status\").isin(in_discharge_vals))\n",
    "\n",
    "def trim_leading_n_trailing_soc_off_masks(tss: DF, masks: list[str]) -> DF:\n",
    "    for mask in masks:\n",
    "        # Créer une colonne temporaire contenant 'soc' uniquement lorsque le masque est vrai\n",
    "        tss = tss.withColumn(\"naned_soc\", when(col(mask), col(\"soc\")))\n",
    "        # Fenêtre pour grouper par 'vin' et l'index associé au masque\n",
    "        w = Window.partitionBy(\"vin\", col(f\"{mask}_idx\")).orderBy(\n",
    "            \"date\"\n",
    "        )  # assuming you have a 'timestamp' for ordering\n",
    "        # Calcul des premières et dernières valeurs non nulles de 'naned_soc' dans chaque groupe\n",
    "        trailing_soc = first(\"naned_soc\", ignorenulls=True).over(w)\n",
    "        leading_soc = last(\"naned_soc\", ignorenulls=True).over(w)\n",
    "        # Ajouter ces colonnes\n",
    "        tss = (\n",
    "            tss.withColumn(\"trailing_soc\", trailing_soc)\n",
    "            .withColumn(\"leading_soc\", leading_soc)\n",
    "            .withColumn(\n",
    "                f\"trimmed_{mask}\",\n",
    "                (col(mask))\n",
    "                & (col(\"soc\") != col(\"trailing_soc\"))\n",
    "                & (col(\"soc\") != col(\"leading_soc\")),\n",
    "            )\n",
    "            .drop(\"naned_soc\")\n",
    "        )\n",
    "\n",
    "    return tss\n",
    "\n",
    "def compute_idx_from_masks(tss, masks: list[str]):\n",
    "    \"\"\"\n",
    "    Spark version of compute_idx_from_masks.\n",
    "\n",
    "    Args:\n",
    "        tss (DataFrame): Spark DataFrame.\n",
    "        masks (list): List of boolean column names to compute idx on.\n",
    "\n",
    "    Returns:\n",
    "        DataFrame: Transformed Spark DataFrame.\n",
    "    \"\"\"\n",
    "\n",
    "    for mask in masks:\n",
    "        idx_col_name = f\"{mask}_idx\"\n",
    "\n",
    "        w = Window.partitionBy(\"vin\").orderBy(\"date\")\n",
    "\n",
    "        # Décalage de mask par groupe\n",
    "        shifted_mask = lag(col(mask), 1).over(w)\n",
    "\n",
    "        # new_period_start_mask = shifted_mask != mask\n",
    "        new_period_start_mask = shifted_mask.isNull() | (shifted_mask != col(mask))\n",
    "\n",
    "        # Si max_td est défini, on ajoute aussi condition sur time_diff\n",
    "        if MAX_TD is not None:\n",
    "            new_period_start_mask = new_period_start_mask | (\n",
    "                col(\"sec_time_diff\") > lit(timedelta_to_interval(MAX_TD))\n",
    "            )\n",
    "\n",
    "        # Génère l'index via cumul\n",
    "        tss = tss.withColumn(\n",
    "            \"new_period_start_mask\",\n",
    "            when(new_period_start_mask, lit(1)).otherwise(lit(0)),\n",
    "        )\n",
    "\n",
    "        tss = tss.withColumn(\n",
    "            idx_col_name, spark_sum(\"new_period_start_mask\").over(w)\n",
    "        ).drop(\"new_period_start_mask\")\n",
    "\n",
    "    return tss\n",
    "\n",
    "def compute_status_col(tss):\n",
    "\n",
    "\n",
    "    # Fenêtre ordonnée par date pour chaque VIN\n",
    "    w = Window.partitionBy(\"vin\").orderBy(\"date\")\n",
    "\n",
    "    # Décalage pour calculer diff(odometer)\n",
    "    prev_odo = lag(\"odometer\").over(w)\n",
    "    delta_odo = col(\"odometer\") - prev_odo\n",
    "\n",
    "    # Première base de status\n",
    "    status = (\n",
    "        when(col(\"in_charge\") == True, lit(\"charging\"))\n",
    "        .when(col(\"in_charge\") == False, lit(\"discharging\"))\n",
    "        .otherwise(lit(\"unknown\"))\n",
    "    )\n",
    "\n",
    "    # Raffinement → si in_charge == False → \"moving\" ou \"idle_discharging\"\n",
    "    status = (\n",
    "        when(col(\"in_charge\") == True, lit(\"charging\"))\n",
    "        .when(\n",
    "            col(\"in_charge\") == False,\n",
    "            when(delta_odo > 0, lit(\"moving\")).otherwise(lit(\"idle_discharging\")),\n",
    "        )\n",
    "        .otherwise(lit(\"unknown\"))\n",
    "    )\n",
    "\n",
    "    return tss.withColumn(\"status\", status)\n",
    "\n",
    "\n",
    "def compute_cum_var(tss, var_col: str, cum_var_col: str):\n",
    "    if var_col not in tss.columns:\n",
    "        return tss\n",
    "    # Schéma de retour attendu → adapte le type si nécessaire\n",
    "    schema = tss.schema.add(cum_var_col, DoubleType())\n",
    "\n",
    "    @pandas_udf(schema, functionType=\"grouped_map\")\n",
    "    def integrate_trapezoid(df: pd.DataFrame) -> pd.DataFrame:\n",
    "\n",
    "        df = df.sort_values(\"date\").copy()\n",
    "\n",
    "        x = df[\"date\"].astype(\"int64\") // 10**9  # Convertit ns → s\n",
    "        y = df[var_col].fillna(0).astype(\"float64\")\n",
    "\n",
    "        cum = cumulative_trapezoid(y=y.values, x=x.values, initial=0) * KJ_TO_KWH\n",
    "\n",
    "        # Ajuste pour que ça commence à zéro\n",
    "        cum = cum - cum[0]\n",
    "\n",
    "        df[cum_var_col] = cum\n",
    "        return df\n",
    "\n",
    "    return tss.groupBy(\"vin\").apply(integrate_trapezoid)\n",
    "\n",
    "def compute_charge_n_discharge_vars(tss: DF) -> DF:\n",
    "    tss = compute_charge_n_discharge_masks(\n",
    "        tss, IN_CHARGE_CHARGING_STATUS_VALS, IN_DISCHARGE_CHARGING_STATUS_VALS\n",
    "    )\n",
    "    tss = compute_charge_idx_bis(tss)\n",
    "    return tss\n",
    "\n",
    "def compute_charge_n_discharge_masks(\n",
    "    tss: DF, in_charge_vals: list, in_discharge_vals: list\n",
    ") -> DF:\n",
    "    \"\"\"Computes the `in_charge` and `in_discharge` masks either from the charging_status column or from the evolution of the soc over time.\"\"\"\n",
    "    if 'tesla-fleet-telemetry' in CHARGE_MASK_WITH_CHARGING_STATUS_MAKES:\n",
    "        return charge_n_discharging_masks_from_charging_status(\n",
    "            tss, in_charge_vals, in_discharge_vals\n",
    "        )\n",
    "\n",
    "def charge_n_discharging_masks_from_charging_status(\n",
    "    tss: DF, in_charge_vals: list, in_discharge_vals: list\n",
    ") -> DF:\n",
    "    assert \"charging_status\" in tss.columns, NO_CHARGING_STATUS_COL_ERROR\n",
    "\n",
    "    # Masques booléens Spark\n",
    "    tss = tss.withColumn(\n",
    "        \"in_charge\",\n",
    "        when(col(\"charging_status\").isin(in_charge_vals), lit(True)).otherwise(\n",
    "            lit(False)\n",
    "        ),\n",
    "    )\n",
    "\n",
    "    tss = tss.withColumn(\n",
    "        \"in_discharge\",\n",
    "        when(col(\"charging_status\").isin(in_discharge_vals), lit(True)).otherwise(\n",
    "            lit(False)\n",
    "        ),\n",
    "    )\n",
    "\n",
    "    return tss\n",
    "\n",
    "def compute_energy_added(tss: DF) -> DF:\n",
    "    tss = tss.withColumn(\n",
    "        \"charge_energy_added\",\n",
    "        when(\n",
    "            col(\"dc_charge_energy_added\").isNotNull()\n",
    "            & (col(\"dc_charge_energy_added\") > 0),\n",
    "            col(\"dc_charge_energy_added\"),\n",
    "        ).otherwise(col(\"ac_charge_energy_added\")),\n",
    "    )\n",
    "    return tss\n",
    "\n",
    "def compute_charge_idx_bis(tss: DF) -> DF:\n",
    "\n",
    "    tss = compute_energy_added(tss)\n",
    "\n",
    "    # 1. Filtrer les lignes où soc n'est pas null\n",
    "    tss_na = tss.filter(col(\"soc\").isNotNull())\n",
    "\n",
    "    # 2. Créer une fenêtre ordonnée par date par VIN\n",
    "    vin_window = Window.partitionBy(\"vin\").orderBy(\"date\")\n",
    "\n",
    "    # 3. Calcul des différences\n",
    "    tss_na = (\n",
    "        tss_na.withColumn(\"soc_diff\", col(\"soc\") - lag(\"soc\", 1).over(vin_window))\n",
    "        .withColumn(\n",
    "            \"trend\",\n",
    "            when(col(\"soc_diff\") > 0, lit(1))\n",
    "            .when(col(\"soc_diff\") < 0, lit(-1))\n",
    "            .otherwise(lit(0)),\n",
    "        )\n",
    "        .withColumn(\"prev_trend\", lag(\"trend\", 1).over(vin_window))\n",
    "        .withColumn(\"prev_prev_trend\", lag(\"trend\", 2).over(vin_window))\n",
    "        .withColumn(\"prev_prev_prev_trend\", lag(\"trend\", 3).over(vin_window))\n",
    "        .withColumn(\"prev_date\", lag(\"date\", 1).over(vin_window))\n",
    "        .withColumn(\n",
    "            \"time_diff_min\",\n",
    "            (unix_timestamp(col(\"date\")) - unix_timestamp(col(\"prev_date\"))) / 60,\n",
    "        )\n",
    "        .withColumn(\"time_gap\", col(\"time_diff_min\") > 60)\n",
    "        .withColumn(\n",
    "            \"trend_change\",\n",
    "            when(\n",
    "                (\n",
    "                    (col(\"trend\") == col(\"prev_trend\"))\n",
    "                    & (col(\"prev_trend\") != col(\"prev_prev_trend\"))\n",
    "                    & (col(\"prev_prev_trend\") == col(\"prev_prev_prev_trend\"))\n",
    "                )\n",
    "                | col(\"time_gap\"),\n",
    "                lit(1),\n",
    "            ).otherwise(lit(0)),\n",
    "        )\n",
    "    )\n",
    "\n",
    "    # 4. Initialiser les premières lignes à 0\n",
    "    tss_na = tss_na.withColumn(\n",
    "        \"trend_change\",\n",
    "        when(col(\"date\") == lag(\"date\", 1).over(vin_window), lit(0)).otherwise(\n",
    "            col(\"trend_change\")\n",
    "        ),\n",
    "    )\n",
    "\n",
    "    # 5. Cumulative sum (session index)\n",
    "    tss_na = tss_na.withColumn(\n",
    "        \"in_charge_idx\",\n",
    "        spark_sum(\"trend_change\").over(\n",
    "            vin_window.rowsBetween(Window.unboundedPreceding, 0)\n",
    "        ),\n",
    "    )\n",
    "\n",
    "    # 6. Join avec le DataFrame original\n",
    "    tss = tss.join(\n",
    "        tss_na.select(\"vin\", \"date\", \"soc\", \"soc_diff\", \"in_charge_idx\"),\n",
    "        on=[\"vin\", \"date\", \"soc\"],\n",
    "        how=\"left\",\n",
    "    )\n",
    "\n",
    "    # 7. Forward-fill `odometer` et `in_charge_idx` (non-natif en Spark, mais on peut approximer)\n",
    "    fill_window = (\n",
    "        Window.partitionBy(\"vin\")\n",
    "        .orderBy(\"date\")\n",
    "        .rowsBetween(Window.unboundedPreceding, 0)\n",
    "    )\n",
    "    tss = tss.withColumn(\n",
    "        \"odometer\",\n",
    "        coalesce(col(\"odometer\"), expr(\"last(odometer, true)\").over(fill_window)),\n",
    "    ).withColumn(\n",
    "        \"in_charge_idx\",\n",
    "        coalesce(\n",
    "            col(\"in_charge_idx\"),\n",
    "            expr(\"last(in_charge_idx, true)\").over(fill_window),\n",
    "        ),\n",
    "    )\n",
    "\n",
    "    return tss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transform.processed_tss.config import RENAME_COLS_DICT\n",
    "from core.spark_utils import safe_astype_spark_with_error_handling\n",
    "from pyspark.sql.functions import min, max, col\n",
    "\n",
    "\"\"\"\n",
    "tss = compute_charge_n_discharge_vars(tss)\n",
    "tss = tss.join(spark.createDataFrame(fleet_info), \"vin\", \"left\")\n",
    "tss = tss.sort(\"vin\", ascending=True)\n",
    "\"\"\"\n",
    "\n",
    "tss = tss_spark.withColumnsRenamed(RENAME_COLS_DICT)\n",
    "tss = safe_astype_spark_with_error_handling(tss)\n",
    "tss = normalize_units_to_metric(tss)\n",
    "tss = tss.orderBy([\"vin\", \"date\"])\n",
    "tss = compute_date_vars(tss)\n",
    "tss = compute_charge_n_discharge_vars(tss)\n",
    "tss = tss.join(spark_session.createDataFrame(fleet_info), \"vin\", \"left\")\n",
    "tss = tss.sort(\"vin\", ascending=True)\n",
    "print(tss.columns)\n",
    "# Obtenir les valeurs min et max\n",
    "min_value = tss.agg(min(col(\"in_charge_idx\"))).collect()\n",
    "max_value = tss.agg(max(col(\"in_charge_idx\"))).collect()\n",
    "\n",
    "tss_pandas = tss.toPandas()\n",
    "\n",
    "print(f\"Min: {min_value}\")\n",
    "print(f\"Max: {max_value}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "left = ptss_filtered_no_spark[['date', 'in_charge_idx', 'soc', 'sec_time_diff']]\n",
    "right = tss_pandas[['date', 'in_charge_idx', 'soc', 'soc_diff', 'sec_time_diff']]\n",
    "\n",
    "left['in_charge_idx_minus_ind_charge_idx_lag'] = left['in_charge_idx'] - left['in_charge_idx'].shift(1)\n",
    "right['in_charge_idx_minus_ind_charge_idx_lag'] = right['in_charge_idx'] - right['in_charge_idx'].shift(1)\n",
    "\n",
    "merged = pd.merge(left, right, on=['date'], how='left', suffixes=('_left', '_right'))\n",
    "\n",
    "merged[merged.in_charge_idx_minus_ind_charge_idx_lag_left != merged.in_charge_idx_minus_ind_charge_idx_lag_right]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tss_pandas[['date', 'in_charge_idx', 'soc', 'soc_diff', 'sec_time_diff']][600:630]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ptss_filtered_no_spark[['date', 'in_charge_idx', 'soc', 'sec_time_diff']][600:630]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "data-env",
   "language": "python",
   "name": "data-env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

