{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark import SparkConf\n",
    "from core.s3.settings import S3Settings\n",
    "import os\n",
    "\n",
    "def create_spark_session(access_key: str, secret_key: str) -> SparkSession:\n",
    "    \"\"\"\n",
    "    Create a session spark with a connexion to scaleway\n",
    "    \"\"\"\n",
    "    os.environ[\"PYSPARK_SUBMIT_ARGS\"] = (\n",
    "        \"--packages org.apache.hadoop:hadoop-aws:3.3.4,org.apache.spark:spark-hadoop-cloud_2.12:3.4.0 pyspark-shell\"\n",
    "    )\n",
    "\n",
    "    g1gc_options = (\n",
    "        \"-XX:+UseG1GC \"\n",
    "        \"-XX:MaxGCPauseMillis=100 \"\n",
    "        \"-XX:G1HeapRegionSize=32m \"\n",
    "        \"-XX:+UseStringDeduplication \"\n",
    "        \"-XX:+UnlockExperimentalVMOptions \"\n",
    "        \"-XX:+UseZGC \"\n",
    "        \"-XX:+DisableExplicitGC \"\n",
    "        \"-XX:+UseGCOverheadLimit \"\n",
    "        \"-XX:GCTimeRatio=9 \"\n",
    "        \"-XX:+PrintGCDetails \"\n",
    "        \"-XX:+PrintGCTimeStamps \"\n",
    "        \"-Xloggc:/tmp/spark-gc.log\"\n",
    "    )\n",
    "    \n",
    "    spark = (\n",
    "        SparkSession.builder.appName(\"Scaleway S3 Read JSON\")\n",
    "        .config(\"spark.jars.packages\", \"org.apache.hadoop:hadoop-aws:3.3.4,org.apache.spark:spark-hadoop-cloud_2.12:3.4.0\")\n",
    "        .config(\"spark.hadoop.fs.s3a.endpoint\", \"https://s3.fr-par.scw.cloud\")\n",
    "        .config(\"spark.hadoop.fs.s3a.access.key\", access_key)\n",
    "        .config(\"spark.hadoop.fs.s3a.secret.key\", secret_key)\n",
    "        .config(\"spark.hadoop.fs.s3a.impl\", \"org.apache.hadoop.fs.s3a.S3AFileSystem\")\n",
    "        .config(\"spark.hadoop.fs.s3a.path.style.access\", \"true\")\n",
    "        .config(\"spark.driver.host\", \"localhost\")\n",
    "        .config(\"spark.hadoop.fs.s3a.aws.credentials.provider\", \"org.apache.hadoop.fs.s3a.SimpleAWSCredentialsProvider\")\n",
    "        # Nouvelles configurations pour résoudre le ClassNotFoundException\n",
    "        .config(\"spark.hadoop.fs.s3a.experimental.input.fadvise\", \"normal\")\n",
    "        .config(\"spark.hadoop.fs.s3a.connection.maximum\", \"1000\")\n",
    "        .config(\"spark.hadoop.fs.s3a.threads.max\", \"20\")\n",
    "        .config(\"spark.hadoop.fs.s3a.threads.core\", \"10\")\n",
    "        .config(\"spark.hadoop.fs.s3a.buffer.dir\", \"/tmp\")\n",
    "        .config(\"spark.hadoop.fs.s3a.block.size\", \"134217728\")  # 128MB\n",
    "        .config(\"spark.hadoop.fs.s3a.multipart.size\", \"134217728\")  # 128MB\n",
    "        .config(\"spark.hadoop.fs.s3a.multipart.threshold\", \"134217728\")  # 128MB\n",
    "        # Configuration pour éviter les problèmes de commit protocol\n",
    "        .config(\"spark.sql.adaptive.enabled\", \"true\")\n",
    "        .config(\"spark.sql.adaptive.advisoryPartitionSizeInBytes\", \"64MB\")\n",
    "        .config(\"spark.sql.shuffle.partitions\", \"200\")\n",
    "        .config(\"spark.default.parallelism\", \"200\")\n",
    "        .config(\"spark.executor.memory\", \"10g\")\n",
    "        .config(\"spark.driver.memory\", \"10g\")\n",
    "        .config(\"spark.driver.maxResultSize\", \"4g\")\n",
    "        .config(\"spark.executor.extraJavaOptions\", g1gc_options)\n",
    "        .config(\"spark.driver.extraJavaOptions\", g1gc_options)\n",
    "        .getOrCreate()\n",
    "    )\n",
    "\n",
    "    return spark\n",
    "\n",
    "settings = S3Settings()\n",
    "\n",
    "spark = create_spark_session(settings.S3_KEY, settings.S3_SECRET)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import time\n",
    "import random\n",
    "from abc import abstractmethod\n",
    "from datetime import datetime\n",
    "from itertools import islice\n",
    "from logging import Logger\n",
    "from typing import Optional\n",
    "\n",
    "from pyspark.sql import DataFrame, SparkSession\n",
    "\n",
    "from core.caching_utils import CachedETLSpark\n",
    "from core.s3.s3_utils import S3Service\n",
    "from core.s3.settings import S3Settings\n",
    "from core.spark_utils import get_optimal_nb_partitions\n",
    "from transform.raw_tss.config import (\n",
    "    ALL_MAKES,\n",
    "    S3_RAW_TSS_KEY_FORMAT,\n",
    "    SCHEMAS,\n",
    ")\n",
    "\n",
    "\n",
    "class ResponseToRawTss(CachedETLSpark):\n",
    "    \"\"\"\n",
    "    Classe pour traiter les données renvoyées par les API stockées dans /response sur Scaleway\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        make: str,\n",
    "        force_update: bool = False,\n",
    "        writing_mode: Optional[str] = \"append\",\n",
    "        spark: SparkSession = None,\n",
    "        logger: Logger = None,\n",
    "        **kwargs,\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Initialise le processeur de télémétrie\n",
    "\n",
    "        Args:\n",
    "            make: Marque du véhicule\n",
    "            bucket: Instance S3Service pour l'accès aux données\n",
    "            spark: Session Spark pour le traitement des données\n",
    "            force_update: Si True, force la mise à jour des données dans CachedETLSpark\n",
    "        \"\"\"\n",
    "\n",
    "        self.logger = logger\n",
    "        self.spark = spark\n",
    "        self.make = make\n",
    "        self.bucket = S3Service()\n",
    "        self.settings = S3Settings()\n",
    "        self.base_s3_path = f\"s3a://{self.settings.S3_BUCKET}\"\n",
    "        self.raw_tss_path = S3_RAW_TSS_KEY_FORMAT.format(brand=self.make)\n",
    "        super().__init__(\n",
    "            S3_RAW_TSS_KEY_FORMAT.format(brand=self.make),\n",
    "            \"s3\",\n",
    "            force_update=force_update,\n",
    "            **kwargs,\n",
    "        )\n",
    "\n",
    "    def run(self):\n",
    "        start = time.time()\n",
    "        keys_to_download_per_vin, paths_to_exclude = (\n",
    "            self._get_keys_to_download()\n",
    "        ) # Clés à télécharger par vin\n",
    "        end = time.time()\n",
    "\n",
    "\n",
    "        # A SUPPRIMER #\n",
    "        keys_to_download_per_vin = dict(islice(keys_to_download_per_vin.items(), 20))\n",
    "\n",
    "        self.logger.info(\n",
    "            f\"Temps écoulé pour récupérer les clés à télécharger: {end - start:.2f} secondes\"\n",
    "        )\n",
    "\n",
    "        start = time.time()\n",
    "        optimal_partitions_nb, batch_size = self._set_optimal_spark_parameters(\n",
    "            keys_to_download_per_vin, paths_to_exclude\n",
    "        )\n",
    "        print(optimal_partitions_nb, batch_size)\n",
    "        end = time.time()\n",
    "        self.logger.info(\n",
    "            f\"Temps écoulé pour déterminer les paramètres Spark: {end - start:.2f} secondes\"\n",
    "        )\n",
    "\n",
    "        print(\"Batch size\", batch_size)\n",
    "        print(\"Nb de vins\", len(list(keys_to_download_per_vin.keys())))\n",
    "        print(\n",
    "            \"Nb de batches\",\n",
    "            len(list(self._batch_dict_items(keys_to_download_per_vin, batch_size))),\n",
    "        )\n",
    "\n",
    "        for batch_num, batch in enumerate(\n",
    "            self._batch_dict_items(keys_to_download_per_vin, batch_size), 1\n",
    "        ):  # Boucle pour faire des batchs d'écriture et ne pas saturer la mémoire\n",
    "            self.logger.info(f\"Batch {batch_num}:\")\n",
    "\n",
    "            start = time.time()\n",
    "            # Extract\n",
    "            raw_tss_unparsed = self._download_keys(batch)\n",
    "            end = time.time()\n",
    "            self.logger.info(\n",
    "                f\"Temps écoulé pour télécharger les json en spark {batch_num}: {end - start:.2f} secondes\"\n",
    "            )\n",
    "\n",
    "            start = time.time()\n",
    "            # Transform\n",
    "            raw_tss_parsed = self.parse_data(raw_tss_unparsed, optimal_partitions_nb)\n",
    "            end = time.time()\n",
    "            self.logger.info(\n",
    "                f\"Temps écoulé pour transformer les données du batch {batch_num}: {end - start:.2f} secondes\"\n",
    "            )\n",
    "\n",
    "            start = time.time()\n",
    "            # Load\n",
    "            self.bucket.append_spark_df_to_parquet(raw_tss_parsed, self.raw_tss_path)\n",
    "            end = time.time()\n",
    "            self.logger.info(\n",
    "                f\"Temps écoulé pour écrire les données dans le bucket {batch_num}: {end - start:.2f} secondes\"\n",
    "            )\n",
    "\n",
    "            raw_tss_parsed.unpersist()\n",
    "            del raw_tss_parsed\n",
    "\n",
    "        self.logger.info(f\"Traitement terminé pour {self.make}\")\n",
    "\n",
    "    def _set_optimal_spark_parameters(\n",
    "        self, keys_to_download_per_vin: dict, paths_to_exclude: list[str], nb_cores: int = 8\n",
    "    ) -> tuple[int, int]:\n",
    "        \"\"\"\n",
    "        Calcule la taille optimale des batches pour le traitement parallèle des VINs.\n",
    "\n",
    "        Cette méthode détermine le nombre optimal de VINs à traiter par batch en fonction\n",
    "        de la taille des données, du nombre de VINs et des ressources système disponibles.\n",
    "        L'optimisation vise à équilibrer la charge de travail entre les cœurs CPU tout\n",
    "        en maximisant l'utilisation des ressources Spark.\n",
    "\n",
    "        Args:\n",
    "            nb_cores (int, optional): Nombre de cœurs CPU disponibles pour le traitement.\n",
    "                                    Défaut: 4\n",
    "\n",
    "        Returns:\n",
    "            int: Nombre optimal de VINs à traiter par batch\n",
    "        \"\"\"\n",
    "        if nb_cores <= 0:\n",
    "            raise ValueError(\"Nombre de cœurs doit être un entier positif\")\n",
    "\n",
    "        file_size, _ = self.bucket.get_object_size(f\"response/{self.make}/\", prefix_to_exclude=paths_to_exclude)\n",
    "\n",
    "        nb_vins = len(list(keys_to_download_per_vin.keys()))\n",
    "\n",
    "        if nb_vins == 0:\n",
    "            self.logger.warning(\"Aucun VIN à traiter, retour de batch_size = 1\")\n",
    "            return 1\n",
    "\n",
    "        optimal_partitions = get_optimal_nb_partitions(file_size, nb_vins)\n",
    "        self.logger.info(f\"Nombre optimal de partitions: {optimal_partitions}\")\n",
    "\n",
    "        vin_per_batch = max(1, int((nb_vins / optimal_partitions) * nb_cores * 4))\n",
    "        self.logger.info(f\"Vin par batch: {vin_per_batch}\")\n",
    "\n",
    "        return (4 * nb_cores, vin_per_batch)\n",
    "\n",
    "    def _group_paths_by_vin(self, paths: list[str]) -> dict[str, list[str]]:\n",
    "        grouped = {}\n",
    "\n",
    "        for path in paths:\n",
    "            if \"/temp/\" not in path:\n",
    "                parts = path.strip(\"/\").split(\"/\")\n",
    "                if len(parts) < 2:\n",
    "                    continue  # ignorer les paths invalides\n",
    "                vin = parts[-2]\n",
    "                # Initialise la liste si vin pas encore vu\n",
    "                if vin not in grouped:\n",
    "                    grouped[vin] = []\n",
    "\n",
    "                grouped[vin].append(path)\n",
    "\n",
    "        return grouped\n",
    "\n",
    "    def _batch_dict_items(self, dictionary: dict, batch_size: int):\n",
    "        \"\"\"Générateur pour traiter un dictionnaire par lots\"\"\"\n",
    "        total_items = len(dictionary)\n",
    "\n",
    "        for i in range(0, total_items, batch_size):\n",
    "            batch = dict(islice(dictionary.items(), i, i + batch_size))\n",
    "            yield batch\n",
    "\n",
    "    def _get_keys_to_download(self) -> (dict[str, list[str]], list):\n",
    "        \"\"\"\n",
    "        Récupère les clés S3 des fichiers à télécharger en filtrant par date de dernière analyse.\n",
    "\n",
    "        Cette méthode compare les dates des fichiers de réponse disponibles avec la date\n",
    "        de dernière analyse stockée dans les données raw TSS pour déterminer quels fichiers\n",
    "        doivent être téléchargés et traités.\n",
    "\n",
    "        Returns:\n",
    "            dict[str, list[str]]: Dictionnaire où les clés sont les VINs et les valeurs sont\n",
    "                                les listes des chemins S3 des fichiers à télécharger.\n",
    "                                Format: {'VIN123': ['response/brand/VIN123/2024-01-01.json', ...]}\n",
    "\n",
    "        Raises:\n",
    "            Exception: Si une erreur survient lors de la lecture des données Parquet ou\n",
    "                    de la liste des fichiers S3\n",
    "        \"\"\"\n",
    "\n",
    "        last_parsed_date_dict = None\n",
    "\n",
    "        if self.bucket.check_spark_file_exists(self.raw_tss_path):\n",
    "            raw_tss = self.bucket.read_parquet_df_spark(self.spark, self.raw_tss_path)\n",
    "            if \"date\" in raw_tss.columns and raw_tss:\n",
    "                # Lecture optimisée\n",
    "                last_dates_df = (\n",
    "                    raw_tss.select(\"vin\", \"date\")\n",
    "                    .groupBy(\"vin\")\n",
    "                    .agg({\"date\": \"max\"})\n",
    "                    .withColumnRenamed(\"max(date)\", \"last_parsed_date\")\n",
    "                )\n",
    "\n",
    "                last_parsed_date_dict = (\n",
    "                    last_dates_df.toPandas()\n",
    "                    .set_index(\"vin\")[\"last_parsed_date\"]\n",
    "                    .to_dict()\n",
    "                )\n",
    "\n",
    "            else:\n",
    "                self.logger.info(f\"Colonne 'date' non trouvée dans le dataset présent.\")\n",
    "\n",
    "        vins_paths = self.bucket.list_files(f\"response/{self.make}/\", type_file=\".json\")\n",
    "        vins_paths_grouped = self._group_paths_by_vin(vins_paths)\n",
    "\n",
    "        paths_to_exclude = []\n",
    "\n",
    "        print(last_parsed_date_dict)\n",
    "\n",
    "        if last_parsed_date_dict:\n",
    "            for vin, paths in vins_paths_grouped.items():\n",
    "                if vin in last_parsed_date_dict.keys():\n",
    "                    vins_paths_grouped[vin] = [\n",
    "                        path\n",
    "                        for path in paths\n",
    "                        if datetime.strptime(path.split(\"/\")[-1], \"%Y-%m-%d.json\")\n",
    "                        > datetime.strptime(str(last_parsed_date_dict[vin]).split()[0], \"%Y-%m-%d\")\n",
    "                    ]\n",
    "\n",
    "                    paths_to_exclude.extend([\n",
    "                        path\n",
    "                        for path in paths\n",
    "                        if datetime.strptime(path.split(\"/\")[-1], \"%Y-%m-%d.json\")\n",
    "                        <= datetime.strptime(str(last_parsed_date_dict[vin]).split()[0], \"%Y-%m-%d\")\n",
    "                    ])\n",
    "\n",
    "\n",
    "\n",
    "        vins_paths_grouped = {k: v for k, v in vins_paths_grouped.items() if v}\n",
    "\n",
    "        # Shuffle the vins to avoid skewness\n",
    "        vins_paths_grouped = dict(random.sample(list(vins_paths_grouped.items()), k=len(vins_paths_grouped)))\n",
    "\n",
    "        return (vins_paths_grouped, paths_to_exclude)\n",
    "\n",
    "    def _download_keys(self, batch: dict[str, list[str]]) -> DataFrame:\n",
    "        \"\"\"\n",
    "        Télécharge les json et retourne un DataFrame Spark\n",
    "        \"\"\"\n",
    "\n",
    "        keys_to_download = []\n",
    "\n",
    "        for _, paths in batch.items():\n",
    "            keys_to_download.extend(paths)\n",
    "\n",
    "        schema = SCHEMAS[self.make]\n",
    "\n",
    "        keys_to_download_str = [\n",
    "            f\"s3a://{self.settings.S3_BUCKET}/{key}\" for key in keys_to_download\n",
    "        ]\n",
    "\n",
    "        return (\n",
    "            self.spark.read.option(\"multiline\", \"true\")\n",
    "            .schema(schema)\n",
    "            .json(keys_to_download_str)\n",
    "        )\n",
    "\n",
    "    @abstractmethod\n",
    "    def parse_data(self, df: DataFrame, optimal_partitions_nb: int) -> DataFrame:\n",
    "        pass\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col, explode, expr, udf\n",
    "from pyspark.sql.types import *\n",
    "from pyspark.sql.types import (ArrayType, BooleanType, DateType, DoubleType,\n",
    "                               IntegerType, LongType, StringType, StructField,\n",
    "                               StructType, TimestampType)\n",
    "from typing import Optional\n",
    "from pyspark.sql import SparkSession\n",
    "from logging import Logger\n",
    "import logging\n",
    "from dotenv import load_dotenv\n",
    "from core.s3.settings import S3Settings\n",
    "from core.spark_utils import create_spark_session\n",
    "from core.console_utils import main_decorator\n",
    "import sys\n",
    "from pyspark.sql import DataFrame\n",
    "from pyspark.sql.functions import input_file_name, regexp_extract, to_timestamp, first\n",
    "from pyspark.sql.functions import col, explode, array, struct, lit, size\n",
    "from functools import reduce\n",
    "\n",
    "\n",
    "class MobilisightResponseToRaw(ResponseToRawTss):\n",
    "    \"\"\"\n",
    "    Classe pour traiter les données émises par les API Mobilisight\n",
    "    stockées dans '/response/bmw/' sur Scaleway\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        make: str = \"stellantis\",\n",
    "        force_update: bool = False,\n",
    "        writing_mode: Optional[str] = \"append\",\n",
    "        spark: SparkSession = None,\n",
    "        logger: Logger = None,\n",
    "        **kwargs,\n",
    "    ):\n",
    "\n",
    "        super().__init__(\n",
    "            make=make,\n",
    "            force_update=force_update,\n",
    "            writing_mode=writing_mode,\n",
    "            spark=spark,\n",
    "            logger=logger,\n",
    "            **kwargs,\n",
    "        )\n",
    "\n",
    "    def parse_data(self, df: DataFrame, optimal_partitions_nb: int) -> DataFrame:\n",
    "        \"\"\"\n",
    "        Parse dict from BMW api response\n",
    "\n",
    "        Args:\n",
    "            response (dict): Contains data to parse\n",
    "            spark (SparkSession): spark session active\n",
    "            vin (str): Vehicle identification number\n",
    "\n",
    "        Returns:\n",
    "            spark.DataFrame: Data with every columns\n",
    "        \"\"\"\n",
    "\n",
    "        df = df.repartition('vin').coalesce(optimal_partitions_nb)\n",
    "\n",
    "        return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "import sys\n",
    "from transform.raw_tss.config import SCHEMAS\n",
    "\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO,\n",
    "    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',\n",
    "    stream=sys.stdout\n",
    ")\n",
    "\n",
    "# Correction : utiliser getLogger au lieu de Logger\n",
    "logger = logging.getLogger('Logger RawTss')\n",
    "\n",
    "df = MobilisightResponseToRaw(make='stellantis', force_update=True, spark=spark, logger=logger).run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s3 = S3Service()\n",
    "\n",
    "df = s3.read_parquet_df_spark(spark=spark, key='raw_ts/stellantis/time_series/raw_ts_spark.parquet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.types import StructType, ArrayType\n",
    "from typing import Dict\n",
    "from pyspark.sql import functions as F\n",
    "\n",
    "def build_fields_from_schema(\n",
    "    schema: StructType,\n",
    "    prefix: str = \"\",\n",
    "    naming_sep: str = \"_\"\n",
    ") -> Dict[str, Dict]:\n",
    "    \"\"\"\n",
    "    Parcours récursif d'un schema StructType pour générer un dict 'fields' adapté\n",
    "    pour extraction à plat, en cherchant les ArrayType contenant un champ 'datetime'.\n",
    "    \n",
    "    Args:\n",
    "      schema: schema Spark (StructType)\n",
    "      prefix: chemin actuel dans la hiérarchie, ex: \"electricity.level\"\n",
    "      naming_sep: séparateur pour noms colonnes\n",
    "    \"\"\"\n",
    "    result = {}\n",
    "\n",
    "    for field in schema.fields:\n",
    "        field_name = field.name\n",
    "        full_path = f\"{prefix}.{field_name}\" if prefix else field_name\n",
    "\n",
    "        # Si c'est un ArrayType dont l'élément est un StructType, on inspecte le struct\n",
    "        if isinstance(field.dataType, ArrayType) and isinstance(field.dataType.elementType, StructType):\n",
    "            struct = field.dataType.elementType\n",
    "            # On cherche si le struct a un champ 'datetime'\n",
    "            has_datetime = any(f.name == \"datetime\" for f in struct.fields)\n",
    "\n",
    "            if has_datetime:\n",
    "                # On récupère tous les champs sauf datetime (qui sert pour join/filtrage)\n",
    "                field_map = {}\n",
    "                for f in struct.fields:\n",
    "                    if f.name != \"datetime\":\n",
    "                        # Nom de la colonne : concat des noms du chemin + nom du champ (ex: electricity_level_percentage)\n",
    "                        col_name = f\"{full_path.replace('.', naming_sep)}{naming_sep}{f.name}\"\n",
    "                        field_map[f.name] = col_name\n",
    "                result[full_path] = {\n",
    "                    \"path\": full_path,\n",
    "                    \"fields\": field_map\n",
    "                }\n",
    "            else:\n",
    "                # Pas de datetime dans ce struct => on descend récursivement\n",
    "                deeper = build_fields_from_schema(struct, full_path, naming_sep)\n",
    "                result.update(deeper)\n",
    "\n",
    "        elif isinstance(field.dataType, StructType):\n",
    "            # Struct simple, on descend récursivement\n",
    "            deeper = build_fields_from_schema(field.dataType, full_path, naming_sep)\n",
    "            result.update(deeper)\n",
    "\n",
    "        else:\n",
    "            # Champ simple non struct ni array, on ignore ici (car tu veux extraire depuis arrays)\n",
    "            pass\n",
    "\n",
    "    return result\n",
    "\n",
    "fields = build_fields_from_schema(SCHEMAS[\"stellantis\"])\n",
    "\n",
    "print(fields)\n",
    "\n",
    "def explode_and_select(df, path, field_mapping):\n",
    "    \"\"\"\n",
    "    Explose un tableau structuré à l'emplacement `path` dans df,\n",
    "    sélectionne les colonnes définies dans `field_mapping` (dictionnaire field_in_struct -> nom_colonne_sortie),\n",
    "    et retourne un DF (vin, datetime, colonnes mappées).\n",
    "    \"\"\"\n",
    "    # Exploser l'array\n",
    "    exploded = df.select(\n",
    "        \"vin\",\n",
    "        F.explode_outer(path).alias(\"exploded_struct\")\n",
    "    )\n",
    "    \n",
    "    # Sélection des colonnes demandées\n",
    "    cols = [\n",
    "        F.col(\"vin\"),\n",
    "        F.col(\"exploded_struct.datetime\").alias(\"datetime\"),\n",
    "    ]\n",
    "    for field_in_struct, alias in field_mapping.items():\n",
    "        cols.append(F.col(f\"exploded_struct.{field_in_struct}\").alias(alias))\n",
    "        \n",
    "    return exploded.select(*cols)\n",
    "\n",
    "def merge_all_exploded(dfs):\n",
    "    \"\"\"\n",
    "    Fusionne les DF éclatés par (vin, datetime) via full outer join\n",
    "    \"\"\"\n",
    "    from functools import reduce\n",
    "    join_expr = [\"vin\", \"datetime\"]\n",
    "    df_merged = reduce(\n",
    "        lambda df1, df2: df1.join(df2, on=join_expr, how=\"full_outer\"),\n",
    "        dfs\n",
    "    )\n",
    "    return df_merged\n",
    "\n",
    "\n",
    "# On prépare la liste des DF éclatés\n",
    "dfs = []\n",
    "for key, params in fields.items():\n",
    "    print(key, params)\n",
    "    path = params[\"path\"]\n",
    "    spark_path = F.col(path)\n",
    "    df_exp = explode_and_select(df, path, params[\"fields\"])\n",
    "    dfs.append(df_exp)\n",
    "\n",
    "# Fusionner tous les dfs sur (vin, datetime)\n",
    "df_final = merge_all_exploded(dfs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_final.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col, explode_outer, expr, coalesce, explode\n",
    "\n",
    "# 1. Explode tous les champs nécessaires\n",
    "df_flat = df.repartition(32)\n",
    "\n",
    "df_flat.show()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def extract_timeseries(df_raw, vin_col, path, field_mapping):\n",
    "    exploded_df = df_raw.select(\n",
    "        col(vin_col),\n",
    "        explode(col(path)).alias(\"entry\")\n",
    "    ).select(\n",
    "        col(vin_col),\n",
    "        col(\"entry.datetime\").alias(\"datetime\"),\n",
    "        *[col(f\"entry.{k}\").alias(v) for k, v in field_mapping.items()]\n",
    "    )\n",
    "    return exploded_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test[test.data.notna()].loc[0, :].data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.select('signal').distinct().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col, explode, array, struct, lit\n",
    "\n",
    "target_schema = ArrayType(StructType([\n",
    "    StructField(\"timestamp\", LongType()),\n",
    "    StructField(\"data\", StructType([\n",
    "        StructField(\"value\", DoubleType()),\n",
    "        StructField(\"unit\", StringType())\n",
    "    ]))\n",
    "]))\n",
    "\n",
    "def normalize_entries(entries):\n",
    "    if entries is None:\n",
    "        return None\n",
    "    normalized = []\n",
    "    for e in entries:\n",
    "        timestamp = e.get(\"timestamp\", None)\n",
    "        data = e.get(\"data\", None)\n",
    "        if isinstance(data, dict):\n",
    "            value = data.get(\"value\", None)\n",
    "            unit = data.get(\"unit\", None)\n",
    "        else:\n",
    "            value = data\n",
    "            unit = None\n",
    "        normalized.append({\"timestamp\": timestamp, \"data\": {\"value\": value, \"unit\": unit}})\n",
    "    return normalized\n",
    "\n",
    "normalize_entries_udf = udf(normalize_entries, target_schema)\n",
    "\n",
    "df_zipped = df.select(\n",
    "    \"vin\",\n",
    "    array(\n",
    "        struct(lit(\"odometer\").alias(\"key\"), normalize_entries_udf(col(\"diagnostics.odometer\")).alias(\"entries\")),\n",
    "        struct(lit(\"battery_voltage\").alias(\"key\"), normalize_entries_udf(col(\"diagnostics.battery_voltage\")).alias(\"entries\")),\n",
    "        struct(lit(\"engine_coolant_temperature\").alias(\"key\"), normalize_entries_udfcol(\"diagnostics.engine_coolant_temperature\").alias(\"entries\")),\n",
    "        struct(lit(\"battery_level\").alias(\"key\"), normalize_entries_udfcol(\"charging.battery_level\").alias(\"entries\")),\n",
    "        struct(lit(\"battery_level_at_departure\").alias(\"key\"), normalize_entries_udfcol(\"charging.battery_level_at_departure\").alias(\"entries\")),\n",
    "        struct(lit(\"charging_rate\").alias(\"key\"), normalize_entries_udf(col(\"charging.charging_rate\")).alias(\"entries\")),\n",
    "        struct(lit(\"estimated_range\").alias(\"key\"), normalize_entries_udf(col(\"charging.estimated_range\")).alias(\"entries\")),\n",
    "        struct(lit(\"max_range\").alias(\"key\"), normalize_entries_udf(col(\"charging.max_range\")).alias(\"entries\")),\n",
    "        struct(lit(\"plugged_in\").alias(\"key\"), normalize_entries_udfcol(\"charging.plugged_in\").alias(\"entries\")),\n",
    "        struct(lit(\"fully_charged_end_times\").alias(\"key\"), normalize_entries_udfcol(\"charging.fully_charged_end_times\").alias(\"entries\")),\n",
    "        struct(lit(\"preconditioning_scheduled_time\").alias(\"key\"), normalize_entries_udf(col(\"charging.preconditioning_scheduled_time\")).alias(\"entries\")),\n",
    "        struct(lit(\"preconditioning_remaining_time\").alias(\"key\"), normalize_entries_udfcol(\"charging.preconditioning_remaining_time\").alias(\"entries\")),\n",
    "        struct(lit(\"preconditioning_departure_status\").alias(\"key\"), normalize_entries_udfcol(\"charging.preconditioning_departure_status\").alias(\"entries\")),\n",
    "        struct(lit(\"smart_charging_status\").alias(\"key\"), normalize_entries_udfcol(\"charging.smart_charging_status\").alias(\"entries\")),\n",
    "        struct(lit(\"starter_battery_state\").alias(\"key\"), normalize_entries_udfcol(\"charging.starter_battery_state\").alias(\"entries\")),\n",
    "        struct(lit(\"status\").alias(\"key\"), normalize_entries_udfcol(\"charging.status\").alias(\"entries\")),\n",
    "        struct(lit(\"drive_start_time\").alias(\"key\"), normalize_entries_udfcol(\"usage.drive_start_time\").alias(\"entries\")),\n",
    "        struct(lit(\"drive_end_time\").alias(\"key\"), normalize_entries_udfcol(\"usage.drive_end_time\").alias(\"entries\")),\n",
    "        struct(lit(\"total_drive_distance\").alias(\"key\"), normalize_entries_udfcol(\"usage.total_drive_distance\").alias(\"entries\")),\n",
    "        struct(lit(\"total_drive_duration\").alias(\"key\"), normalize_entries_udfcol(\"usage.total_drive_duration\").alias(\"entries\")),\n",
    "        struct(lit(\"total_drive_energy_consumed\").alias(\"key\"), normalize_entries_udfcol(\"usage.total_drive_energy_consumed\").alias(\"entries\")),\n",
    "        struct(lit(\"session_start_time\").alias(\"key\"), normalize_entries_udfcol(\"charging_session.session_start_time\").alias(\"entries\")),\n",
    "        struct(lit(\"session_end_time\").alias(\"key\"), normalize_entries_udfcol(\"charging_session.session_end_time\").alias(\"entries\")),\n",
    "        struct(lit(\"session_energy_delivered\").alias(\"key\"), normalize_entries_udfcol(\"charging_session.session_energy_delivered\").alias(\"entries\")),\n",
    "        struct(lit(\"session_duration\").alias(\"key\"), normalize_entries_udfcol(\"charging_session.session_duration\").alias(\"entries\"))\n",
    "    ).alias(\"signals\")\n",
    ")\n",
    "\n",
    "# Explode sur les signaux\n",
    "df_exploded = df_zipped.select(\n",
    "    \"vin\",\n",
    "    explode(\"signals\").alias(\"signal\")\n",
    ").select(\n",
    "    \"vin\",\n",
    "    col(\"signal.key\").alias(\"key\"),\n",
    "    explode(col(\"signal.entries\")).alias(\"entry\")\n",
    ")\n",
    "\n",
    "\n",
    "df_exploded.show("
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Développement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.conf.set(\"spark.hadoop.fs.s3a.committer.name\", \"directory\")\n",
    "spark.conf.set(\"spark.sql.sources.commitProtocolClass\", \n",
    "               \"org.apache.spark.internal.io.cloud.PathOutputCommitProtocol\")\n",
    "spark.conf.set(\"spark.sql.parquet.output.committer.class\", \n",
    "               \"org.apache.spark.internal.io.cloud.BindingParquetOutputCommitter\")\n",
    "spark.conf.set(\"spark.shuffle.io.retryWait\", \"60s\")    # Attendre plus longtemps avant retry\n",
    "spark.conf.set(\"spark.shuffle.io.maxRetries\", \"10\")    # Plus de retries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pydantic_settings import BaseSettings\n",
    "from pydantic import Field\n",
    "import boto3\n",
    "from abc import ABC, abstractmethod\n",
    "import asyncio\n",
    "import aioboto3\n",
    "from botocore.exceptions import ClientError\n",
    "from fastapi import Depends\n",
    "from datetime import datetime\n",
    "import msgspec\n",
    "import asyncio\n",
    "from datetime import datetime\n",
    "import pandas as pd\n",
    "import pyarrow.parquet as pq\n",
    "import io\n",
    "from abc import ABC, abstractmethod\n",
    "import pyarrow as pa\n",
    "import pyarrow.fs as fs\n",
    "import pyarrow.dataset as ds\n",
    "from typing_extensions import Annotated, Iterable\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AsyncS3:\n",
    "    def __init__(self, max_concurrency: int = 100):\n",
    "        self.session = aioboto3.Session(\n",
    "            aws_access_key_id=\"SCW9P6Q1T26F2JGSC1AS\",\n",
    "            aws_secret_access_key=\"c702e16a-5a48-45f3-8538-5783b5c58e44\",\n",
    "            region_name=\"fr-par\",\n",
    "        )\n",
    "        self.bucket = \"bib-platform-prod-data\"\n",
    "        self._sem = asyncio.Semaphore(max_concurrency)\n",
    "        self.filesystem = fs.S3FileSystem(\n",
    "            access_key=\"SCW9P6Q1T26F2JGSC1AS\",\n",
    "            secret_key=\"c702e16a-5a48-45f3-8538-5783b5c58e44\",\n",
    "            endpoint_override=\"https://s3.fr-par.scw.cloud\",\n",
    "            region=\"fr-par\"\n",
    "        )\n",
    "\n",
    "        self.access_key = \"SCW9P6Q1T26F2JGSC1AS\"\n",
    "        self.secret_key = \"c702e16a-5a48-45f3-8538-5783b5c58e44\"\n",
    "\n",
    "        self.client_non_async = boto3.client(\n",
    "            \"s3\",\n",
    "            region_name=\"fr-par\",\n",
    "            endpoint_url=\"https://s3.fr-par.scw.cloud\",\n",
    "            aws_access_key_id=self.access_key,\n",
    "            aws_secret_access_key=self.secret_key\n",
    "        )\n",
    "\n",
    "    @property\n",
    "    def _client(self):\n",
    "        return self.session.client(\n",
    "            \"s3\",\n",
    "            region_name=\"fr-par\",\n",
    "            endpoint_url=\"https://s3.fr-par.scw.cloud\",\n",
    "            aws_access_key_id=\"SCW9P6Q1T26F2JGSC1AS\",\n",
    "            aws_secret_access_key=\"c702e16a-5a48-45f3-8538-5783b5c58e44\"\n",
    "        )\n",
    "\n",
    "\n",
    "    def list_folders(self, path: str = \"\"):\n",
    "        folders = set()\n",
    "        client = self.client_non_async\n",
    "        paginator = client.get_paginator(\"list_objects_v2\")\n",
    "        for page in paginator.paginate(Bucket=self.bucket, Prefix=path, Delimiter=\"/\"):\n",
    "            for cp in page.get(\"CommonPrefixes\", []):\n",
    "                prefix = cp.get(\"Prefix\")\n",
    "                if prefix:\n",
    "                    folders.add(prefix.rstrip(\"/\").split(\"/\")[-1])\n",
    "        return sorted(folders)\n",
    "\n",
    "\n",
    "    def list_files(self, path: str = \"\", type_file:str = \"\"):\n",
    "        files = []\n",
    "        client = self.client_non_async\n",
    "        paginator = client.get_paginator(\"list_objects_v2\")\n",
    "        for page in paginator.paginate(Bucket=self.bucket, Prefix=path):\n",
    "            for content in page.get(\"Contents\", []):\n",
    "                key = content[\"Key\"]\n",
    "                if key.endswith(type_file):\n",
    "                    files.append(key)\n",
    "        return sorted(files)\n",
    "\n",
    "    async def get_file(self, path: str) -> bytes:\n",
    "        async with self._sem:\n",
    "            async with self._client as client:  # type: ignore\n",
    "                try:\n",
    "                    response = await client.get_object(Bucket=self.bucket, Key=path)\n",
    "                    async with response[\"Body\"] as stream:\n",
    "                        return await stream.read()\n",
    "                except ClientError as e:\n",
    "                    if e.response[\"Error\"][\"Code\"] == \"NoSuchKey\":\n",
    "                        return None\n",
    "                    raise\n",
    "\n",
    "    async def get_files(self, paths):\n",
    "        results: dict[str, bytes] = {}\n",
    "\n",
    "        async def download(key: str):\n",
    "            async with self._sem:\n",
    "                data = await self.get_file(key)\n",
    "                if data is not None:\n",
    "                    results[key] = data\n",
    "        \n",
    "        await asyncio.gather(*(download(f) for f in paths))\n",
    "        return results\n",
    "\n",
    "    async def upload_file(self, path: str, file: bytes) -> None:\n",
    "        async with self._client as client:  # type: ignore\n",
    "            await client.put_object(Bucket=self.bucket, Key=path, Body=file)\n",
    "\n",
    "    async def delete_file(self, path: str) -> bool:\n",
    "        async with self._sem:\n",
    "            async with self._client as client:  # type: ignore\n",
    "                try:\n",
    "                    await client.delete_object(Bucket=self.bucket, Key=path)\n",
    "                    return True\n",
    "                except ClientError as e:\n",
    "                    if e.response[\"Error\"][\"Code\"] == \"NoSuchKey\":\n",
    "                        return False\n",
    "                    raise\n",
    "\n",
    "    async def download_folder(self, folder_path: str):\n",
    "        files = await self.list_files(folder_path)\n",
    "        return await self.get_files(files)\n",
    "        \n",
    "    async def delete_folder(self, prefix: str) -> int:\n",
    "        deleted_count = 0\n",
    "        async with self._sem:\n",
    "            async with self._client as client:  # type: ignore\n",
    "                paginator = client.get_paginator(\"list_objects_v2\")\n",
    "                async for page in paginator.paginate(Bucket=self.bucket, Prefix=prefix):\n",
    "                    contents = page.get(\"Contents\", [])\n",
    "                    if not contents:\n",
    "                        continue\n",
    "\n",
    "                    # Batch delete (doc says max 1000 at a time)\n",
    "                    for i in range(0, len(contents), 1000):\n",
    "                        batch = contents[i : i + 1000]\n",
    "                        keys = [{\"Key\": obj[\"Key\"]} for obj in batch]\n",
    "\n",
    "                        await client.delete_objects(\n",
    "                            Bucket=self.bucket, Delete={\"Objects\": keys, \"Quiet\": True}\n",
    "                        )\n",
    "\n",
    "            return deleted_count\n",
    "    \n",
    "    def get_parquet_partitioned_file(self, path: str):\n",
    "            return ds.dataset(f\"{self._settings.S3_BUCKET}/{path}\", filesystem=self.filesystem, format=\"parquet\", partitioning=\"hive\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "class ResponseToRaw(ABC):\n",
    "    \"\"\"Converter class that could be extended or modified easily to be used for all car brand conversion from response to raw. Only the  _get_files_to_add method have to be implemented to return the list of new data as dict. Each element of the list should be a dict representing one datapoint and the dict must contain at least the key 'date' and 'vin'\"\"\"\n",
    "\n",
    "    def __init__(self, brand_prefix: str, spark) -> None:\n",
    "        self._s3 = AsyncS3()\n",
    "        self.brand_prefix = brand_prefix\n",
    "        self.spark = spark\n",
    "\n",
    "\n",
    "    async def convert(self):\n",
    "        ts = await self._get_ts()\n",
    "        if ts.shape[0] > 0:\n",
    "            ts_last_date: datetime = pd.to_datetime(ts[\"readable_date\"]).max().to_pydatetime()\n",
    "        else:\n",
    "            ts_last_date: datetime = datetime(0,0,0)\n",
    "            \n",
    "\n",
    "        new_files = await self._get_files_to_add(ts_last_date)\n",
    "        new_df = pd.DataFrame(new_files)\n",
    "        extended_ts = pd.concat([ts, new_df])\n",
    "        await self._save_ts(extended_ts)\n",
    "\n",
    "    async def _get_ts(self) -> pd.DataFrame:\n",
    "        \n",
    "        dataset = self._s3.get_parquet_partitioned_file(\n",
    "            f\"raw_ts/{self.brand_prefix}/time_series/spark_raw_tss.parquet\"\n",
    "        ).to_table(columns=['readable_date'])\n",
    "\n",
    "\n",
    "        return dataset.to_pandas()\n",
    "        files = await self._s3.list_files(\n",
    "            f\"raw_ts/{self.brand_prefix}/time_series/spark_raw_tss.parquet\"\n",
    "            , type_file=\".parquet\"\n",
    "        )\n",
    "\n",
    "        files_bytes_dict = await self._s3.get_files(files)\n",
    "\n",
    "        print(\"Turn bites into pandas\", datetime.now())\n",
    "        # tables = []\n",
    "        # for b in files_bytes_dict.values():\n",
    "        #     buffer = pa.BufferReader(b)\n",
    "        #     table = pq.read_table(buffer)\n",
    "        #     tables.append(table)\n",
    "\n",
    "        # combined = pa.concat_tables(tables)\n",
    "\n",
    "        # if combined is None:\n",
    "        #     return pd.DataFrame()\n",
    "\n",
    "        return combined.to_pandas()\n",
    "\n",
    "    async def _save_ts(self, df: pd.DataFrame):\n",
    "        buffer = io.BytesIO()\n",
    "        df.to_parquet(buffer, engine=\"pyarrow\")\n",
    "        parquet_bytes = buffer.getvalue()\n",
    "        await self._s3.upload_file(\n",
    "            f\"raw_ts/{self.brand_prefix}/time_series/cp_spark_raw_tss.parquet\", parquet_bytes\n",
    "        )\n",
    "    \n",
    "    @abstractmethod\n",
    "    async def _get_files_to_add(self, last_date: datetime):\n",
    "        pass\n",
    "\n",
    "    def _paths_to_download(self, last_date: datetime, **kwargs):\n",
    "        vins = self._s3.list_folders(f\"response/{self.brand_prefix}/\")\n",
    "\n",
    "        vins_data = [self._s3.list_files(f\"response/{self.brand_prefix}/{vin}/\", **kwargs) for vin in vins]\n",
    "\n",
    "\n",
    "        path_to_dl = []\n",
    "        for vin_daily_files in vins_data:\n",
    "            pattern = re.compile(r\"\\d{4}-\\d{2}-\\d{2}\\.json$\")\n",
    "            vin_daily_files = [p for p in vin_daily_files if pattern.search(p.split(\"/\")[-1])]\n",
    "            path_to_dl.extend(\n",
    "                path\n",
    "                for path in vin_daily_files\n",
    "                if datetime.strptime(path.split(\"/\")[-1], \"%Y-%m-%d.json\") > last_date\n",
    "            )\n",
    "\n",
    "        return path_to_dl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def align_dataframes_for_union(df1, df2, strategy=\"intersection\"):\n",
    "    \"\"\"\n",
    "    Aligne deux DataFrames pour l'union\n",
    "\n",
    "    Args:\n",
    "        df1, df2: DataFrames à unir\n",
    "        strategy: \"intersection\" (colonnes communes) ou \"union\" (toutes les colonnes)\n",
    "    \"\"\"\n",
    "\n",
    "    cols1 = set(df1.columns)\n",
    "    cols2 = set(df2.columns)\n",
    "\n",
    "    print(f\"DataFrame 1: {len(cols1)} colonnes\")\n",
    "    print(f\"DataFrame 2: {len(cols2)} colonnes\")\n",
    "\n",
    "    if strategy == \"intersection\":\n",
    "        # Utiliser seulement les colonnes communes\n",
    "        common_cols = cols1 & cols2\n",
    "        print(f\"Colonnes communes: {len(common_cols)}\")\n",
    "\n",
    "        # Colonnes manquantes dans chaque DataFrame\n",
    "        missing_in_df1 = cols2 - cols1\n",
    "        missing_in_df2 = cols1 - cols2\n",
    "\n",
    "        if missing_in_df1:\n",
    "            print(f\"Colonnes manquantes dans df1: {missing_in_df1}\")\n",
    "        if missing_in_df2:\n",
    "            print(f\"Colonnes manquantes dans df2: {missing_in_df2}\")\n",
    "\n",
    "        # Sélectionner seulement les colonnes communes\n",
    "        df1_aligned = df1.select(*sorted(common_cols))\n",
    "        df2_aligned = df2.select(*sorted(common_cols))\n",
    "\n",
    "    elif strategy == \"union\":\n",
    "        # Utiliser toutes les colonnes, ajouter des colonnes NULL pour les manquantes\n",
    "        all_cols = sorted(cols1 | cols2)\n",
    "        print(f\"Toutes les colonnes: {len(all_cols)}\")\n",
    "\n",
    "        # Ajouter les colonnes manquantes à df1\n",
    "        for col in all_cols:\n",
    "            if col not in cols1:\n",
    "                df1 = df1.withColumn(col, lit(None).cast(\"string\"))\n",
    "\n",
    "        # Ajouter les colonnes manquantes à df2\n",
    "        for col in all_cols:\n",
    "            if col not in cols2:\n",
    "                df2 = df2.withColumn(col, lit(None).cast(\"string\"))\n",
    "\n",
    "        df1_aligned = df1.select(*all_cols)\n",
    "        df2_aligned = df2.select(*all_cols)\n",
    "\n",
    "    return df1_aligned, df2_aligned"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.types import StructType, StructField, LongType, DoubleType, StringType\n",
    "from pyspark.sql.types import StructType, StructField, StringType\n",
    "import time\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.types import *\n",
    "\n",
    "schema = StructType([\n",
    "    StructField(\"vin\", StringType(), True),\n",
    "    StructField(\"timestamp\", LongType(), True),\n",
    "    StructField(\"readable_date\", StringType(), True),\n",
    "    StructField(\"createdAt\", StringType(), True),\n",
    "    StructField(\"data\", ArrayType(\n",
    "        StructType([\n",
    "            StructField(\"key\", StringType(), True),\n",
    "            StructField(\"value\", StructType([\n",
    "                StructField(\"doubleValue\", DoubleType(), True),\n",
    "                StructField(\"intValue\", IntegerType(), True),\n",
    "                StructField(\"booleanValue\", BooleanType(), True),\n",
    "                StructField(\"stringValue\", StringType(), True),\n",
    "                StructField(\"carTypeValue\", StringType(), True),\n",
    "                StructField(\"bmsStateValue\", StringType(), True),\n",
    "                StructField(\"climateKeeperModeValue\", StringType(), True),\n",
    "                StructField(\"chargePortValue\", StringType(), True),\n",
    "                StructField(\"defrostModeValue\", StringType(), True),\n",
    "                StructField(\"detailedChargeStateValue\", StringType(), True),\n",
    "                StructField(\"fastChargerValue\", StringType(), True),\n",
    "                StructField(\"hvacAutoModeValue\", StringType(), True),\n",
    "                StructField(\"hvacPowerValue\", StringType(), True),\n",
    "                StructField(\"sentryModeStateValue\", StringType(), True),\n",
    "                StructField(\"invalid\", BooleanType(), True),\n",
    "            ]))\n",
    "        ])\n",
    "    ), True)\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col, coalesce, explode, expr\n",
    "from pyspark.sql.functions import udf\n",
    "\n",
    "\n",
    "\n",
    "class TFTResponseToRawSpark(ResponseToRaw):\n",
    "\n",
    "    def _get_files_to_add(self, last_date: datetime, schema):\n",
    "        logger.info(\"Retrieve files to \", datetime.now())\n",
    "        path_to_download: list[str] = self._paths_to_download(last_date, type_file=\".json\")\n",
    "        logger.info(\"Change all files to complete path\", datetime.now())\n",
    "        logger.info(len(path_to_download))\n",
    "\n",
    "        path_to_download = [f\"s3a://bib-platform-prod-data/{path}\" for path in path_to_download][:1000]\n",
    "\n",
    "        path_to_download = self.group_paths_by_vin(path_to_download)\n",
    "        \n",
    "        df_write = None \n",
    "        \n",
    "        for vin, paths in path_to_download.items():\n",
    "            sp_df = self.spark.read.option(\"multiline\", \"true\").schema(schema).json(paths)\n",
    "            \n",
    "            logger.info(\"Parse the data\", datetime.now())\n",
    "            \n",
    "            parsed_df = self._parse_tesla_data(sp_df)\n",
    "        \n",
    "            logger.info(\"Pivot the data\", datetime.now())\n",
    "            pivoted = parsed_df.repartition(\"vin\").groupBy(\"vin\", \"timestamp\", \"readable_date\", \"createdAt\") \\\n",
    "                .pivot(\"key\") \\\n",
    "                .agg(expr(\"first(value)\"))    \n",
    "\n",
    "\n",
    "            if df_write is not None:\n",
    "                pivoted, df_write = align_dataframes_for_union(\n",
    "                    pivoted, df_write, strategy=\"intersection\"\n",
    "                )\n",
    "                df_write = df_write.union(pivoted).dropDuplicates()\n",
    "            else:\n",
    "                df_write = pivoted\n",
    "                \n",
    "            pivoted.unpersist()\n",
    "            del pivoted\n",
    "            \n",
    "            \n",
    "        self._save_as_parquet(df_write, \"raw_ts_spark.parquet\")\n",
    "\n",
    "        return df_write\n",
    "\n",
    "    def group_paths_by_vin(self, paths):\n",
    "        grouped = {}\n",
    "    \n",
    "        for path in paths:\n",
    "            parts = path.strip(\"/\").split(\"/\")\n",
    "            if len(parts) < 2:\n",
    "                continue  # ignorer les paths invalides\n",
    "            vin = parts[-2]\n",
    "            # Initialise la liste si vin pas encore vu\n",
    "            if vin not in grouped:\n",
    "                grouped[vin] = []\n",
    "    \n",
    "            grouped[vin].append(path)\n",
    "    \n",
    "        return grouped\n",
    "\n",
    "    def _parse_tesla_data(self, df):\n",
    "        # Explode le tableau data\n",
    "        exploded_df = df.select(\n",
    "            \"vin\", \"timestamp\", \"readable_date\", \"createdAt\",\n",
    "            explode(\"data\").alias(\"data_item\")\n",
    "        )\n",
    "        \n",
    "        # Extraire key et value\n",
    "        parsed_df = exploded_df.select(\n",
    "            \"vin\", \"timestamp\", \"readable_date\", \"createdAt\",\n",
    "            col(\"data_item.key\").alias(\"key\"),\n",
    "            col(\"data_item.value\").alias(\"value\")\n",
    "        )\n",
    "        \n",
    "        # Extraire les valeurs selon leur type avec une UDF\n",
    "        def extract_value(value_struct):\n",
    "            if value_struct is None:\n",
    "                return None\n",
    "            \n",
    "            # Essayer stringValue\n",
    "            if hasattr(value_struct, 'stringValue') and value_struct.stringValue is not None:\n",
    "                return str(value_struct.stringValue)\n",
    "            # Essayer doubleValue\n",
    "            elif hasattr(value_struct, 'doubleValue') and value_struct.doubleValue is not None:\n",
    "                return str(value_struct.doubleValue)\n",
    "            # Essayer intValue\n",
    "            elif hasattr(value_struct, 'intValue') and value_struct.intValue is not None:\n",
    "                return str(value_struct.intValue)\n",
    "            # Essayer booleanValue\n",
    "            elif hasattr(value_struct, 'booleanValue') and value_struct.booleanValue is not None:\n",
    "                return str(value_struct.booleanValue)\n",
    "            # Essayer detailedChargeStateValue\n",
    "            elif hasattr(value_struct, 'detailedChargeStateValue') and value_struct.detailedChargeStateValue is not None:\n",
    "                return str(value_struct.detailedChargeStateValue)\n",
    "            # Essayer les autres types si nécessaire\n",
    "            elif hasattr(value_struct, 'cableTypeValue') and value_struct.cableTypeValue is not None:\n",
    "                return str(value_struct.cableTypeValue)\n",
    "            elif hasattr(value_struct, 'climateKeeperModeValue') and value_struct.climateKeeperModeValue is not None:\n",
    "                return str(value_struct.climateKeeperModeValue)\n",
    "            elif hasattr(value_struct, 'defrostModeValue') and value_struct.defrostModeValue is not None:\n",
    "                return str(value_struct.defrostModeValue)\n",
    "            elif hasattr(value_struct, 'fastChargerValue') and value_struct.fastChargerValue is not None:\n",
    "                return str(value_struct.fastChargerValue)\n",
    "            elif hasattr(value_struct, 'hvacAutoModeValue') and value_struct.hvacAutoModeValue is not None:\n",
    "                return str(value_struct.hvacAutoModeValue)\n",
    "            elif hasattr(value_struct, 'hvacPowerValue') and value_struct.hvacPowerValue is not None:\n",
    "                return str(value_struct.hvacPowerValue)\n",
    "            \n",
    "            return None\n",
    "        extract_value_udf = udf(extract_value, StringType())\n",
    "        \n",
    "        result_df = parsed_df.select(\n",
    "            \"vin\", \"timestamp\", \"readable_date\", \"createdAt\", \"key\",\n",
    "            extract_value_udf(\"value\").alias(\"value\")\n",
    "        )\n",
    "        \n",
    "        return result_df\n",
    "        \n",
    "    def _save_as_parquet(self, df, key: str):\n",
    "            \"\"\"\n",
    "            Censée faire une concat de ce qui existe et écraser les fichiers déjà présents dans scaleway.\n",
    "            Aucune idée de la rapidité ni de la viabilité.\n",
    "            \"\"\"\n",
    "\n",
    "            s3_path = f\"s3a://bib-platform-prod-data/raw_ts/tesla-fleet-telemetry/time_series/{key}\"\n",
    "\n",
    "            try:\n",
    "                # Essayer de lire le fichier existant\n",
    "                processed = self.spark.read.parquet(s3_path)\n",
    "\n",
    "                # Vérifier si le DataFrame n'est pas vide\n",
    "                if processed.count() > 0:\n",
    "                    processed, df = align_dataframes_for_union(\n",
    "                        processed, df, strategy=\"intersection\"\n",
    "                    )\n",
    "                    df_write = processed.union(df).dropDuplicates()\n",
    "\n",
    "                else:\n",
    "                    df_write = df\n",
    "\n",
    "            except Exception as e:\n",
    "                # Si le fichier n'existe pas ou est corrompu\n",
    "                if (\n",
    "                    \"PATH_NOT_FOUND\" in str(e)\n",
    "                    or \"does not exist\" in str(e)\n",
    "                    or \"UNABLE_TO_INFER_SCHEMA\" in str(e)\n",
    "                ):\n",
    "                    df_write = df\n",
    "                else:\n",
    "                    # Autre erreur, on la relance\n",
    "                    raise e\n",
    "            \n",
    "\n",
    "            print(\"Write the data\", datetime.now())\n",
    "            df = df.persist()\n",
    "            df.count()\n",
    "            df_write.coalesce(100).write \\\n",
    "                .mode(\"overwrite\") \\\n",
    "                .option(\"parquet.compression\", \"snappy\") \\\n",
    "                .option(\"parquet.block.size\", 67108864) \\\n",
    "                .partitionBy(\"vin\") \\\n",
    "                .parquet(s3_path)\n",
    "            \"\"\"\n",
    "            df_write.coalesce(1).write \\\n",
    "            .mode(\"append\") \\\n",
    "            .option(\"parquet.compression\", \"snappy\") \\\n",
    "            .option(\"parquet.block.size\", 67108864) \\\n",
    "            .partitionBy(\"vin\") \\\n",
    "            .parquet(s3_path)\n",
    "            print(\"✅ Écriture réussie\")\n",
    "            \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TRtR = TFTResponseToRawSpark('tesla-fleet-telemetry', spark)\n",
    "\n",
    "paths = TRtR._paths_to_download(datetime(2025, 1, 1), type_file=\".json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def group_paths_by_vin(paths):\n",
    "    grouped = {}\n",
    "\n",
    "    for path in paths:\n",
    "        parts = path.strip(\"/\").split(\"/\")\n",
    "        if len(parts) < 2:\n",
    "            continue  # ignorer les paths invalides\n",
    "        vin = parts[-2]\n",
    "        # Initialise la liste si vin pas encore vu\n",
    "        if vin not in grouped:\n",
    "            grouped[vin] = []\n",
    "\n",
    "        grouped[vin].append(path)\n",
    "\n",
    "    return grouped\n",
    "\n",
    "paths_added = [f\"s3a://bib-platform-prod-data/{path}\" for path in paths]\n",
    "paths_grouped = group_paths_by_vin(paths_added)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "keys = list(paths_grouped.keys())[400:440] # Next 460 - 480\n",
    "paths_final = [paths_grouped[key] for key in keys]\n",
    "flattened = [item for sublist in paths_final for item in sublist]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start = time.time()\n",
    "\n",
    "sp_df = spark.read.option(\"multiline\", \"true\").schema(schema).json(flattened)\n",
    "end = time.time()\n",
    "\n",
    "sp_df = sp_df.repartition('vin')\n",
    "sp_df = sp_df.coalesce(32)\n",
    "\n",
    "\n",
    "sp_df.rdd.getNumPartitions()\n",
    "\n",
    "print(f\"Temps écoulé avec schema: {end - start:.2f} secondes\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(sp_df.rdd.getNumPartitions())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_tesla_data(df):\n",
    "    # Explode le tableau data\n",
    "    exploded_df = df.select(\n",
    "        \"vin\", \"timestamp\", \"readable_date\", \"createdAt\",\n",
    "        explode(\"data\").alias(\"data_item\")\n",
    "    )\n",
    "    \n",
    "    # Extraire key et value\n",
    "    parsed_df = exploded_df.select(\n",
    "        \"vin\", \"timestamp\", \"readable_date\", \"createdAt\",\n",
    "        col(\"data_item.key\").alias(\"key\"),\n",
    "        col(\"data_item.value\").alias(\"value\")\n",
    "    ) # .coalesce(32) à tester\n",
    "    \n",
    "    # Extraire les valeurs selon leur type avec une UDF\n",
    "    def extract_value(value_struct):\n",
    "        if value_struct is None:\n",
    "            return None\n",
    "        \n",
    "        # Essayer stringValue\n",
    "        if hasattr(value_struct, 'stringValue') and value_struct.stringValue is not None:\n",
    "            return str(value_struct.stringValue)\n",
    "        # Essayer doubleValue\n",
    "        elif hasattr(value_struct, 'doubleValue') and value_struct.doubleValue is not None:\n",
    "            return str(value_struct.doubleValue)\n",
    "        # Essayer intValue\n",
    "        elif hasattr(value_struct, 'intValue') and value_struct.intValue is not None:\n",
    "            return str(value_struct.intValue)\n",
    "        # Essayer booleanValue\n",
    "        elif hasattr(value_struct, 'booleanValue') and value_struct.booleanValue is not None:\n",
    "            return str(value_struct.booleanValue)\n",
    "        # Essayer detailedChargeStateValue\n",
    "        elif hasattr(value_struct, 'detailedChargeStateValue') and value_struct.detailedChargeStateValue is not None:\n",
    "            return str(value_struct.detailedChargeStateValue)\n",
    "        # Essayer les autres types si nécessaire\n",
    "        elif hasattr(value_struct, 'cableTypeValue') and value_struct.cableTypeValue is not None:\n",
    "            return str(value_struct.cableTypeValue)\n",
    "        elif hasattr(value_struct, 'climateKeeperModeValue') and value_struct.climateKeeperModeValue is not None:\n",
    "            return str(value_struct.climateKeeperModeValue)\n",
    "        elif hasattr(value_struct, 'defrostModeValue') and value_struct.defrostModeValue is not None:\n",
    "            return str(value_struct.defrostModeValue)\n",
    "        elif hasattr(value_struct, 'fastChargerValue') and value_struct.fastChargerValue is not None:\n",
    "            return str(value_struct.fastChargerValue)\n",
    "        elif hasattr(value_struct, 'hvacAutoModeValue') and value_struct.hvacAutoModeValue is not None:\n",
    "            return str(value_struct.hvacAutoModeValue)\n",
    "        elif hasattr(value_struct, 'hvacPowerValue') and value_struct.hvacPowerValue is not None:\n",
    "            return str(value_struct.hvacPowerValue)\n",
    "        \n",
    "        return None\n",
    "    extract_value_udf = udf(extract_value, StringType())\n",
    "    \n",
    "    result_df = parsed_df.select(\n",
    "        \"vin\", \"timestamp\", \"readable_date\", \"createdAt\", \"key\",\n",
    "        extract_value_udf(\"value\").alias(\"value\")\n",
    "    )\n",
    "    \n",
    "    return result_df\n",
    "\n",
    "\n",
    "start = time.time()\n",
    "parsed_df = parse_tesla_data(sp_df)\n",
    "end = time.time()\n",
    "print(f\"Temps écoulé pour parsing: {end - start:.2f} secondes\")\n",
    "\n",
    "start = time.time()\n",
    "pivoted = parsed_df.groupBy(\"vin\", \"timestamp\", \"readable_date\", \"createdAt\") \\\n",
    "    .pivot(\"key\") \\\n",
    "    .agg(expr(\"first(value)\"))\n",
    "end = time.time()\n",
    "print(f\"Temps écoulé pour pivoté: {end - start:.2f} secondes\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import DataFrame\n",
    "from pyspark.sql.utils import AnalysisException\n",
    "from datetime import datetime\n",
    "import logging\n",
    "\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "def save_as_parquet(df: DataFrame, key: str):\n",
    "    \"\"\"\n",
    "    Écrit un DataFrame en Parquet dans S3 :\n",
    "    - Fusionne avec les données existantes si présentes\n",
    "    - Évite les doublons\n",
    "    - Partitionne par 'vin'\n",
    "    - Gère les cas d'erreur proprement\n",
    "    \"\"\"\n",
    "\n",
    "    s3_path = f\"s3a://bib-platform-prod-data/raw_ts/tesla-fleet-telemetry/time_series/{key}\"\n",
    "\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Lecture sécurisée du parquet existant (s'il existe)\n",
    "        processed = spark.read.parquet(s3_path)\n",
    "        if processed.rdd.isEmpty():\n",
    "            df_write = df\n",
    "        else:\n",
    "            start = time.time()\n",
    "            processed, df = align_dataframes_for_union(processed, df, strategy=\"intersection\")\n",
    "            df_write = processed.unionByName(df).dropDuplicates()\n",
    "            end = time.time()\n",
    "            print(f\"Temps écoulé pour écrire: {end - start:.2f} secondes\")\n",
    "            \n",
    "    except AnalysisException as e:\n",
    "        if \"Path does not exist\" in str(e) or \"Unable to infer schema\" in str(e):\n",
    "            df_write = df\n",
    "        else:\n",
    "            raise e\n",
    "    except Exception as e:\n",
    "        # Pour tout autre cas (permissions, corruptions…)\n",
    "        df_write = df\n",
    "    \"\"\"\n",
    "    \n",
    "    start = time.time()\n",
    "    (\n",
    "        df.repartition('vin')  # Ajustable selon le volume (~100-200MB par fichier)\n",
    "        .write\n",
    "        .mode(\"append\")  # ou \"append\" si tu veux accumuler sans relire\n",
    "        .option(\"parquet.compression\", \"snappy\")\n",
    "        .option(\"parquet.block.size\", 134217728)  # 128MB\n",
    "        .partitionBy(\"vin\")\n",
    "        .parquet(s3_path)\n",
    "    )\n",
    "    end = time.time()\n",
    "    print(f\"Temps écoulé pour écrire: {end - start:.2f} secondes\")\n",
    "\n",
    "start = time.time()\n",
    "save_as_parquet(pivoted, \"raw_ts_spark.parquet\")\n",
    "end = time.time()\n",
    "print(f\"Temps écoulé pour écrire: {end - start:.2f} secondes\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# To delete"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "from core.s3.s3_utils import S3Service\n",
    "\n",
    "\n",
    "total_size = []\n",
    "bucket = S3Service()\n",
    "\n",
    "\n",
    "total_size = 4019.10141225962 *1024 *1024\n",
    "nb_vin = 550"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_date = ptss.coalesce(32).orderBy(\"date\", ascending=False).select(\"date\").first()[0]\n",
    "print(f\"Date maximale: {max_date}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ptss = bucket.read_parquet_df_spark(spark,'raw_results/spark_tesla-fleet-telemetry.parquet/vin=XP7YGCFS4RB395709ts\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_pd = ptss.toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# df_pd[['createdAt', 'readable_date']][df_pd.readable_date.isna()].sort_values('createdAt', ascending=False)\n",
    "\n",
    "df_pd.sort_values('charging_status_idx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from core.s3.s3_utils import S3Service\n",
    "\n",
    "bucket = S3Service()\n",
    "\n",
    "size, length = bucket.get_object_size(\"response/tesla-fleet-telemetry/\")\n",
    "\n",
    "\n",
    "\n",
    "def get_ideal_nb_partitions(file_size_bytes: float, nb_vin: int) -> int:\n",
    "    \"\"\"\n",
    "    Calcule le nombre idéal de partitions Spark basé sur la taille du fichier et le nombre de VINs.\n",
    "    \n",
    "    Cette fonction détermine le nombre optimal de partitions pour optimiser les performances\n",
    "    Spark en fonction de la taille moyenne par VIN et de la recommandation de 128MB par partition.\n",
    "    \n",
    "    Args:\n",
    "        file_size_bytes (float): Taille totale du fichier en octets\n",
    "        nb_vin (int): Nombre de VINs (véhicules) dans le fichier\n",
    "    \n",
    "    Returns:\n",
    "        int: Nombre idéal de partitions Spark\n",
    "        \n",
    "    Raises:\n",
    "        ValueError: Si file_size_bytes ou nb_vin sont négatifs ou nuls\n",
    "    \"\"\"\n",
    "    # Validation des paramètres\n",
    "    if file_size_bytes <= 0 or nb_vin <= 0:\n",
    "        raise ValueError(\"file_size_bytes et nb_vin doivent être positifs\")\n",
    "    \n",
    "    # Calcul de la taille moyenne par VIN\n",
    "    size_file_mb = file_size_bytes / (1024 * 1024)\n",
    "    print(f\"Taille du fichier: {size_file_mb:.2f} MB\")\n",
    "    \n",
    "    avg_size_file_vin_mb = size_file_mb / nb_vin\n",
    "    print(f\"Taille moyenne par VIN: {avg_size_file_vin_mb:.2f} MB\")\n",
    "    \n",
    "    # Calcul du nombre idéal de VINs par partition (basé sur 128MB recommandé)\n",
    "    nb_vin_ideal_size = 128 / avg_size_file_vin_mb\n",
    "    print(f\"Nombre idéal de VINs par partition: {nb_vin_ideal_size:.2f}\")\n",
    "\n",
    "    # Logique de décision\n",
    "    if nb_vin_ideal_size < 0.5:\n",
    "        print(\"⚠️  Taille moyenne par VIN > 256 MB, partitionnement par VIN non recommandé\")\n",
    "        return nb_vin\n",
    "    elif nb_vin_ideal_size < 1:\n",
    "        print(\"ℹ️  Taille par VIN optimale, utilisation du nombre de VINs\")\n",
    "        return nb_vin\n",
    "    else:\n",
    "        optimal_partitions = int(nb_vin / nb_vin_ideal_size)\n",
    "        if optimal_partitions % 2 == 0:\n",
    "            pass\n",
    "        else:\n",
    "            optimal_partitions += 1\n",
    "        print(f\"✅ Partitionnement optimisé: {optimal_partitions} partitions\") \n",
    "        return optimal_partitions,\n",
    "\n",
    "get_ideal_nb_partitions(size, 550)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_size / (104*1024)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.coalesce(32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import functions as F\n",
    "result = df.filter(F.col(\"soh\").isNotNull()) \\\n",
    "          .groupBy(\"vin\") \\\n",
    "        .distinct('vin')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tous les VINs\n",
    "all_vins = df.select(\"vin\").distinct().collect()\n",
    "all_vin_set = {row[\"vin\"] for row in all_vins}\n",
    "\n",
    "# VINs avec SOH\n",
    "vins_with_soh = df.filter(F.col(\"soh\").isNotNull()) \\\n",
    "                 .select(\"vin\") \\\n",
    "                 .distinct() \\\n",
    "                 .collect()\n",
    "vins_with_soh_set = {row[\"vin\"] for row in vins_with_soh}\n",
    "\n",
    "# VINs sans SOH\n",
    "vins_without_soh_set = all_vin_set - vins_with_soh_set\n",
    "\n",
    "print(f\"VINs sans SOH: {vins_without_soh_set}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.select(\"vin\", \"model\", \"net_capacity\", \"energy_added\", \"soc_diff\", \"soh\").filter(F.col(\"vin\") == \"5YJ3E7EB1KF334219\").toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vins_without_soh_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "result = df.groupBy(\"vin\") \\\n",
    "          .agg(F.count(col(\"soh\").isNotNull().cast(\"int\")).alias(\"soh_count\")) \\\n",
    "          .filter(col(\"soh_count\") == 0) \\\n",
    "          .count()\n",
    "\n",
    "print(f\"Nombre de VINs sans SOH: {result}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import *\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark import SparkConf\n",
    "from core.s3.settings import S3Settings\n",
    "import os\n",
    "\n",
    "def create_spark_session(access_key: str, secret_key: str) -> SparkSession:\n",
    "    \"\"\"\n",
    "    Create a session spark with a connexion to scaleway\n",
    "    \"\"\"\n",
    "    os.environ[\"PYSPARK_SUBMIT_ARGS\"] = (\n",
    "        \"--packages org.apache.hadoop:hadoop-aws:3.3.4,org.apache.spark:spark-hadoop-cloud_2.12:3.4.0 pyspark-shell\"\n",
    "    )\n",
    "\n",
    "    g1gc_options = (\n",
    "        \"-XX:+UseG1GC \"\n",
    "        \"-XX:MaxGCPauseMillis=100 \"\n",
    "        \"-XX:G1HeapRegionSize=32m \"\n",
    "        \"-XX:+UseStringDeduplication \"\n",
    "        \"-XX:+UnlockExperimentalVMOptions \"\n",
    "        \"-XX:+UseZGC \"\n",
    "        \"-XX:+DisableExplicitGC \"\n",
    "        \"-XX:+UseGCOverheadLimit \"\n",
    "        \"-XX:GCTimeRatio=9 \"\n",
    "        \"-XX:+PrintGCDetails \"\n",
    "        \"-XX:+PrintGCTimeStamps \"\n",
    "        \"-Xloggc:/tmp/spark-gc.log\"\n",
    "    )\n",
    "    \n",
    "    spark = (\n",
    "        SparkSession.builder.appName(\"Scaleway S3 Read JSON\")\n",
    "        .config(\"spark.jars.packages\", \"org.apache.hadoop:hadoop-aws:3.3.4,org.apache.spark:spark-hadoop-cloud_2.12:3.4.0\")\n",
    "        .config(\"spark.hadoop.fs.s3a.endpoint\", \"https://s3.fr-par.scw.cloud\")\n",
    "        .config(\"spark.hadoop.fs.s3a.access.key\", access_key)\n",
    "        .config(\"spark.hadoop.fs.s3a.secret.key\", secret_key)\n",
    "        .config(\"spark.hadoop.fs.s3a.impl\", \"org.apache.hadoop.fs.s3a.S3AFileSystem\")\n",
    "        .config(\"spark.hadoop.fs.s3a.path.style.access\", \"true\")\n",
    "        .config(\"spark.driver.host\", \"localhost\")\n",
    "        .config(\"spark.hadoop.fs.s3a.aws.credentials.provider\", \"org.apache.hadoop.fs.s3a.SimpleAWSCredentialsProvider\")\n",
    "        # Nouvelles configurations pour résoudre le ClassNotFoundException\n",
    "        .config(\"spark.hadoop.fs.s3a.experimental.input.fadvise\", \"normal\")\n",
    "        .config(\"spark.hadoop.fs.s3a.connection.maximum\", \"1000\")\n",
    "        .config(\"spark.hadoop.fs.s3a.threads.max\", \"20\")\n",
    "        .config(\"spark.hadoop.fs.s3a.threads.core\", \"10\")\n",
    "        .config(\"spark.hadoop.fs.s3a.buffer.dir\", \"/tmp\")\n",
    "        .config(\"spark.hadoop.fs.s3a.block.size\", \"134217728\")  # 128MB\n",
    "        .config(\"spark.hadoop.fs.s3a.multipart.size\", \"134217728\")  # 128MB\n",
    "        .config(\"spark.hadoop.fs.s3a.multipart.threshold\", \"134217728\")  # 128MB\n",
    "        # Configuration pour éviter les problèmes de commit protocol\n",
    "        .config(\"spark.sql.adaptive.enabled\", \"true\")\n",
    "        .config(\"spark.sql.adaptive.shuffle.targetPostShuffleInputSize\", \"64MB\")\n",
    "        .config(\"spark.sql.shuffle.partitions\", \"200\")\n",
    "        .config(\"spark.default.parallelism\", \"200\")\n",
    "        .config(\"spark.executor.memory\", \"10g\")\n",
    "        .config(\"spark.driver.memory\", \"10g\")\n",
    "        .config(\"spark.driver.maxResultSize\", \"4g\")\n",
    "        .config(\"spark.executor.extraJavaOptions\", g1gc_options)\n",
    "        .config(\"spark.driver.extraJavaOptions\", g1gc_options)\n",
    "        .getOrCreate()\n",
    "    )\n",
    "\n",
    "    return spark\n",
    "\n",
    "settings = S3Settings()\n",
    "\n",
    "spark = create_spark_session(settings.S3_KEY, settings.S3_SECRET)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = bucket.read_parquet_df_spark(spark, \"raw_results/spark_tesla-fleet-telemetry_v1.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[['consumption', 'estimated_cycles', 'odometer', 'range', 'soh', 'vin']].filter(col('consu').isNotNull()).count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.filter(F.col(\"vin\") == \"LRW3E7ET6RC169216\").toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import functions as F\n",
    "\n",
    "df.filter(F.col('soh').isNotNull()).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from core.sql_utils import *\n",
    "\n",
    "prod_engine = get_sqlalchemy_engine(is_prod=True)\n",
    "dev_engine = get_sqlalchemy_engine()\n",
    "df_vehicle_prod = pd.read_sql(\"\"\"\n",
    "    select vin, soh, timestamp, version\n",
    "    from vehicle_data vd\n",
    "    left join vehicle vm \n",
    "    on vm.id = vd.vehicle_id\n",
    "    left join vehicle_model vm2 \n",
    "    on vm2.id = vm.vehicle_model_id\n",
    "    left join oem o \n",
    "    on o.id= vm2.oem_id\n",
    "    where 1=1\n",
    "    and o.oem_name = 'tesla'\n",
    "    and vm.fleet_id = '70260bd9-2449-4f5b-81f2-73d9cb6b4b93'\n",
    "    \"\"\"\n",
    "    , prod_engine)\n",
    " \n",
    " \n",
    "df_vehicle_dev = pd.read_sql(\"\"\"\n",
    "    select vin, soh, timestamp\n",
    "    from vehicle_data vd\n",
    "    left join vehicle vm \n",
    "    on vm.id = vd.vehicle_id\n",
    "    left join vehicle_model vm2 \n",
    "    on vm2.id = vm.vehicle_model_id\n",
    "    left join oem o \n",
    "    on o.id= vm2.oem_id\n",
    "    where 1=1\n",
    "    and o.oem_name = 'tesla'\n",
    "    \"\"\"\n",
    "    , dev_engine)\n",
    "\n",
    "merged_df = pd.merge(df_vehicle_dev, df_vehicle_prod, on=['vin', 'timestamp'], how='left', suffixes=('_dev', '_prod'))\n",
    "merged_df[['vin', 'timestamp', 'soh_dev', 'soh_prod']][merged_df['soh_dev'].notna() & merged_df['soh_prod'].notna()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_df = pd.merge(df_vehicle_dev, df_vehicle_prod, on=['vin', 'timestamp'], how='left', suffixes=('_dev', '_prod'))\n",
    "\n",
    "merged_df['soh_diff'] = merged_df['soh_dev'] - merged_df['soh_prod']\n",
    "merged_df[['vin', 'timestamp', 'soh_dev', 'soh_prod', 'soh_diff']][merged_df['soh_dev'].notna() & merged_df['soh_prod'].notna()].sort_values(by='timestamp', ascending=False)\n",
    "\n",
    "merged_df[merged_df.soh_diff.min() == merged_df.soh_diff]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Filtrer les données où on a à la fois soh_dev et soh_prod\n",
    "filtered_df = merged_df[merged_df['soh_dev'].notna() & merged_df['soh_prod'].notna()]\n",
    "\n",
    "# Créer la figure avec plusieurs sous-graphiques\n",
    "fig, axes = plt.subplots(2, 2, figsize=(15, 12))\n",
    "fig.suptitle('Répartition des différences de SOH (dev - prod)', fontsize=16, fontweight='bold')\n",
    "\n",
    "# 1. Histogramme de la distribution\n",
    "axes[0, 0].hist(filtered_df['soh_diff'], bins=50, alpha=0.7, color='skyblue', edgecolor='black')\n",
    "axes[0, 0].set_title('Distribution des différences de SOH')\n",
    "axes[0, 0].set_xlabel('Différence SOH (dev - prod)')\n",
    "axes[0, 0].set_ylabel('Fréquence')\n",
    "axes[0, 0].axvline(x=0, color='red', linestyle='--', alpha=0.7, label='Différence = 0')\n",
    "axes[0, 0].legend()\n",
    "axes[0, 0].grid(True, alpha=0.3)\n",
    "\n",
    "# 2. Box plot\n",
    "axes[0, 1].boxplot(filtered_df['soh_diff'], patch_artist=True, \n",
    "                   boxprops=dict(facecolor='lightgreen', alpha=0.7))\n",
    "axes[0, 1].set_title('Box Plot des différences de SOH')\n",
    "axes[0, 1].set_ylabel('Différence SOH (dev - prod)')\n",
    "axes[0, 1].grid(True, alpha=0.3)\n",
    "\n",
    "# 3. Scatter plot soh_dev vs soh_prod\n",
    "axes[1, 0].scatter(filtered_df['soh_prod'], filtered_df['soh_dev'], alpha=0.6, s=20)\n",
    "axes[1, 0].plot([filtered_df['soh_prod'].min(), filtered_df['soh_prod'].max()], \n",
    "                [filtered_df['soh_prod'].min(), filtered_df['soh_prod'].max()], \n",
    "                'r--', alpha=0.7, label='Ligne de parité')\n",
    "axes[1, 0].set_title('SOH Dev vs SOH Prod')\n",
    "axes[1, 0].set_xlabel('SOH Prod')\n",
    "axes[1, 0].set_ylabel('SOH Dev')\n",
    "axes[1, 0].legend()\n",
    "axes[1, 0].grid(True, alpha=0.3)\n",
    "\n",
    "# 4. Violin plot\n",
    "axes[1, 1].violinplot(filtered_df['soh_diff'], showmeans=True)\n",
    "axes[1, 1].set_title('Violin Plot des différences de SOH')\n",
    "axes[1, 1].set_ylabel('Différence SOH (dev - prod)')\n",
    "axes[1, 1].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Statistiques descriptives\n",
    "print(\"Statistiques descriptives des différences de SOH:\")\n",
    "print(filtered_df['soh_diff'].describe())\n",
    "\n",
    "# Pourcentage de différences positives/négatives\n",
    "positive_diff = (filtered_df['soh_diff'] > 0).sum()\n",
    "negative_diff = (filtered_df['soh_diff'] < 0).sum()\n",
    "zero_diff = (filtered_df['soh_diff'] == 0).sum()\n",
    "total = len(filtered_df)\n",
    "\n",
    "print(f\"\\nRépartition des différences:\")\n",
    "print(f\"Différences positives (dev > prod): {positive_diff} ({positive_diff/total*100:.1f}%)\")\n",
    "print(f\"Différences négatives (dev < prod): {negative_diff} ({negative_diff/total*100:.1f}%)\")\n",
    "print(f\"Différences nulles (dev = prod): {zero_diff} ({zero_diff/total*100:.1f}%)\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "data-env",
   "language": "python",
   "name": "data-env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

