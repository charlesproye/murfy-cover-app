{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from core.s3_utils import S3_Bucket\n",
    "from core.spark_utils import create_spark_session\n",
    "\n",
    "bucket = S3_Bucket()\n",
    "\n",
    "# Création de la session Spark\n",
    "creds = bucket.get_creds_from_dot_env()\n",
    "spark_session = create_spark_session(\n",
    "        creds[\"aws_access_key_id\"],\n",
    "        creds[\"aws_secret_access_key\"]\n",
    ")\n",
    "\n",
    "spark_session.conf.set(\"spark.sql.adaptive.enabled\", \"true\")\n",
    "spark_session.conf.set(\n",
    "    \"spark.sql.adaptive.advisoryPartitionSizeInBytes\", \"128MB\"\n",
    ")\n",
    "spark_session.conf.set(\"spark.sql.adaptive.coalescePartitions.enabled\", \"true\")\n",
    "spark_session.conf.set(\"spark.sql.adaptive.advisoryPartitionSizeInBytes\", \"128m\")\n",
    "spark_session.conf.set(\"spark.sql.debug.maxToStringFields\", 1000)\n",
    "\n",
    "# Configuration spécifique pour Scaleway S3\n",
    "spark_session.conf.set(\"spark.hadoop.fs.s3a.impl\", \"org.apache.hadoop.fs.s3a.S3AFileSystem\")\n",
    "spark_session.conf.set(\"spark.hadoop.fs.s3a.aws.credentials.provider\", \"org.apache.hadoop.fs.s3a.SimpleAWSCredentialsProvider\")\n",
    "spark_session.conf.set(\"spark.hadoop.fs.s3a.access.key\", creds[\"aws_access_key_id\"])\n",
    "spark_session.conf.set(\"spark.hadoop.fs.s3a.secret.key\", creds[\"aws_secret_access_key\"])\n",
    "spark_session.conf.set(\"spark.hadoop.fs.s3a.endpoint\", \"s3.fr-par.scw.cloud\")\n",
    "spark_session.conf.set(\"spark.hadoop.fs.s3a.path.style.access\", \"true\")\n",
    "spark_session.conf.set(\"spark.hadoop.fs.s3a.connection.ssl.enabled\", \"true\")\n",
    "spark_session.conf.set(\"spark.hadoop.fs.s3a.connection.maximum\", \"100\")\n",
    "\n",
    "# Configuration spécifique pour éviter AWS par défaut\n",
    "spark_session.conf.set(\"spark.hadoop.fs.s3a.aws.credentials.provider\", \"org.apache.hadoop.fs.s3a.SimpleAWSCredentialsProvider\")\n",
    "spark_session.conf.set(\"spark.hadoop.fs.s3a.endpoint.region\", \"fr-par\")\n",
    "\n",
    "# Utiliser l'URL avec le bon endpoint\n",
    "df_raw = spark_session.read \\\n",
    "    .option(\"multiline\", \"true\") \\\n",
    "    .json(\"s3a://bib-platform-prod-data/response/tesla-fleet-telemetry/5YJ3E7EB1KF334219/2025-03-24.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "vins = bucket.list_subfolders(\"response/tesla-fleet-telemetry/\")\n",
    "vin_random = random.sample(vins, 1)[0]\n",
    "print(vin_random)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sp_raw = spark_session.read.option(\"multiline\", \"true\").json(f\"s3a://bib-platform-prod-data/response/tesla-fleet-telemetry/{vin_random}/*.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sp_raw.rdd.getNumPartitions()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sp_raw.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "df_spark = bucket.read_parquet_df_spark(spark_session, \"raw_ts/tesla-fleet-telemetry/time_series/spark_raw_tss.parquet/vin=XP7YGCES5RB479409/part-00000-d9d5b1f6-565e-453c-96f3-c650091ecb9a.c000.snappy.parquet\")\n",
    "df_spark.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "LIST_COL_TO_DROP = [\"model\"]\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col, explode, first\n",
    "from pyspark.sql.types import StringType\n",
    "from pyspark.sql.functions import udf\n",
    "\n",
    "def parse_tesla_data(df):\n",
    "    # Explode le tableau data\n",
    "    exploded_df = df.select(\n",
    "        \"vin\", \"timestamp\", \"readable_date\", \"createdAt\",\n",
    "        explode(\"data\").alias(\"data_item\")\n",
    "    )\n",
    "    \n",
    "    # Extraire key et value\n",
    "    parsed_df = exploded_df.select(\n",
    "        \"vin\", \"timestamp\", \"readable_date\", \"createdAt\",\n",
    "        col(\"data_item.key\").alias(\"key\"),\n",
    "        col(\"data_item.value\").alias(\"value\")\n",
    "    )\n",
    "    \n",
    "    # Extraire les valeurs selon leur type avec une UDF\n",
    "    def extract_value(value_struct):\n",
    "        if value_struct is None:\n",
    "            return None\n",
    "        \n",
    "        # Essayer stringValue\n",
    "        if hasattr(value_struct, 'stringValue') and value_struct.stringValue is not None:\n",
    "            return str(value_struct.stringValue)\n",
    "        # Essayer doubleValue\n",
    "        elif hasattr(value_struct, 'doubleValue') and value_struct.doubleValue is not None:\n",
    "            return str(value_struct.doubleValue)\n",
    "        # Essayer intValue\n",
    "        elif hasattr(value_struct, 'intValue') and value_struct.intValue is not None:\n",
    "            return str(value_struct.intValue)\n",
    "        # Essayer booleanValue\n",
    "        elif hasattr(value_struct, 'booleanValue') and value_struct.booleanValue is not None:\n",
    "            return str(value_struct.booleanValue)\n",
    "        # Essayer detailedChargeStateValue\n",
    "        elif hasattr(value_struct, 'detailedChargeStateValue') and value_struct.detailedChargeStateValue is not None:\n",
    "            return str(value_struct.detailedChargeStateValue)\n",
    "        # Essayer les autres types si nécessaire\n",
    "        elif hasattr(value_struct, 'cableTypeValue') and value_struct.cableTypeValue is not None:\n",
    "            return str(value_struct.cableTypeValue)\n",
    "        elif hasattr(value_struct, 'climateKeeperModeValue') and value_struct.climateKeeperModeValue is not None:\n",
    "            return str(value_struct.climateKeeperModeValue)\n",
    "        elif hasattr(value_struct, 'defrostModeValue') and value_struct.defrostModeValue is not None:\n",
    "            return str(value_struct.defrostModeValue)\n",
    "        elif hasattr(value_struct, 'fastChargerValue') and value_struct.fastChargerValue is not None:\n",
    "            return str(value_struct.fastChargerValue)\n",
    "        elif hasattr(value_struct, 'hvacAutoModeValue') and value_struct.hvacAutoModeValue is not None:\n",
    "            return str(value_struct.hvacAutoModeValue)\n",
    "        elif hasattr(value_struct, 'hvacPowerValue') and value_struct.hvacPowerValue is not None:\n",
    "            return str(value_struct.hvacPowerValue)\n",
    "        \n",
    "        return None\n",
    "    \n",
    "    extract_value_udf = udf(extract_value, StringType())\n",
    "    \n",
    "    result_df = parsed_df.select(\n",
    "        \"vin\", \"timestamp\", \"readable_date\", \"createdAt\", \"key\",\n",
    "        extract_value_udf(\"value\").alias(\"value\")\n",
    "    )\n",
    "    \n",
    "    return result_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_df = parse_tesla_data(sp_raw)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_df.repartition('readable_date').filter(col('readable_date') == '2025-06-11 00:00:02').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_spark.filter(col('readable_date') == '2025-06-11 00:00:02').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1_size = final_df.rdd.map(lambda row: len(str(row))).reduce(lambda a, b: a + b)\n",
    "df2_size = df_spark.rdd.map(lambda row: len(str(row))).reduce(lambda a, b: a + b)\n",
    "\n",
    "print(f\"Taille estimée de df1 : {df1_size / 1024:.2f} Ko\")\n",
    "print(f\"Taille estimée de df2 : {df2_size / 1024:.2f} Ko\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_dir_size(path):\n",
    "    total = 0\n",
    "    for dirpath, dirnames, filenames in os.walk(path):\n",
    "        for f in filenames:\n",
    "            fp = os.path.join(dirpath, f)\n",
    "            total += os.path.getsize(fp)\n",
    "    return total\n",
    "\n",
    "size1 = get_dir_size(\"/chemin/vers/parquet1\")\n",
    "size2 = get_dir_size(\"/chemin/vers/parquet2\")\n",
    "\n",
    "print(f\"Parquet1 : {size1 / 1024 / 1024:.2f} Mo\")\n",
    "print(f\"Parquet2 : {size2 / 1024 / 1024:.2f} Mo\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_spark.repartition(4).cache\n",
    "final_df.coalesce(4).cache"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_spark.write.mode(\"overwrite\").parquet(\"s3a://bib-platform-prod-data/raw_ts/tesla-fleet-telemetry/time_series/many_cols\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_df.write.mode(\"overwrite\").parquet(\"s3a://bib-platform-prod-data/raw_ts/tesla-fleet-telemetry/time_series//tmp/many_rows\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_spark"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "data-env",
   "language": "python",
   "name": "data-env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

