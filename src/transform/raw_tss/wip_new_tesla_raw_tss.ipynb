{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from core.pandas_utils import *\n",
    "from core.s3_utils import S3_Bucket\n",
    "from core.singleton_s3_bucket import bucket\n",
    "from core.caching_utils import cache_result\n",
    "from transform.raw_tss.tesla_raw_tss import get_raw_tss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@cache_result(\"./tesla_response_keys.parquet\", \"local_storage\")\n",
    "def get_tesla_keys() -> DF:\n",
    "    return bucket.list_responses_keys_of_brand(\"tesla\")\n",
    "\n",
    "response_keys = get_tesla_keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_tss_subset = get_raw_tss(read_parquet_kwargs={\"columns\":[\"vin\", \"readable_date\"]})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response_keys[\"date\"] = response_keys[\"file\"].str[:-5]\n",
    "response_keys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "last_parsed_date = (\n",
    "    raw_tss_subset\n",
    "    .groupby(\"vin\", observed=True, as_index=False)\n",
    "    .agg(last_parsed_date=pd.NamedAgg(\"readable_date\", \"max\"))\n",
    ")\n",
    "response_keys_to_parse = (\n",
    "    response_keys\n",
    "    .merge(last_parsed_date, \"left\", \"vin\")\n",
    "    .query(\"date > last_parsed_date\")\n",
    ")\n",
    "\n",
    "len(response_keys_to_parse) / len(response_keys)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response_keys_to_parse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response_keys_to_parse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def get_raw_tss(keys:DF) -> DF:\n",
    "#     return (\n",
    "#         keys\n",
    "#         .apply(parse_response_as_raw_ts, axis=\"columns\")\n",
    "#     )\n",
    "\n",
    "# def parse_response_as_raw_ts(key: Series) -> DF:\n",
    "#     # print(key)\n",
    "#     response = bucket.read_json_file(key[\"key\"])\n",
    "#     if response is None:\n",
    "#         print(f\"Did not parse key {key['key']} because the object returned by read_json_file was None.\")\n",
    "#         return DF()\n",
    "#     raw_ts = DF.from_records(response)\n",
    "#     raw_ts[\"vin\"] = key[\"vin\"]\n",
    "\n",
    "#     return raw_ts\n",
    "\n",
    "# raw_tss = get_raw_tss(response_keys_to_parse.iloc[:1000])\n",
    "# sanity_check(raw_tss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import os\n",
    "# import json\n",
    "# import logging\n",
    "# import boto3\n",
    "# import aioboto3\n",
    "# import pandas as pd\n",
    "# from concurrent.futures import ThreadPoolExecutor\n",
    "# from pandas import DataFrame as DF, Series\n",
    "\n",
    "# class Asyinc_S3_Bucket:\n",
    "#     def __init__(self, creds: dict[str, str] = None):\n",
    "#         \"\"\"Initialize S3 client with given credentials and environment variables.\"\"\"\n",
    "#         assert \"S3_ENDPOINT\" in os.environ, \"S3_ENDPOINT variable is not in the environment.\"\n",
    "        \n",
    "#         if creds is None:\n",
    "#             creds = S3_Bucket.get_creds_from_dot_env()\n",
    "#         self.creds = creds\n",
    "\n",
    "#         self._s3_client = boto3.client(\n",
    "#             \"s3\",\n",
    "#             region_name=\"fr-par\",\n",
    "#             endpoint_url=os.getenv(\"S3_ENDPOINT\"),\n",
    "#             aws_access_key_id=creds[\"aws_access_key_id\"],\n",
    "#             aws_secret_access_key=creds[\"aws_secret_access_key\"],\n",
    "#         )\n",
    "#         self.bucket_name = creds[\"bucket_name\"]\n",
    "#         self.logger = logging.getLogger(\"S3_BUCKET\")\n",
    "\n",
    "#     def read_json_file(self, key: str):\n",
    "#         \"\"\"Reads a single JSON file from S3.\"\"\"\n",
    "#         try:\n",
    "#             response = self._s3_client.get_object(Bucket=self.bucket_name, Key=key)\n",
    "#             object_content = response[\"Body\"].read().decode(\"utf-8\")\n",
    "#             return json.loads(object_content)\n",
    "#         except Exception as e:\n",
    "#             self.logger.error(f\"Failed to read key {key}: {e}\")\n",
    "#             return None\n",
    "\n",
    "#     def read_multiple_json_files(self, keys: list, max_workers=32):\n",
    "#         \"\"\"Reads multiple JSON files concurrently using ThreadPoolExecutor.\"\"\"\n",
    "#         with ThreadPoolExecutor(max_workers=max_workers) as executor:\n",
    "#             results = list(executor.map(self.read_json_file, keys))\n",
    "#         return results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_raw_tss(keys: DF, bucket: S3_Bucket) -> DF:\n",
    "    \"\"\"Reads multiple JSON files in parallel and parses them into a DataFrame.\"\"\"\n",
    "    json_responses = bucket.read_multiple_json_files(keys[\"key\"].tolist())\n",
    "\n",
    "    # Parse responses into DataFrame\n",
    "    parsed_dfs = [\n",
    "        parse_response_as_raw_ts(response, vin)\n",
    "        for response, vin in zip(json_responses, keys[\"vin\"])\n",
    "        if response is not None\n",
    "    ]\n",
    "    \n",
    "    # Concatenate results into a single DataFrame\n",
    "    return concat(parsed_dfs, ignore_index=True) if parsed_dfs else DF()\n",
    "\n",
    "def parse_response_as_raw_ts(response: dict, vin: str) -> DF:\n",
    "    \"\"\"Parses a single JSON response into a DataFrame and adds VIN.\"\"\"\n",
    "    raw_ts = DF.from_records(response)\n",
    "    raw_ts[\"vin\"] = vin\n",
    "    return raw_ts\n",
    "\n",
    "new_week_raw_tss = []\n",
    "\n",
    "# Usage\n",
    "for week, keys in response_keys_to_parse.astype({\"date\":\"datetime64[ms]\"}).groupby(pd.Grouper(key='date', freq='W')):\n",
    "    print(keys.shape)\n",
    "    raw_tss = get_raw_tss(keys, bucket)\n",
    "    bucket.save_df_as_parquet(raw_tss, f\"raw_ts/tesla/tesla_raw_tss_{week.date()}.parquet\")\n",
    "    new_week_raw_tss.append(raw_tss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_raw_tss = concat((get_raw_tss(), *new_week_raw_tss))\n",
    "bucket.save_df_as_parquet(new_raw_tss, \"/raw_tss/tesla/new_raw_tss.parquet\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "data_ev",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

