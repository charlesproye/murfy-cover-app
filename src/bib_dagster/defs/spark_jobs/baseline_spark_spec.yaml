type: Python
pythonVersion: "3"
mode: cluster
image: "rg.fr-par.scw.cloud/data-engineering/spark:latest"
imagePullPolicy: Always
sparkVersion: "4.0.1"
sparkConf:
  "spark.hadoop.fs.s3a.endpoint": "https://s3.fr-par.scw.cloud"
  "spark.hadoop.fs.s3a.impl": "org.apache.hadoop.fs.s3a.S3AFileSystem"
  "spark.hadoop.fs.s3a.path.style.access": "true"
  "spark.hadoop.fs.s3a.aws.credentials.provider": "org.apache.hadoop.fs.s3a.SimpleAWSCredentialsProvider"
  "spark.kubernetes.container.image.pullPolicy": "Always"
  "spark.eventLog.enabled": "true"
  "spark.eventLog.dir": "s3a://bib-prod-spark-history/spark-events"
timeToLiveSeconds: 300
restartPolicy:
  type: Never
driver:
  cores: 1
  memory: "512m"
  memoryOverhead: "512m"
  serviceAccount: spark-operator-spark
  envFrom:
    - secretRef:
        name: db-data-ev-secret
    - secretRef:
        name: db-data-eng-secret
  env:
    - name: S3_KEY
      valueFrom:
        secretKeyRef:
          name: s3-secret
          key: S3_KEY
    - name: S3_SECRET
      valueFrom:
        secretKeyRef:
          name: s3-secret
          key: S3_SECRET
    - name: AWS_ACCESS_KEY_ID
      valueFrom:
        secretKeyRef:
          name: s3-secret
          key: S3_KEY
    - name: AWS_SECRET_ACCESS_KEY
      valueFrom:
        secretKeyRef:
          name: s3-secret
          key: S3_SECRET
    - name: S3_BUCKET
      value: "bib-platform-prod-data"
executor:
  cores: 1
  instances: 1
  memory: "512m"
  envFrom:
    - secretRef:
        name: db-data-ev-secret
    - secretRef:
        name: db-data-eng-secret
  env:
  - name: S3_KEY
    valueFrom:
      secretKeyRef:
        name: s3-secret
        key: S3_KEY
  - name: S3_SECRET
    valueFrom:
      secretKeyRef:
        name: s3-secret
        key: S3_SECRET
  - name: S3_BUCKET
    value: "bib-platform-prod-data"
