{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Filtering soh results\n",
    "The goal of this notebook is to show how we filter out soh resutlts that are not valid.  \n",
    "As of writing this(2024-11-26) all the critirions are arbitrary."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import plotly.express as px\n",
    "\n",
    "from core.pandas_utils import *\n",
    "from core.config import valid_soh_points\n",
    "from core.s3.s3_utils import S3Service, S3Settings\n",
    "from core.spark_utils import create_spark_session\n",
    "settings = S3Settings()\n",
    "\n",
    "spark = create_spark_session(\n",
    "    settings.S3_KEY,\n",
    "    settings.S3_SECRET\n",
    ")\n",
    "\n",
    "s3 = S3Service()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data extraction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The results will be used to fill the vehicle_data table so we will format them to match the expected frequency."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = s3.read_parquet_df_spark(spark, 'result_phases/result_phases_tesla_fleet_telemetry.parquet').toPandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's visualize the raw results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "px.scatter(df, x=\"ODOMETER_LAST\", y=\"SOH\", color=\"VIN\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It's pretty clear that som results are outliers.  \n",
    "While this is *okay* for a statistical analysis we would prefet not to show them to our clients.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Filtering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To filter the results we will use the following criteria:\n",
    "- SoH must be between 0.5 and 1.0\n",
    "- SoH must be within a range defined by two slopes and intercepts.   \n",
    "These slopes are themselves defined by two points A and B stored in `transform.raw_results.config.VALID_SOH_POINTS`.   \n",
    "A and B were chosen arbitrarily.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "VALID_SOH_POINT = pd.DataFrame({\n",
    "  \"ODOMETER_LAST\": [20_000, 200_000, 0, 200_000],\n",
    "  \"SOH\": [1.0, 0.95, 0.9, 0.6],\n",
    "  \"point\": [\"A\", \"B\", \"A\", \"B\"],\n",
    "  \"bound\": [\"max\", \"max\", \"min\", \"min\"]\n",
    "}).set_index([\"bound\", \"point\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_results_by_lines_bounds(results: DF) -> DF:\n",
    "    max_intercept, max_slope = intercept_and_slope_from_points(VALID_SOH_POINT.xs(\"max\", level=0, drop_level=True))\n",
    "    min_intercept, min_slope = intercept_and_slope_from_points(VALID_SOH_POINT.xs(\"min\", level=0, drop_level=True))\n",
    "    return (\n",
    "        results\n",
    "        .eval(f\"max_valid_soh = ODOMETER_LAST * {max_slope:f} + {max_intercept:f}\")\n",
    "        .eval(f\"min_valid_soh = ODOMETER_LAST * {min_slope:f} + {min_intercept:f}\")\n",
    "        .eval(f\"soh_is_valid = SOH <= max_valid_soh & SOH >= min_valid_soh & SOH > 0.5 & SOH < 1.0\")\n",
    "        .pipe(debug_df, subset=[\"SOH\", \"max_valid_soh\", \"min_valid_soh\", \"soh_is_valid\"], logger=logger)\n",
    "        .query(\"soh_is_valid\")\n",
    "        .dropna(subset=[\"SOH\", \"ODOMETER_LAST\"], how=\"any\")\n",
    "    )\n",
    "\n",
    "def intercept_and_slope_from_points(points: DF) -> tuple[float, float]:\n",
    "    slope = (points.at[\"B\", \"SOH\"] - points.at[\"A\", \"SOH\"]) / (points.at[\"B\", \"ODOMETER_LAST\"] - points.at[\"A\", \"ODOMETER_LAST\"])\n",
    "    intercept = points.at[\"A\", \"SOH\"] - slope * points.at[\"A\", \"ODOMETER_LAST\"]\n",
    "    return intercept, slope\n",
    "\n",
    "filtered_results = filter_results_by_lines_bounds(df)\n",
    "px.scatter(filtered_results, x=\"ODOMETER_LAST\", y=\"SOH\", color=\"VIN\", hover_data=[\"DATETIME_BEGIN\"])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "data_ev",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

