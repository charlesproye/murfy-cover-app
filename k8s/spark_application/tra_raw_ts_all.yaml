apiVersion: sparkoperator.k8s.io/v1beta2
kind: SparkApplication
metadata:
  name: raw-ts-all
  namespace: spark-operator
spec:
  type: Python
  pythonVersion: "3"
  mode: cluster
  image: "rg.fr-par.scw.cloud/bib-images/spark:1"
  imagePullPolicy: Always
  mainApplicationFile: local:///app/src/transform/raw_tss/main.py
  arguments:
    - run
    - --all
  sparkVersion: "4.0.1"
  sparkConf:
    "spark.hadoop.fs.s3a.endpoint": "https://s3.fr-par.scw.cloud"
    "spark.hadoop.fs.s3a.impl": "org.apache.hadoop.fs.s3a.S3AFileSystem"
    "spark.hadoop.fs.s3a.path.style.access": "true"
    "spark.hadoop.fs.s3a.aws.credentials.provider": "org.apache.hadoop.fs.s3a.SimpleAWSCredentialsProvider"
    "spark.hadoop.fs.s3a.experimental.input.fadvise": "normal"
    "spark.hadoop.fs.s3a.connection.maximum": "1000"
    "spark.hadoop.fs.s3a.threads.max": "20"
    "spark.hadoop.fs.s3a.threads.core": "10"
    "spark.hadoop.fs.s3a.buffer.dir": "/tmp"
    "spark.hadoop.fs.s3a.block.size": "134217728"
    "spark.hadoop.fs.s3a.multipart.size": "134217728"
    "spark.hadoop.fs.s3a.multipart.threshold": "134217728"
    "spark.sql.adaptive.enabled": "true"
    "spark.sql.adaptive.advisoryPartitionSizeInBytes": "64MB"
    "spark.sql.shuffle.partitions": "200"
    "spark.default.parallelism": "200"
    "spark.kubernetes.container.image.pullPolicy": "Always"
  restartPolicy:
    type: Never
  driver:
    cores: 2
    coreLimit: "3000m"
    memory: "4g"
    memoryOverhead: "2g"
    serviceAccount: spark-operator-spark  # Adjust to your service account
    envFrom:
      - secretRef:
          name: s3-secret
      - secretRef:
          name: db-data-ev-secret
    env:
      - name: PYTHONPATH
        value: "/app/src"
  executor:
    cores: 3
    coreLimit: "4000m"
    instances: 3
    memory: "4g"
    memoryOverhead: "2g"
    envFrom:
      - secretRef:
          name: s3-secret
      - secretRef:
          name: db-data-ev-secret
    env:
      - name: PYTHONPATH
        value: "/app/src"

